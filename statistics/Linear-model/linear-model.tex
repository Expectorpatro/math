\section{一般线性模型}
\begin{definition}\label{model:LinearModel}
	称以下模型为\gls{LinearModel}：
	\begin{equation*}
		\begin{cases}
			y=X\beta+\varepsilon \\
			\operatorname{E}(\varepsilon)=\mathbf{0} \\
			\operatorname{Cov}(\varepsilon)=\sigma^2I_n
		\end{cases}
	\end{equation*}
	其中$y$为$n\times 1$\gls{ObservationVector}，$X$为$n\times p$\gls{DesignMatrix}，$\beta$为$p\times 1$未知参数向量，$\varepsilon$为随机误差，$\sigma^2$为误差方差。
\end{definition}
\begin{definition}
	称方程$X^TX\beta=X^Ty$为\gls{NormalEquation}。
\end{definition}
\begin{theorem}\label{theo:LSELinearModel}
	对于\cref{model:LinearModel}，$\hat{\beta}=(X^TX)^-X^Ty$是其唯一的最小二乘解。
\end{theorem}
\begin{proof}
	注意到：
	\begin{gather*}
		\begin{aligned}
			Q(\beta)&=||y-X\beta||^2=(y-X\beta)^T(y-X\beta) \\
			&=y^Ty-y^TX\beta-\beta^TX^Ty+\beta^TX^TX\beta \\
			&=y^Ty-2y^TX\beta+\beta^TX^TX\beta
		\end{aligned}\\
		\frac{\partial y^TX\beta}{\partial\beta}=X^Ty,\quad
		\frac{\partial \beta^TX^TX\beta}{\partial\beta}=2X^TX\beta \\
		\frac{\partial Q(\beta)}{\partial\beta}=2X^Ty-2X^TX\beta=0 \\
		X^TX\beta=X^Ty
	\end{gather*}
	由\cref{theo:VectorSpaceAAAT}和\cref{theo:SolutionOfSLE2}(1)可知方程$X^TX\beta=X^Ty$是相容的，根据\cref{theo:InhomogeneousLinearEq'sGeneralSolution2}可知其通解为：
	\begin{equation*}
		\hat{\beta}=(X^TX)^-X^Ty
	\end{equation*}
	其中$(X^TX)^-$是$X^TX$的任意一个广义逆矩阵。\par
	对任意的$\beta$，有：
	\begin{align*}
		Q(\beta)&=||y-X\beta||^2=||y-X\hat{\beta}+X\hat{\beta}-X\beta||^2=||y-X\hat{\beta}+X(\hat{\beta}-\beta)||^2 \\
		&=||y-X\hat{\beta}||^2+||X(\hat{\beta}-\beta)||^2+2(y-X\hat{\beta})^TX(\hat{\beta}-\beta)
	\end{align*}
	注意到正则方程即为：
	\begin{equation*}
		X^T(y-X\beta)=\mathbf{0}
	\end{equation*}
	于是：
	\begin{equation*}
		2(y-X\hat{\beta})^TX(\hat{\beta}-\beta)=2[X^T(y-X\hat{\beta})]^T(\hat{\beta}-\beta)=0
	\end{equation*}
	所以：
	\begin{equation*}
		Q(\beta)=||y-X\hat{\beta}||^2+||X(\hat{\beta}-\beta)||^2
	\end{equation*}
	上第二项总是非负的，由范数的定义可知其为$0$当且仅当$X\hat{\beta}=X\beta$，即当且仅当$X^TX\beta=X^TX\hat{\beta}=X^Ty$，所以使$Q(\beta)$达到最小值的$\beta$必为正则方程的解$\hat{\beta}=(X^TX)^-X^Ty$。
\end{proof}
\begin{derivation}
	若$\operatorname{rank}(X)=p$，则$X$的列向量组线性无关。考虑二次型$y^TX^TXy$，$y^TX^TXy=0\Leftrightarrow||Xy||=0\Leftrightarrow Xy=\mathbf{0}$，而$X$的列向量是线性无关的，所以不存在非零向量的$y$使得$Xy=\mathbf{0}$，于是$y^TX^TXy$是一个正定二次型，$X^TX$是一个正定矩阵。由\cref{theo:PositiveDefinite}(6)可得$X^TX$可逆。此时$\hat{\beta}=(X^TX)^{-1}X^Ty$，称$\hat{\beta}$为$\beta$的\gls{LSE}。
\end{derivation}
\begin{note}
	接下来我们将涉及到非常多的公式，初学会觉得非常难记，但如果从投影的角度去理解的话，很多公式都是直观上非常显然的，也就更好记忆了。
\end{note}
\subsection{参数估计}
\subsubsection{回归系数}
\begin{definition}
	若存在$n\times 1$向量$\alpha$使得$\operatorname{E}(\alpha^Ty)=c^T\beta$对一切的$\beta$成立，则称$c^T\beta$为\gls{EstimableFunction}。
\end{definition}
\begin{property}\label{prop:EstimableFunction}
	对于\cref{model:LinearModel}，$c^T\beta$和$d^T\beta$是可估函数，$\hat{\beta}$是正则方程的解，则：
	\begin{enumerate}
		\item 使$c^T\beta$成为可估函数的全体向量$c$构成$\mathcal{M}(X^T)$；
		\item 可估函数的全体构成一个$\operatorname{rank}(X)$维线性空间；
		\item $c^T\hat{\beta}$与$(X^TX)^-$的选择无关；
		\item $c^T\hat{\beta}$为$c^T\beta$的无偏估计；
		\item  $\operatorname{Cov}(c^T\hat{\beta},d^T\hat{\beta})=\sigma^2c^T(X^TX)^-d$，且与$(X^TX)^-$的选择无关；
		\item (Gauss-Markov)$\;c^T\hat{\beta}$是$c^T\beta$唯一的BLUE；
		\item 设$\varphi_i=c_i^T\beta,\;i=1,2,\dots,k$都是可估函数，$\seq{\alpha}{k}\in\mathbb{R}^{}$，则$\varphi=\sum\limits_{i=1}^{k}\alpha_i\varphi_i$也是可估的，且$\hat{\varphi}=\sum\limits_{i=1}^{k}\alpha_ic_i^T\hat{\beta}$是$\varphi$的BLUE。
	\end{enumerate}
\end{property}
\begin{proof}
	(1)$\;c^T\beta$是可估函数$\Leftrightarrow$存在$n\times1$向量$\alpha$使得$\operatorname{E}(\alpha^Ty)=\alpha^T\operatorname{E}(y)=\alpha^TX\beta=c^T\beta$对一切的$\beta$成立$\Leftrightarrow c=X^T\alpha$。\par
	(2)由(1)可得可估函数对加法和数乘封闭，根据\cref{theo:Subspace}可知所有可估函数构成一个线性空间。注意到可估函数$c_1^T\beta$和$c_2^T\beta$线性相关当且仅当$c_1$和$c_2$线性相关，由\cref{prop:SpanSubspace}(3)和\cref{prop:MatrixRank}(7)可得可估函数空间的秩为$\operatorname{rank}(X)$。\par
	(3)因为$c^T\beta$可估，由(1)可知存在$n\times 1$向量$\alpha$使得$c=X^T\alpha$，于是由\cref{prop:Transpose}(4)可得：
	\begin{equation*}
		c^T\hat{\beta}=\alpha^TX(X^TX)^-X^Ty
	\end{equation*}
	由\cref{prop:A-}(4)即可得出结论。\par
	(4)因为$c^T\beta$可估，由(1)可知存在$n\times 1$向量$\alpha$使得$c=X^T\alpha$，根据\cref{prop:A-}(7)可得：
	\begin{equation*}
		\operatorname{E}(c^T\hat{\beta})=\operatorname{E}[c^T(X^TX)^-X^Ty]=c^T(X^TX)^-X^TX\beta=c^T\beta
	\end{equation*}\par
	(5)因为$c^T\beta,d^T\beta$是可估函数，由(1)可知存在$\alpha,\gamma$使得$c=X^T\alpha,d=X^T\gamma$。由\cref{prop:CovMat}(3)、\cref{prop:Transpose}(4)和\cref{prop:A-}(5)(7)可知：
	\begin{align*}
		\operatorname{Cov}(c^T\hat{\beta},d^T\hat{\beta})
		&=\operatorname{Cov}[c^T(X^TX)^-X^Ty,d^T(X^TX)^-X^Ty] \\
		&=c^T(X^TX)^-X^T\operatorname{Cov}(y)X[(X^TX)^-]^Td \\
		&=c^T(X^TX)^-X^T\sigma^2I_nX(X^TX)^-d \\
		&=\sigma^2\alpha^TX(X^TX)^-X^TX(X^TX)^-X^T\gamma \\
		&=\sigma^2c^T(X^TX)^-d=\sigma^2\alpha^TX(X^TX)^-X^T\gamma
	\end{align*}
	由\cref{prop:A-}(4)即可知$\operatorname{Cov}(c^T\hat{\beta},d^T\hat{\beta})$与$(X^TX)^-$的选择无关。\par
	(6)无偏性由(4)可得，线性性由正则方程可知，下证方差最小。
	设$a^Ty$为$c^T\beta$的任一无偏估计，由(1)的过程可知$c=X^Ta$。根据\cref{prop:A-}(5)和(5)可得：
	\begin{align*}
		\operatorname{Var}(a^Ty)-\operatorname{Var}(c^T\hat{\beta})&=\sigma^2[a^Ta-c^T(X^TX)^-c] \\
		&=\sigma^2[a^T-c^T(X^TX)^-X^T][a-X(X^TX)^-c] \\
		&=\sigma^2||a-X(X^TX)^-c||^2\geqslant0
	\end{align*}
	上式第一行到第二行是由于\cref{prop:A-}(7)和\cref{prop:Transpose}(4)：
	\begin{align*}
		&[a^T-c^T(X^TX)^-X^T][a-X(X^TX)^-c] \\
		=&a^Ta-a^TX(X^TX)^-c-c^T(X^TX)^-X^Ta+c^T(X^TX)^-X^TX(X^TX)^-c \\
		=&a^Ta-c^T(X^TX)^-c-c^T(X^TX)^-c+c^T(X^TX)^-c \\
		=&a^Ta-c^T(X^TX)^-c
	\end{align*}
	由范数的性质可知$\operatorname{Var}(a^Ty)=\operatorname{Var}(c^T\hat{\beta})$当且仅当$a=X(X^TX)^-c$，由\cref{prop:A+}(3)可知$a=X(X^TX)^-c\Leftrightarrow a^T=c^T(X^TX)^-X^T\Leftrightarrow a^Ty=c^T(X^TX)^-X^Ty=c^T\hat{\beta}$。\par
	(7)由(2)可知$\varphi$是可估的。\par
	由(4)可得$c_i^T\hat{\beta}$是$c_i^T\beta$的无偏估计，所以：
	\begin{equation*}
		\operatorname{E}(\hat{\varphi})=\operatorname{E}\left(\sum_{i=1}^{k}\alpha_ic_i^T\hat{\beta}\right)=\sum_{i=1}^{k}\alpha_i\operatorname{E}(c_i^T\hat{\beta})=\sum_{i=1}^{k}\alpha_ic_i^T\beta=\varphi
	\end{equation*}
	即$\hat{\varphi}$是一个无偏估计。\par
	令$c=\sum\limits_{i=1}^{k}\alpha_ic_i$，则$\varphi=c^T\beta$。设$\gamma^Ty$是$\varphi$的一个无偏估计，于是由(6)可得：
	\begin{equation*}
		\operatorname{Var}(\gamma^Ty)-\operatorname{Var}(c^T\hat{\beta})=\sigma^2||\gamma-X(X^TX)^-c||^2
	\end{equation*}
	上式等于$0\Leftrightarrow \gamma^Ty=c^T\hat{\beta}=\hat{\varphi}$，即$\hat{\varphi}$是唯一的BLUE。
\end{proof}
\begin{definition}
	对于\cref{model:LinearModel}，若$c^T\beta$是可估函数，称$c^T\hat{\beta}$为$c^T\beta$的LSE，其中$\hat{\beta}$为正则方程的解。
\end{definition}
\subsubsection{残差}
\begin{definition}
	记$\hat{y}=X\hat{\beta}$，称$\hat{e}=y-\hat{y}$为\gls{ResidualVector}，称$\hat{e}^T\hat{e}$为\gls{SSE}，记为$\operatorname{SSE}$。
\end{definition}
\begin{property}\label{prop:ehat}
	对于\cref{model:LinearModel}，$\hat{\beta}$为正则方程的解，则残差向量$\hat{e}$满足：
	\begin{enumerate}
		\item $\operatorname{E}(\hat{e})=0,\;\operatorname{Cov}(\hat{e})=\sigma^2(I_n-P_X)$；
		\item $\operatorname{SSE}=y^T(I_n-P_X)y$；
		\item $\operatorname{Cov}(\hat{y},\hat{e})=\mathbf{0}$；
	\end{enumerate}
\end{property}
\begin{proof}
	(1)由\cref{prop:OrthogonalProjectionMat}(2)可知向$\mathcal{M}(X)$的正交投影阵$P_X=X(X^TX)^-X^T$，根据\cref{prop:OrthogonalProjectionMat}(7)(3)可知$I_n-P_X$是对称幂等阵，所以由\cref{prop:CovMat}(3)可得：
	\begin{gather*}
		\begin{aligned}
			\operatorname{E}(\hat{e})&=\operatorname{E}(y-X\hat{\beta})=\operatorname{E}[I_ny-X(X^TX)^-X^Ty]=(I_n-P_X)\operatorname{E}(y) \\
			&=(I_n-P_X)X\beta=(X-X)\beta=0
		\end{aligned} \\
		\begin{aligned}
			\operatorname{Cov}(\hat{e})&=\operatorname{Cov}[(I_n-P_X)y]=(I_n-P_X)\operatorname{Cov}(y)(I_n-P_X)^T \\
			&=(I_n-P_X)\operatorname{Cov}(y)(I_n-P_X)=\sigma^2(I_n-P_X)
		\end{aligned}
	\end{gather*}\par
	(2)由(1)的证明过程可知：
	\begin{equation*}
		\hat{e}=(I_n-P_X)y
	\end{equation*}
	且$I_n-P_X$是一个对称幂等阵，于是由\cref{prop:Transpose}(4)可得：
	\begin{equation*}
		\hat{e}^T\hat{e}=y^T(I_n-P_X)^T(I_n-P_X)y=y^T(I_n-P_X)(I_n-P_X)y=y^T(I_n-P_X)y
	\end{equation*}\par
	(3)由\cref{prop:CovMat}(5)(3)和\cref{prop:OrthogonalProjectionMat}(2)(8)可得
	\begin{align*}
		\operatorname{Cov}(\hat{y},\hat{e})&=\operatorname{Cov}(\hat{y},y-\hat{y})=\operatorname{Cov}(\hat{y},y)-\operatorname{Cov}(\hat{y}) \\
		&=\operatorname{Cov}[X(X^TX)^-X^Ty,y]-\operatorname{Cov}[X(X^TX)^-X^Ty] \\
		&=\operatorname{Cov}[P_Xy,(I_n-P_X)y]=P_X\operatorname{Cov}(y)(I_n-P_X) \\
		&=\sigma^2P_X(I_n-P_X)=\mathbf{0}\qedhere
	\end{align*}
\end{proof}
\subsubsection{误差方差}
\begin{theorem}\label{theo:VarianceOfErrorTerm}
	对于\cref{model:LinearModel}，$\hat{\beta}$为正则方程的解，$\operatorname{rank}(X)=r$，则：
	\begin{equation*}
		\hat{\sigma}^2=\frac{\operatorname{SSE}}{n-r}
	\end{equation*}
	是$\sigma^2$的无偏估计。
\end{theorem}
\begin{proof}
	注意到$(I_n-P_X)X=X-X=\mathbf{0}$，由\cref{prop:ehat}(2)、\cref{theo:ERVQuadraticForm}和\cref{prop:Transpose}(4)可得：
	\begin{align*}
		\operatorname{E}(\operatorname{SSE})&=\operatorname{E}[y^T(I_n-P_X)y] =\beta^TX^T(I_n-P_X)X\beta+\operatorname{tr}[(I_n-P_X)\sigma^2I_n] \\
		&=\sigma^2\operatorname{tr}(I_n-P_X)
	\end{align*}
	由\cref{prop:OrthogonalProjectionMat}(7)(3)、\cref{prop:IdempotentMat}(2)(5)和\cref{prop:OrthogonalProjectionMat}(1)可得：
	\begin{equation*}
		\operatorname{tr}(I_n-P_X)=\operatorname{rank}(I_n-P_X)=n-\operatorname{rank}(P_X)=n-\operatorname{rank}(X)=n-r
	\end{equation*}
	根据\cref{prop:MeasurableIntegral}(5)即：
	\begin{equation*}
		\operatorname{E}\left(\frac{\operatorname{SSE}}{n-r}\right)=\sigma^2\qedhere
	\end{equation*}
\end{proof}
\begin{definition}
	称$\hat{\sigma}^2$为$\sigma^2$的LSE。
\end{definition}

\subsection{约束最小二乘估计}
\begin{theorem}\label{theo:ConstraintLinearModel}
	对于\cref{model:LinearModel}，假设：
	\begin{equation*}
		A\beta=b,\quad A\in M_{k\times p}(K),\quad\operatorname{rank}(A)=k,\quad\mathcal{M}(A^T)\subseteq\mathcal{M}(X^T)
	\end{equation*}
	且$A\beta=b$相容，则：
	\begin{equation*}
		\hat{\beta}_A=\hat{\beta}-(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b)
	\end{equation*}
	为$\beta$在约束$A\beta=b$下的约束LS解，$A\hat{\beta}_A$为$A\beta$的约束LSE。
\end{theorem}
\begin{proof}
	使用Lagrange乘子法构造辅助函数（$\lambda$为Lagrange乘子，乘子前加上系数$2$是为了下面不出现分数，对结果没有影响）：
	\begin{align*}
		F(\beta,\lambda)&=||y-X\beta||^2+2\lambda^T(A\beta-b) \\
		&=y^Ty-y^TX\beta-\beta^TX^Ty+\beta^TX^TX\beta+2\lambda^TA\beta-2\lambda^Tb
	\end{align*}
	于是：
	\begin{equation*}
		\frac{\partial F(\beta,\lambda)}{\partial\beta}=-2X^Ty+2X^TX\beta+2A^T\lambda
	\end{equation*}
	令上式为$0$，得到：
	\begin{equation*}
		X^TX\beta=X^Ty-A^T\lambda
	\end{equation*}
	于是约束下的解即为方程组：
	\begin{equation*}
		\begin{cases}
			X^TX\beta=X^Ty-A^T\lambda \\
			A\beta=b
		\end{cases}
	\end{equation*}
	的解，将其记为$\hat{\beta}_A,\hat{\lambda}$。因为$\mathcal{M}(A^T)\subseteq\mathcal{M}(X^T)$，由\cref{theo:VectorSpaceAAAT}和\cref{theo:SolutionOfSLE2}(1)可知方程组是相容的。由\cref{theo:InhomogeneousLinearEq'sGeneralSolution2}可知：
	\begin{equation*}
		\hat{\beta}_A=(X^TX)^-X^Ty-(X^TX)^-A^T\hat{\lambda}=\hat{\beta}-(X^TX)^-A^T\hat{\lambda}
	\end{equation*}
	代入方程组的第二个方程可得：
	\begin{equation*}
		A\hat{\beta}-A(X^TX)^-A^T\hat{\lambda}=b
	\end{equation*}
	由\info{$A(X^TX)^-A^T$的可逆性}可知：
	\begin{equation*}
		\hat{\lambda}=[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b)
	\end{equation*}
	于是：
	\begin{equation*}
		\hat{\beta}_A=\hat{\beta}-(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b)
	\end{equation*}
	下证明这个解确实是最小二乘解。\par
	做分解：
	\begin{align*}
		||y-X\beta||^2
		&=||y-X\hat{\beta}+X(\hat{\beta}-\beta)||^2 \\
		&=||y-X\hat{\beta}||^2+2(y-X\hat{\beta})^TX(\hat{\beta}-\beta)+(\hat{\beta}-\beta)^TX^TX(\hat{\beta}-\beta)
	\end{align*}
	由\cref{prop:Transpose}和\cref{prop:A-}(5)(6)可得：
	\begin{align*}
		(y-X\hat{\beta})^TX(\hat{\beta}-\beta)&=y^TX(\hat{\beta}-\beta)-\hat{\beta}^TX^TX(\hat{\beta}-\beta) \\
		&=y^TX(\hat{\beta}-\beta)-[(X^TX)^-X^Ty]^TX^TX(\hat{\beta}-\beta) \\
		&=y^TX(\hat{\beta}-\beta)-y^TX(X^TX)^-X^TX(\hat{\beta}-\beta) \\
		&=y^TX(\hat{\beta}-\beta)-y^TX(\hat{\beta}-\beta)=0
	\end{align*}
	于是：
	\begin{align*}
		||y-X\beta||^2
		&=||y-X\hat{\beta}||^2+(\hat{\beta}-\beta)^TX^TX(\hat{\beta}-\beta) \\
		&=||y-X\hat{\beta}||^2+(\hat{\beta}-\hat{\beta}_A+\hat{\beta}_A-\beta)^TX^TX(\hat{\beta}-\hat{\beta}_A+\hat{\beta}_A-\beta) \\
		&=||y-X\hat{\beta}||^2+||X(\hat{\beta}-\hat{\beta}_A)||^2+||X(\hat{\beta}_A-\beta)||^2+2(\hat{\beta}-\hat{\beta}_A)^TX^TX(\hat{\beta}_A-\beta)
	\end{align*}
	由\cref{prop:A-}(5)(7)以及$\mathcal{M}(A^T)\subseteq\mathcal{M}(X^T)$可得：
	\begin{align*}
		(\hat{\beta}-\hat{\beta}_A)^TX^TX(\hat{\beta}_A-\beta)
		&=[(X^TX)^-A^T\hat{\lambda}]^TX^TX(\beta_A-\beta)
		=\hat{\lambda}^TA(X^TX)^-X^TX(\beta_A-\beta) \\
		&=\hat{\lambda}^TA(\beta_A-\beta)
		=\hat{\lambda}^T(A\beta_A-A\beta)=0
	\end{align*}
	所以：
	\begin{equation*}
		||y-X\beta||^2=||y-X\hat{\beta}||^2+||X(\hat{\beta}-\hat{\beta}_A)||^2+||X(\hat{\beta}_A-\beta)||^2
	\end{equation*}
	即对任意满足$A\beta=b$的$\beta$都有：
	\begin{equation*}
		||y-X\beta||^2\geqslant||y-X\hat{\beta}||^2+||X(\hat{\beta}-\hat{\beta}_A)||^2
	\end{equation*}
	等号成立当且仅当$\beta=\hat{\beta}_A$，于是$\hat{\beta}_A$是LSE。
\end{proof}
\subsubsection{误差方差}
\begin{theorem}
	在\cref{theo:ConstraintLinearModel}的假设下，在参数区域$A\beta=b$上，
	\begin{equation*}
		\hat{\sigma}_A^2=\frac{||y-X\hat{\beta}_A||^2}{n-r+k}=\frac{\operatorname{SSE}_A}{n-r+k}
	\end{equation*}
	是$\sigma^2$的无偏估计。
\end{theorem}
\begin{proof}
	由\cref{theo:ConstraintLinearModel}可知：
	\begin{equation*}
		\operatorname{E}(||y-X\hat{\beta}_A||^2)=\operatorname{E}[||y-X\hat{\beta}||^2+||X(\hat{\beta}-\hat{\beta}_A)||^2]=\operatorname{E}(||y-X\hat{\beta}||^2)+\operatorname{E}[||X(\hat{\beta}-\hat{\beta}_A)||^2]
	\end{equation*}
	根据\cref{theo:VarianceOfErrorTerm}可知：
	\begin{equation*}
		\operatorname{E}(||y-X\hat{\beta}||^2)=(n-r)\sigma^2
	\end{equation*}
	由\cref{prop:A-}(5)(7)、$\mathcal{M}(A^T)\subseteq\mathcal{M}(X^T)$
	\begin{align*}
		||X(\hat{\beta}-\hat{\beta}_A)||&=(\hat{\beta}-\hat{\beta}_A)^TX^TX(\hat{\beta}-\hat{\beta}_A) \\
		&=\{(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b)\}^T \\
		&\quad\cdot X^TX(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&=(A\hat{\beta}-b)^T[A(X^TX)^-A^T]^{-1}A(X^TX)^- \\
		&\quad\cdot X^TX(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&=(A\hat{\beta}-b)^T[A(X^TX)^-A^T]^{-1}A(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&=(A\hat{\beta}-b)^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b)
	\end{align*}
	因为$\mathcal{M}(A^T)\subseteq\mathcal{M}(X^T)$，所以由\cref{prop:EstimableFunction}(1)可知$A\beta$的每一个元素都是可估函数，于是由\cref{theo:ERVQuadraticForm}和\cref{prop:EstimableFunction}(4)(5)可知：
	\begin{align*}
		\operatorname{E}(||X(\hat{\beta}-\hat{\beta}_A)||)
		&=\operatorname{E}\{(A\hat{\beta}-b)^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b)\} \\
		&=(A\beta-b)^T[A(X^TX)^-A^T]^{-1}(A\beta-b) \\
		&\quad+\operatorname{tr}\{[A(X^TX)^-A^T]^{-1}\operatorname{Cov}(A\hat{\beta}-b)\} \\
		&=\operatorname{tr}\{[A(X^TX)^-A^T]^{-1}\sigma^2A(X^TX)^-A^T\}=\sigma^2\operatorname{tr}(I_k)=k\sigma^2
	\end{align*}
	所以：
	\begin{equation*}
		\operatorname{E}(||y-X\hat{\beta}_A||^2)=(n-r+k)\sigma^2
	\end{equation*}
	即在参数区域$A\beta=b$上，
	\begin{equation*}
		\hat{\sigma}_A^2=\frac{||y-X\hat{\beta}_A||^2}{n-r+k}
	\end{equation*}
	是$\sigma^2$的无偏估计。
\end{proof}

\subsection{实际计算}
\begin{theorem}\label{theo:SSESSEACalculate}
	对于无约束条件以及约束$A\beta=\mathbf{0}$，有：
	\begin{gather*}
		\operatorname{SSE}=||y-X\hat{\beta}||^2=y^Ty-\hat{\beta}^TX^Ty,\quad
		\operatorname{SSE}_A=||y-X\hat{\beta}_A||^2=y^Ty-\hat{\beta}_A^TX^Ty
	\end{gather*}
\end{theorem}
\begin{proof}
	由\cref{prop:A-}(6)可知：
	\begin{align*}
		\operatorname{SSE}&=(y-X\hat{\beta})^T(y-X\hat{\beta})=y^Ty-y^TX\hat{\beta}-\hat{\beta}^TX^Ty+\hat{\beta}^TX^TX\hat{\beta} \\
		&=y^Ty-2\hat{\beta}^TX^Ty+\hat{\beta}^TX^TX(X^TX)^-X^Ty=y^Ty-2\hat{\beta}^TX^Ty+\hat{\beta}^TX^Ty \\
		&=y^Ty-\hat{\beta}^TX^Ty
	\end{align*}
	由\cref{theo:ConstraintLinearModel}可知：
	\begin{equation*}
		\begin{cases}
			X^TX\hat{\beta}_A=X^Ty-A^T\hat{\lambda} \\
			A\hat{\beta}_A=\mathbf{0}
		\end{cases}
	\end{equation*}
	其中$\lambda$为Lagrange乘子，于是有：
	\begin{align*}
		\operatorname{SSE}_A&=(y-X\hat{\beta}_A)^T(y-X\hat{\beta}_A)=y^Ty-y^TX\hat{\beta}_A-\hat{\beta}_A^TX^Ty+\hat{\beta}_A^TX^TX\hat{\beta}_A \\
		&=y^Ty-\hat{\beta}_A^TX^Ty+\hat{\beta}_A^TX^TX\hat{\beta}_A-\hat{\beta}^TX^Ty=y^Ty-\hat{\beta}_A^TX^Ty+\hat{\beta}_A^T(X^TX\hat{\beta}_A-X^Ty) \\
		&=y^Ty-\hat{\beta}_A^TX^Ty-\hat{\beta}_A^TA^T\hat{\lambda}=y^Ty-\hat{\beta}_A^TX^Ty\qedhere
	\end{align*}
\end{proof}
\begin{definition}
	称$\hat{\beta}^TX^Ty=y^TP_Xy$为\gls{RSS}，记为$\operatorname{RSS}(\beta)$。称$\hat{\beta}_A^TX^Ty$为约束条件$A\beta=\mathbf{0}$下的回归平方和，记为$\operatorname{RSS}_A(\beta)$。称$y^Ty$为总平方和，记作$\operatorname{RSS}_{\text{总}}$。称$\sqrt{\dfrac{\operatorname{RSS}(\beta)}{\operatorname{RSS}_{\text{总}}}}$为$y$与$\seq{X}{p}$之间的复相关系数，记作$\operatorname{R}$。
\end{definition}
\begin{property}\label{prop:RSS=RSSbeta+SSE}
	$\operatorname{RSS}_{\text{总}}=\operatorname{RSS}(\beta)+\operatorname{SSE}$。
\end{property}
\begin{proof}
	由\cref{prop:ehat}(2)可得$\operatorname{RSS}(\beta)+\operatorname{SSE}=y^TP_Xy+y^T(I_n-P_X)y=y^Ty=\operatorname{RSS}_{\text{总}}$。
\end{proof}
\begin{note}
	回归平方和表示了数据平方和$y^Ty$中能够由因变量$y$与自变量$\seq{X}{p}$的线性关系解释的部分，因为它是$y$在$<\seq{X}{p}>$上投影长度的平方。由此可以看出$\operatorname{R}^2\leqslant1$。我们用$\operatorname{R}^2$来度量$\seq{X}{p}$对$y$线性拟合程度的好坏，越接近$1$越好。
\end{note}

\subsection{预测}
\label{def:LinearModelForcast}
假设要预测$m$个点$x_{0i}=(x_{0i1},x_{0i2},\dots,x_{0ip}),\;i=1,2,\dots,m$，它们所对应的因变量为$y_{01},y_{02},\dots,y_{0m}$。已知$y_{0i}$和历史数据服从同一个线性模型，即：
\begin{gather*}
	y_0=X_0\beta+\varepsilon_0,\quad\operatorname{E}(\varepsilon_0)=\mathbf{0},\quad\operatorname{Cov}(\varepsilon_0)=\sigma^2I_m \\
	y_0=
	\begin{pmatrix}
		y_{01} \\
		y_{02} \\
		\vdots \\
		y_{0m}
	\end{pmatrix},\quad
	X_0=
	\begin{pmatrix}
		x_{011} & x_{012} & \cdots & x_{01p} \\
		x_{021} & x_{022} & \cdots & x_{02p} \\
		\vdots & \vdots & \ddots & \vdots \\
		x_{0m1} & x_{0m2} & \cdots & x_{0mp} 
	\end{pmatrix},\quad
	\varepsilon_0=
	\begin{pmatrix}
		\varepsilon_{01} \\
		\varepsilon_{02} \\
		\vdots \\
		\varepsilon_{0m}
	\end{pmatrix}
\end{gather*}
\textbf{假设$\mathcal{M}(X_0^T)\subseteq\mathcal{M}(X^T)$}。
\subsubsection{被测量与历史数据无关}
\begin{derivation}
	此时使用$\operatorname{E}(y_0)=X_0\beta$的估计去进行预测。\par
	由\cref{theo:VectorSpaceAAAT}可知$\mathcal{M}(X^T)=\mathcal{M}(X^TX)$，于是$\mathcal{M}(X_0^T)\subseteq\mathcal{M}(X^T)=\mathcal{M}(X^TX)$，由\cref{prop:A-}(3)可知$X_0(X^TX)^-X^Ty$与广义逆$(X^TX)^-$的选择无关。\par
	因为$\mathcal{M}(X_0^T)\subseteq\mathcal{M}(X^T)$，所以$X_0\beta$是可估的，它具有可估函数的性质。但需注意，因为$y_0$也是一个随机变量，所以这里的无偏性指的是$\operatorname{E}(\hat{y}_0-y_0)=0$。
\end{derivation}
\subsubsection{被测量与历史数据相关}
\begin{derivation}
	在某些情况下，$y_0$和$y$确实具有一定的相关性，用$\operatorname{Cov}(\varepsilon,\varepsilon_0)=\sigma^2V^T$来度量它们之间的相关性，此时有：
	\begin{equation*}
		\operatorname{Cov}[(y,y_0)^T]=\operatorname{Cov}[(\varepsilon,\varepsilon_0)^T]=\sigma^2
		\begin{pmatrix}
			I_n & V^T \\
			V & I_m 
		\end{pmatrix}
	\end{equation*}
\end{derivation}
\begin{definition}
	记$\hat{y}=Cy$是$y$的一个线性无偏估计，称：
	\begin{equation*}
		\operatorname{PMSE}(\hat{y})=\operatorname{E}[(\hat{y}-y)^TA(\hat{y}-y)]
	\end{equation*}
	为\gls{PMSE}，其中$A>0$。
\end{definition}
\begin{theorem}\label{theo:PECorrelatedLinearModelForcast}
	$y_0$在广义预测均方误差意义下的最优线性无偏估计为：
	\begin{equation*}
		\hat{y}_0=X_0\hat{\beta}+V(y-X\hat{\beta})
	\end{equation*}
	其中$\hat{\beta}$为正则方程的解。
\end{theorem}
\begin{proof}
	令$\hat{y}_0=Cy$是一个无偏估计，由\cref{prop:MeasurableIntegral}(5)和\cref{prop:CovMat}(3)可得：
	\begin{gather*}
		\hat{y}_0-y_0=Cy-X_0\beta-\varepsilon_0=CX\beta+C\varepsilon-X_0\beta-\varepsilon_0=(CX-X_0)\beta+C\varepsilon-\varepsilon_0 \\
		\operatorname{E}(\hat{y}_0-y_0)=(CX-X_0)\beta=\mathbf{0}\text{对一切$\beta$成立}\iff CX=X_0 \\
		\begin{aligned}
			\operatorname{Cov}(C\varepsilon-\varepsilon_0)&=\operatorname{E}[(C\varepsilon-\varepsilon_0)(C\varepsilon-\varepsilon_0)^T]=\operatorname{E}(C\varepsilon\varepsilon^TC^T-C\varepsilon\varepsilon_0^T-\varepsilon_0\varepsilon^TC^T+\varepsilon_0\varepsilon_0^T) \\
			&=\operatorname{E}(C\varepsilon\varepsilon^TC^T)-\operatorname{E}(C\varepsilon\varepsilon_0^T)-\operatorname{E}(\varepsilon_0\varepsilon^TC^T)+\operatorname{E}(\varepsilon_0\varepsilon_0^T) \\
			&=C\operatorname{E}(\varepsilon\varepsilon^T)C^T-\operatorname{Cov}(C\varepsilon,\varepsilon_0)-\operatorname{Cov}(\varepsilon_0,C\varepsilon)+\operatorname{Cov}(\varepsilon_0) \\
			&=C\operatorname{Cov}(\varepsilon)C^T-2\operatorname{Cov}(C\varepsilon,\varepsilon_0)+\sigma^2I_m \\
			&=\sigma^2CC^T-2C\operatorname{Cov}(\varepsilon,\varepsilon_0)+\sigma^2I_m \\
			&=\sigma^2CC^T-2\sigma^2CV^T+\sigma^2I_m=\sigma^2(CC^T-2CV^T+I_m)
		\end{aligned}
	\end{gather*}
	由\cref{theo:ERVQuadraticForm}和\cref{prop:Trace}(2)可得：
	\begin{align*}
		\operatorname{PMSE}(\hat{y}_0)&=\operatorname{E}[(\hat{y}_0-y)^TA(\hat{y}_0-y)] =\operatorname{E}[(C\varepsilon-\varepsilon_0)^TA(C\varepsilon-\varepsilon_0)] \\
		&=\operatorname{tr}[A\sigma^2(CC^T-2CV^T+I_m)] =\sigma^2\operatorname{tr}[A(CC^T-2CV^T+I_m)]
	\end{align*}
	接下来的目标就是求解：
	\begin{equation*}
		\min_{C}\operatorname{PMSE}(\hat{y}_0),\quad\operatorname{s.t.}CX=X_0
	\end{equation*}\par
	使用Lagrange乘子法，构造辅助函数（不引入$X_0$是因为它是常数，对结果没影响）：
	\begin{equation*}
		F(C,\varLambda)=\sigma^2\operatorname{tr}[A(CC^T-2CV^T+I_m)]-2\operatorname{tr}(CX\varLambda)
	\end{equation*}
	其中$\varLambda$是Lagrange乘子。由矩阵求导和\cref{prop:Trace}(1)可得：
	\begin{align*}
		\frac{\partial\operatorname{PSME}(\hat{y}_0)}{\partial C}&=\sigma^2\frac{\partial\operatorname{tr}(ACC^T)}{\partial C}-2\sigma^2\frac{\partial\operatorname{tr}(ACV^T)}{\partial C}-2\frac{\partial\operatorname{tr}(CX\varLambda)}{\partial C} \\
		&=\sigma^22AC-2\sigma^2AV-2\varLambda^TX^T
	\end{align*}
	令上式为$0$可得：
	\begin{gather*}
		\sigma^2AC=\sigma^2AV+\varLambda^TX^T \\
		C=V+\frac{A^{-1}\varLambda^TX^T}{\sigma^2}
	\end{gather*}
	代入$CX=X_0$可得：
	\begin{equation*}
		VX+\frac{A^{-1}\varLambda^TX^TX}{\sigma^2}=X_0 \\
		\varLambda^TX^TX=\sigma^2A(X_0-VX) \\
		X^TX\varLambda=\sigma^2(X_0^T-X^TV^T)A^T
	\end{equation*}
	由\cref{theo:VectorSpaceAAAT}可知$\mathcal{M}(X^T)=\mathcal{M}(X^TX)$，而$\mathcal{M}(X_0^T)\subseteq\mathcal{M}(X^T)$，所以上式等式右边矩阵的每一列都在$\mathcal{M}(X^TX)$中，即方程组是相容的。由\cref{theo:InhomogeneousLinearEq'sGeneralSolution2}可得：
	\begin{equation*}
		\varLambda=\sigma^2(X^TX)^-(X_0^T-X^TV^T)A^T
	\end{equation*}
	于是由\cref{prop:A-}(5)可得：
	\begin{align*}
		C&=V+A^{-1}A(X_0-VX)(X^TX)^-X^T=V+(X_0-VX)(X^TX)^-X^T \\
		&=X_0(X^TX)^-X^T+V[I_n-X(X^TX)^-X^T]
	\end{align*}
	由\cref{prop:A-}(3)(4)可知$C$是唯一的，与$(X^TX)^-$的选择无关。所以：
	\begin{equation*}
		\hat{y}_0=X_0(X^TX)^-X^Ty+V[I_n-X(X^TX)^-X^T]y=X_0\hat{\beta}+V(y-X\hat{\beta})
	\end{equation*}
	为$y_0$在广义预测均方误差意义下的最优线性无偏估计。
\end{proof}
