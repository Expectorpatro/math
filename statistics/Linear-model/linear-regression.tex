\section{线性回归模型}
\begin{definition}\label{model:LinearRegressionModel}
	设因变量$Y$和自变量$\seq{X}{p-1}$满足：
	\begin{equation*}
		Y=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_{p-1}X_{p-1}+e
	\end{equation*}
	若对因变量$Y$和自变量$\seq{X}{p-1}$进行了$n$次观察，得到$n$组数据，它们满足：
	\begin{equation*}
		y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_{p-1}x_{ip-1}+\varepsilon_i,\quad i=1,2,\dots,n
	\end{equation*}
	记：
	\begin{equation*}
		y=
		\begin{pmatrix}
			y_1 \\
			y_2 \\
			\vdots \\
			y_n
		\end{pmatrix},\quad
		X=
		\begin{pmatrix}
			1 & x_{11} & \cdots & x_{1p-1} \\
			1 & x_{21} & \cdots & x_{2p-1} \\
			\vdots & \vdots & \ddots & \vdots \\
			1 & x_{n1} & \cdots & x_{np-1} \\
		\end{pmatrix},\quad
		\beta=
		\begin{pmatrix}
			\beta_0 \\
			\beta_1 \\
			\vdots \\
			\beta_{p-1}
		\end{pmatrix},\quad
		\varepsilon=
		\begin{pmatrix}
			\varepsilon_1 \\
			\varepsilon_2 \\
			\vdots \\
			\varepsilon_n
		\end{pmatrix}
	\end{equation*}
	且假设$\operatorname{rank}(X)=p,\;\operatorname{E}(\varepsilon)=\mathbf{0},\;\operatorname{Cov}(\varepsilon)=\sigma^2I_n$，则得到\gls{LinearRegressionModel}：
	\begin{equation*}
		y=X\beta+\varepsilon,\quad\operatorname{E}(\varepsilon)=\mathbf{0},\quad\operatorname{Cov}(\varepsilon)=\sigma^2I_n
	\end{equation*}
	称$\beta_0$为常数项，$\beta_I=(\seq{\beta}{p-1})^T$为回归系数。若记$X=(\mathbf{1}_n,\tilde{X})$，其中$\mathbf{1}_n$为由$n$个$1$构成的列向量，则线性回归模型可被改写为：
	\begin{equation*}
		y=\beta_0\mathbf{1}_n+\tilde{X}\beta_I+\varepsilon,\quad\operatorname{E}(\varepsilon)=\mathbf{0},\quad\operatorname{Cov}(\varepsilon)=\sigma^2I_n
	\end{equation*}
\end{definition}
\begin{definition}\label{model:NormalLinearRegressionModel}
	对于\cref{model:LinearRegressionModel}，若$\varepsilon\sim\operatorname{N}_n(\mathbf{0},\sigma^2I_n)$，则称此时的线性回归模型为\gls{NormalLinearRegressionModel}。
\end{definition}
\subsubsection{中心化与标准化处理}
\begin{definition}\label{model:CentralizedStandardizedLinearModel}
	对于\cref{model:LinearRegressionModel}，记：
	\begin{equation*}
		\overline{x}_j=\frac{1}{n}\sum_{i=1}^{n}x_{ij},\quad s_j^2=\sum_{i=1}^{n}(x_{ij}-\overline{x}_j)^2
	\end{equation*}\par
	称：
	\begin{gather*}
		y_i=\alpha_0+\alpha_1(x_{i1}-\overline{x}_1)+\alpha_2(x_{i2}-\overline{x}_2)+\cdots+\alpha_{p-1}(x_{ip-1}-\overline{x}_{p-1})+\varepsilon_i \\
		y=\alpha_0\mathbf{1}_n+\tilde{X}_c\alpha_I+\varepsilon,\quad\operatorname{E}(\varepsilon)=\mathbf{0},\quad\operatorname{Cov}(\varepsilon)=\sigma^2I_n
	\end{gather*}
	为\gls{CentralizedLinearRegressionModel}，其中$\tilde{X}_c=\left(I_n-\dfrac{1}{n}\mathbf{1}_n\mathbf{1}_n^T\right)\tilde{X}$（请证明该结论）被称为\gls{CentralizedDesignMatrix}，第二行是第一行的矩阵表示。\par
	称：
	\begin{gather*}
		y_i=\gamma_0+\gamma_1\frac{x_{i1}-\overline{x}_1}{s_1}+\gamma_2\frac{x_{i2}-\overline{x}_2}{s_2}+\cdots+\gamma_{p-1}\frac{x_{ip-1}-\overline{x}_{p-1}}{s_{p-1}}+\varepsilon_i \\
		y=\gamma_0\mathbf{1}_n+\tilde{X}_s\gamma_I+\varepsilon,\quad\operatorname{E}(\varepsilon)=\mathbf{0},\quad\operatorname{Cov}(\varepsilon)=\sigma^2I_n
	\end{gather*}
	为\gls{StandardizedLinearRegressionModel}，其中$\tilde{X}_s=\tilde{X}_c\operatorname{diag}\left\{\dfrac{1}{s_1},\dfrac{1}{s_2},\dots,\dfrac{1}{s_{p-1}}\right\}$（请证明该结论）被称为\gls{StandardizedDesignMatrix}，第二行是第一行的矩阵表示。
\end{definition}
\begin{theorem}\label{theo:LinearRegressionModel}
	对于线性回归模型，设$\hat{\beta}=(\hat{\beta}_0,\seq{\hat{\beta}}{p-1})$为正则方程的解，则有以下结论：
	\begin{enumerate}
		\item $X^TX,\tilde{X}^T\tilde{X},\tilde{X}_c^T\tilde{X}_c$和$\tilde{X}_s^T\tilde{X}_s$都是正定阵，于是都可逆；
		\item $\mathbf{1}_n^T\tilde{X}_c=\mathbf{1}_n^T\tilde{X}_s=\mathbf{0}$；
		\item $\hat{\alpha}_0=\hat{\gamma}_0=\overline{y}=\hat{\beta}_0+\overline{x}^T\hat{\beta}_I,\;\hat{\alpha}_I=(\tilde{X}_c^T\tilde{X}_c)^{-1}\tilde{X}_c^Ty=\hat{\beta}_I,\;\hat{\gamma}_I=(\tilde{X}_s^T\tilde{X}_s)^{-1}\tilde{X}_s^Ty=\operatorname{diag}\{\seq{s}{p-1}\}\hat{\alpha}_I(\hat{\beta}_I)$，即原经验回归方程、中心化后得到的经验回归方程与标准化后得到的经验回归方程是等价的，且有$\tilde{X}_c\hat{\alpha}_I=\tilde{X}_s\hat{\gamma}_I$；
		\item $\operatorname{SSE}=\operatorname{SSE}_c=\operatorname{SSE}_s,\;\operatorname{SSE}_c=(y-\overline{y})^T(y-\overline{y})-\hat{\alpha}_I^T\tilde{X}_c^Ty,\;\operatorname{SSE}_s=(y-\overline{y})^T(y-\overline{y})-\hat{\gamma}_I^T\tilde{X}_c^Ty$，即$\hat{\sigma}^2=\hat{\sigma}_c^2=\hat{\sigma}_s^2$；
		\item $R=\tilde{X}_s^T\tilde{X}_s$，其中$R$为回归自变量之间的样本相关系数矩阵；
		\item $\operatorname{Cov}(\hat{\beta})=\sigma^2(X^TX)^{-1},\;\operatorname{Cov}(\hat{\beta}_I)=\operatorname{Cov}(\hat{\alpha}_I)$，且：
		\begin{equation*}
			\operatorname{Cov}(\hat{\alpha})=\sigma^2
			\begin{pmatrix}
				\dfrac{1}{n} & \mathbf{0} \\
				\mathbf{0} & (\tilde{X}_c^T\tilde{X}_c)^{-1}
			\end{pmatrix},\quad
			\operatorname{Cov}(\hat{\gamma})=\sigma^2
			\begin{pmatrix}
				\dfrac{1}{n} & \mathbf{0} \\
				\mathbf{0} & (\tilde{X}_s^T\tilde{X}_s)^{-1}
			\end{pmatrix}
		\end{equation*}
	\end{enumerate}
\end{theorem}
\begin{proof}
	(1)由\cref{theo:AATPositiveSemidefinite}可知$X^TX$是一个半正定阵。若存在不为零向量的$\alpha$使得$\alpha^TX^TX\alpha=0$，则有$||X\alpha||=0$，即$X\alpha=\mathbf{0}$。而$\operatorname{rank}(X)=p$，所以$\alpha=\mathbf{0}$，矛盾，于是$X^TX$是一个正定阵，由\cref{theo:PositiveDefinite}(6)可知它是可逆的。$\tilde{X}^T\tilde{X},\tilde{X}_c^T\tilde{X}_c$和$\tilde{X}_s^T\tilde{X}_s$类似可得。\par
	(2)由定义即可得到。\par
	(3)由(1)，对于中心化模型，此时的正则方程变为：
	\begin{gather*}
		\begin{pmatrix}
			\mathbf{1}_n & \tilde{X}_c \\
		\end{pmatrix}^T
		\begin{pmatrix}
			\mathbf{1}_n & \tilde{X}_c \\
		\end{pmatrix}\alpha=
		\begin{pmatrix}
			\mathbf{1}_n & \tilde{X}_c \\
		\end{pmatrix}^Ty \\
		\begin{pmatrix}
			\mathbf{1}_n^T \\
			\tilde{X}_c^T
		\end{pmatrix}
		\begin{pmatrix}
			\mathbf{1}_n & \tilde{X}_c \\
		\end{pmatrix}\alpha=
		\begin{pmatrix}
			\mathbf{1}_n & \tilde{X}_c \\
		\end{pmatrix}^Ty \\
		\begin{pmatrix}
			n & \mathbf{0} \\
			\mathbf{0} & \tilde{X}_c^T\tilde{X}_c
		\end{pmatrix}\alpha=
		\begin{pmatrix}
			\sum\limits_{i=1}^{n}y_i \\
			\tilde{X}_c^Ty
		\end{pmatrix} \\
		\begin{pmatrix}
			n\alpha_0  \\
			\tilde{X}_c^T\tilde{X}_c\alpha_I
		\end{pmatrix}=
		\begin{pmatrix}
			\sum\limits_{i=1}^{n}y_i \\
			\tilde{X}_c^Ty
		\end{pmatrix} 
	\end{gather*}
	也即：
	\begin{equation*}
		\hat{\alpha}_0=\overline{y},\quad\hat{\alpha}_I=(\tilde{X}_c^T\tilde{X}_c)^{-1}\tilde{X}_c^Ty
	\end{equation*}
	同理可得：
	\begin{equation*}
		\hat{\gamma}_0=\overline{y},\quad\hat{\gamma}_I=(\tilde{X}_s^T\tilde{X}_s)^{-1}\tilde{X}_s^Ty
	\end{equation*}
	由前述和\cref{prop:Transpose}(4)可得：
	\begin{align*}
		&\tilde{X}_c^T\tilde{X}_c=\tilde{X}^T\left(I_n-\dfrac{1}{n}\mathbf{1}_n\mathbf{1}_n^T\right)^T\tilde{X}_c=\tilde{X}^T\left(I_n-\dfrac{1}{n}\mathbf{1}_n\mathbf{1}_n^T\right)\tilde{X}_c \\
		=&\tilde{X}^T\tilde{X}_c-\tilde{X}^T\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T\tilde{X}_c=\tilde{X}^T\tilde{X}_c=\tilde{X}^T\left(I_n-\dfrac{1}{n}\mathbf{1}_n\mathbf{1}_n^T\right)\tilde{X} \\
		=&\tilde{X}^T\tilde{X}-\dfrac{1}{n}\tilde{X}^T\mathbf{1}_n\mathbf{1}_n^T\tilde{X}=\tilde{X}^T\tilde{X}-\dfrac{1}{n}n\overline{x}n\overline{x}^T=\tilde{X}^T\tilde{X}-n\overline{x}\overline{x}^T
	\end{align*}
	于是根据\cref{prop:InvertibleMatrix}(17)可得：
	\begin{align*}
		&\hat{\beta}=(X^TX)^{-1}X^Ty=
		\begin{pmatrix}
			n & \mathbf{1}_{n}^T\tilde{X} \\
			\tilde{X}^T\mathbf{1}_n & \tilde{X}^T\tilde{X}
		\end{pmatrix}^{-1}
		\begin{pmatrix}
			\mathbf{1}_n^Ty \\
			\tilde{X}^Ty
		\end{pmatrix} \\
		=&
		\begin{pmatrix}
			\dfrac{1}{n}+\dfrac{1}{n}\mathbf{1}_{n}^T\tilde{X}( \tilde{X}^T\tilde{X}-\tilde{X}^T\mathbf{1}_n\dfrac{1}{n}\mathbf{1}_{n}^T\tilde{X})^{-1}\tilde{X}^T\mathbf{1}_n\dfrac{1}{n} & -\dfrac{1}{n}\mathbf{1}_{n}^T\tilde{X}(\tilde{X}^T\tilde{X}-\tilde{X}^T\mathbf{1}_n\dfrac{1}{n}\mathbf{1}_{n}^T\tilde{X})^{-1} \\
			-(\tilde{X}^T\tilde{X}-\tilde{X}^T\mathbf{1}_n\dfrac{1}{n}\mathbf{1}_{n}^T\tilde{X})^{-1}\tilde{X}^T\mathbf{1}_n\dfrac{1}{n} & (\tilde{X}^T\tilde{X}-\tilde{X}^T\mathbf{1}_n\dfrac{1}{n}\mathbf{1}_{n}^T\tilde{X})^{-1}
		\end{pmatrix}
		\begin{pmatrix}
			\mathbf{1}_n^Ty \\
			\tilde{X}^Ty
		\end{pmatrix} \\
		=&
		\begin{pmatrix}
			\dfrac{1}{n}+\overline{x}^T(\tilde{X}^T\tilde{X}-\overline{x}n\overline{x}^T)^{-1}\overline{x} & -\overline{x}^T(\tilde{X}^T\tilde{X}-\overline{x}n\overline{x}^T)^{-1} \\
			-(\tilde{X}^T\tilde{X}-\overline{x}n\overline{x}^T)^{-1}\overline{x} & (\tilde{X}^T\tilde{X}-\overline{x}n\overline{x}^T)^{-1}
		\end{pmatrix}
		\begin{pmatrix}
			\mathbf{1}_n^Ty \\
			\tilde{X}^Ty
		\end{pmatrix} \\
		=&
		\begin{pmatrix}
			\dfrac{1}{n}+\overline{x}^T(\tilde{X}_c^T\tilde{X}_c)^{-1}\overline{x} & -\overline{x}^T(\tilde{X}_c^T\tilde{X}_c)^{-1} \\
			-(\tilde{X}_c^T\tilde{X}_c)^{-1}\overline{x} & (\tilde{X}_c^T\tilde{X}_c)^{-1}
		\end{pmatrix}
		\begin{pmatrix}
			\mathbf{1}_n^Ty \\
			\tilde{X}^Ty
		\end{pmatrix} \\
		=&
		\begin{pmatrix}
		\overline{y}+\overline{x}^T(\tilde{X}_c^T\tilde{X}_c)^{-1}\overline{x}\mathbf{1}_n^Ty-\overline{x}^T(\tilde{X}_c^T\tilde{X}_c)^{-1}\tilde{X}^Ty \\
		-(\tilde{X}_c^T\tilde{X}_c)^{-1}\overline{x}\mathbf{1}_n^Ty+(\tilde{X}_c^T\tilde{X}_c)^{-1}\tilde{X}^Ty
		\end{pmatrix} \\
		=&
		\begin{pmatrix}
		\overline{y}+\overline{x}^T(\tilde{X}_c^T\tilde{X}_c)^{-1}(\overline{x}\mathbf{1}_n^T-\tilde{X}^T)y \\
		(\tilde{X}_c^T\tilde{X}_c)^{-1}(\tilde{X}^T-\overline{x}\mathbf{1}_n^T)y
		\end{pmatrix}=
		\begin{pmatrix}
		\overline{y}-\overline{x}^T(\tilde{X}_c^T\tilde{X}_c)^{-1}\tilde{X}_c^Ty \\
		(\tilde{X}_c^T\tilde{X}_c)^{-1}\tilde{X}_c^Ty
		\end{pmatrix}
	\end{align*}
	所以有：
	\begin{equation*}
		\hat{\beta}_I=\hat{\alpha}_I,\quad\hat{\beta}_0=\overline{y}-\overline{x}^T\hat{\beta}_I
	\end{equation*}
	由前述、\cref{prop:Transpose}(4)和\cref{prop:InvertibleMatrix}(11)可得：
	\begin{align*}
		\hat{\gamma}_I&=\left[\left(\tilde{X}_c\operatorname{diag}\left\{\dfrac{1}{s_1},\dfrac{1}{s_2},\dots,\dfrac{1}{s_{p-1}}\right\}\right)^T\tilde{X}_c\operatorname{diag}\left\{\dfrac{1}{s_1},\dfrac{1}{s_2},\dots,\dfrac{1}{s_{p-1}}\right\}\right]^{-1} \\
		&\quad\cdot\left(\tilde{X}_c\operatorname{diag}\left\{\dfrac{1}{s_1},\dfrac{1}{s_2},\dots,\dfrac{1}{s_{p-1}}\right\}\right)^Ty \\
		&=\left(\operatorname{diag}\left\{\dfrac{1}{s_1},\dfrac{1}{s_2},\dots,\dfrac{1}{s_{p-1}}\right\}\tilde{X}_c^T\tilde{X}_c\operatorname{diag}\left\{\dfrac{1}{s_1},\dfrac{1}{s_2},\dots,\dfrac{1}{s_{p-1}}\right\}\right)^{-1} \\
		&\quad\cdot\operatorname{diag}\left\{\dfrac{1}{s_1},\dfrac{1}{s_2},\dots,\dfrac{1}{s_{p-1}}\right\}\tilde{X}_c^Ty \\
		=&\operatorname{diag}\{\seq{s}{p-1}\}(\tilde{X}_c^T\tilde{X}_c)^{-1}\operatorname{diag}\{\seq{s}{p-1}\}\operatorname{diag}\left\{\dfrac{1}{s_1},\dfrac{1}{s_2},\dots,\dfrac{1}{s_{p-1}}\right\}\tilde{X}_c^Ty  \\
		=&\operatorname{diag}\{\seq{s}{p-1}\}\hat{\alpha}_I
	\end{align*}
	将上述结果代入经验回归方程即可得到三种方程的等价性，同时：
	\begin{equation*}
		\tilde{X}_s\hat{\gamma}_I=\tilde{X}_c\operatorname{diag}\left\{\dfrac{1}{s_1},\dfrac{1}{s_2},\dots,\dfrac{1}{s_{p-1}}\right\}\operatorname{diag}\{\seq{s}{p-1}\}\hat{\alpha}_I=\tilde{X}_c\hat{\alpha}_I
	\end{equation*}\par
	(4)根据(3)可得：
	\begin{gather*}
		\begin{aligned}
			&\operatorname{SSE}_c=||y-\hat{\alpha}_0\mathbf{1}_n-\tilde{X}_c\hat{\alpha}_I||^2=||y-\hat{\beta}_0\mathbf{1}_n-\overline{x}^T\hat{\beta}_I\mathbf{1}_n-\tilde{X}_c\hat{\beta}_I||^2 \\
			=&||y-\hat{\beta}_0\mathbf{1}_n-\mathbf{1}_n\overline{x}^T\hat{\beta}_I-\tilde{X}_c\hat{\beta}_I||^2=||y-\hat{\beta}_0\mathbf{1}_n-\tilde{X}\hat{\beta}_I||^2=||y-X\hat{\beta}||^2=\operatorname{SSE}
		\end{aligned} \\
		\operatorname{SSE}_s=||y-\hat{\gamma}_0\mathbf{1}_n-\tilde{X}_s\hat{\gamma}_I||^2=||y-\hat{\alpha}_0\mathbf{1}_n-\tilde{X}_c\hat{\alpha}_I||^2=\operatorname{SSE}_c
	\end{gather*}
	于是$\operatorname{SSE}=\operatorname{SSE}_c=\operatorname{SSE}_s$，类似\cref{theo:SSESSEACalculate}可得中间两式，由\cref{theo:VarianceOfErrorTerm}可知后式成立。\par
	(5)设$R=(r_{ij})$，注意到：
	\begin{align*}
		r_{ij}&=\left(\frac{x_{1i}-\overline{x}_i}{s_i},\frac{x_{2i}-\overline{x}_i}{s_i},\dots,\frac{x_{ni}-\overline{x}_i}{s_i}\right)^T\left(\frac{x_{1j}-\overline{x}_j}{s_j},\frac{x_{2j}-\overline{x}_j}{s_j},\dots,\frac{x_{nj}-\overline{x}_j}{s_j}\right) \\
		&=\frac{1}{s_is_j}\sum_{k=1}^{n}(x_{ki}-\overline{x}_i)(x_{kj}-\overline{x}_j)=\frac{n-1}{s_is_j}\frac{1}{n-1}\sum_{k=1}^{n}(x_{ki}-\overline{x}_i)(x_{kj}-\overline{x}_j)=\widehat{\operatorname{Corr}}(\tilde{X})(i;j)
	\end{align*}\par
	(6)由\cref{prop:EstimableFunction}(5)和\cref{prop:CovMat}(3)可知$I_p\operatorname{Cov}(\hat{\beta})I_p=\sigma^2(X^TX)^{-1}$，即$\operatorname{Cov}(\hat{\beta})=\sigma^2(X^TX)^{-1}$。第二式由(3)第二式即可得到。后两式由对应的正则方程和\cref{prop:CovMat}(3)即可得到。
\end{proof}
\begin{note}
	由上述定理第三条又可以看出中心化和标准化能够使得常数项与回归系数的估计被分开，常数项的估计值直接就等于$y$的均值，并且由上述定理第五条可知对于中心化或标准化模型，常数项与回归系数的估计是不相关的，于是在理论分析中我们可以只关注回归系数的估计。标准化模型的另一个优势是可以消除变量量纲的影响，使得回归系数的估计可直接反应各自变量对因变量预测的重要程度。
\end{note}

\subsection{显著性检验与复相关系数}
\subsubsection{显著性检验}
\begin{definition}
	对于\cref{model:NormalLinearRegressionModel}，称假设检验：
	\begin{equation*}
		H_0:\beta_1=\beta_2=\cdots=\beta_{p-1}=0,\quad H_1:\text{至少有一个回归系数不为}0
	\end{equation*}
	为\textbf{回归方程的显著性检验}。称：
	\begin{equation*}
		H_0:\beta_i=0,\quad H_1:\beta_i\ne0
	\end{equation*}
	为\textbf{回归系数$\beta_i$的显著性检验}。
\end{definition}
\begin{theorem}\label{theo:LinearRegressionModelTSH}
	对于\cref{model:NormalLinearRegressionModel}，回归方程的显著性检验的统计量和拒绝域为：
	\begin{gather*}
		F=\frac{\hat{\beta}_I^T\tilde{X}^T\left(I_n-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T\right)y/(p-1)}{\operatorname{SSE}/(n-p)},\quad \{F:F>\operatorname{F}_{p-1,n-p}(1-\alpha)\} \\
		F=\frac{\hat{\alpha}_I^T\tilde{X}_c^Ty/(p-1)}{\operatorname{SSE}_c/(n-p)},\quad \{F:F>\operatorname{F}_{p-1,n-p}(1-\alpha)\} \\
		F=\frac{\hat{\gamma}_I^T\tilde{X}_s^Ty/(p-1)}{\operatorname{SSE}_s/(n-p)},\quad \{F:F>\operatorname{F}_{p-1,n-p}(1-\alpha)\}
	\end{gather*}
	回归系数$\beta_i$的显著性检验的统计量和拒绝域为：
	\begin{gather*}
		t=\frac{\hat{\beta}_i}{\sqrt{c_{ii}}\hat{\sigma}},\quad\left\{t:|t|>\operatorname{t}_{n-p}\left(1-\frac{\alpha}{2}\right)\right\}
	\end{gather*}
	其中$(X^TX)^{-1}=(c_{ij})$。
\end{theorem}
\begin{proof}
	(1)可以发现此时的假设检验即为在\cref{theo:NormalLinearModelHypothesisTesting}中取$A=(\mathbf{0},I_{p-1}),\;b=\mathbf{0}$时的情况。原假设下模型变为：
	\begin{equation*}
		y_i=\alpha_0+\varepsilon_i,\quad i=1,2,\dots,n
	\end{equation*}
		此时设计阵为$\mathbf{1}_n$，正则方程为$n\hat{\alpha}_0=n\overline{y}$，$\hat{\alpha}=\overline{y}$，由\cref{theo:SSESSEACalculate}可得$\operatorname{SSE}_A=y^Ty-\overline{y}\mathbf{1}_n^Ty=y^Ty-\overline{y}n\overline{y}=y^Ty-n\overline{y}^2$。备择假设下根据\cref{theo:LinearRegressionModel}(3)可得：
	\begin{equation*}
		\operatorname{SSE}_c=y^Ty-\hat{\alpha}^TX_c^Ty=y^Ty-\hat{\alpha}_0n\overline{y}-\hat{\alpha}_I^T\tilde{X}_c^Ty=y^Ty-n\overline{y}^2-\hat{\alpha}_I^T\tilde{X}_c^Ty
	\end{equation*}
	于是有：
	\begin{equation*}
		\operatorname{SSE}_A-\operatorname{SSE}_c=y^Ty-n\overline{y}^2-(y^Ty-n\overline{y}^2-\hat{\alpha}_I^T\tilde{X}_c^Ty)=\hat{\alpha}_I^T\tilde{X}_c^Ty
	\end{equation*}
	由\cref{theo:NormalLinearModelHypothesisTesting}(5)即可得出第二式。由$\tilde{X}_c=\left(I_n-\dfrac{1}{n}\mathbf{1}_n\mathbf{1}_n^T\right)\tilde{X}$和\cref{theo:LinearRegressionModel}(3)(4)即可得剩下两式。\par
	(2)仿照\cref{prop:NormalLinearModel}(2)的证明过程可知：
	\begin{equation*}
		\hat{\beta}\sim\operatorname{N}_p[\beta,\sigma^2(X^TX)^{-1}]
	\end{equation*}
	记$(X^TX)^{-1}=(c_{ij})$，由\cref{prop:MultiNormal}(3)可得：
	\begin{equation*}
		\hat{\beta}_i\sim\operatorname{N}(\beta_i,\sigma^2c_{ii})
	\end{equation*}
	所以当$H_0$成立时有：
	\begin{equation*}
		\frac{\hat{\beta}_i}{\sigma\sqrt{c_{ii}}}\sim\operatorname{N}(0,1)
	\end{equation*}
	由\cref{prop:NormalLinearModel}(2)可知：
	\begin{equation*}
		\frac{(n-p)\hat{\sigma}^2}{\sigma^2}\sim\chi_{n-p}^2
	\end{equation*}
	所以：
	\begin{equation*}
		\frac{\hat{\beta}_i}{\sigma\sqrt{c_{ii}}}\Big/\sqrt{\frac{(n-p)\hat{\sigma}^2}{\sigma^2(n-p)}}=\frac{\hat{\beta}_i}{\sqrt{c_{ii}}\hat{\sigma}}\sim t_{n-p}\qedhere
	\end{equation*}
\end{proof}
\begin{note}
	为什么给了三种回归方程的显著性检验统计量但只给了一种回归系数的显著性检验统计量？可以发现三种回归方程的显著性检验统计量其实是一样的，但如果按照回归系数的显著性检验统计量的推导给出中心化和标准化模型时的情况，会发现它们的值是不一样的，可以自行验证一下。
\end{note}
\subsubsection{复相关系数}
\begin{note}
	复相关系数度量的是回归自变量和因变量之间的最大相关系数\info{以后给出证明并联系多元统计中的典型相关分析，修改前述复相关系数的叙述与此处的叙述}，由前述，当选用中心化模型或者标准化模型后我们可以把常数项剥离出去，这样就可以只剩下因变量与回归自变量了。由\cref{prop:Trace}(4)和\cref{theo:LinearRegressionModel}(3)可知此时的回归平方和应该修改为：
	\begin{equation*}
		\operatorname{RSS}(\beta)=\hat{\beta}_I^T\tilde{X}^T(y-\overline{y})=\hat{\beta}_I^T\tilde{X}^T\left(I_n-\frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T\right)y=\hat{\alpha}_I^T\tilde{X}_c^Ty=\hat{\gamma}_I^T\tilde{X}_s^Ty
	\end{equation*}
	总平方和为：
	\begin{equation*}
		\operatorname{RSS}_{\text{总}}=(y-\overline{y})^T(y-\overline{y})
	\end{equation*}
	由\cref{prop:Trace}(4)和\cref{theo:LinearRegressionModel}(3)可得线性回归模型的复相关系数公式为：
	\begin{equation*}
		R^2=\frac{\hat{\alpha}_I^T\tilde{X}_c^Ty}{(y-\overline{y})^T(y-\overline{y})}=\frac{\hat{\gamma}_I^T\tilde{X}_s^Ty}{(y-\overline{y})^T(y-\overline{y})}
	\end{equation*}\info{补充调整后$R^2$}
\end{note}
\begin{theorem}\label{theo:LinearRegressionModelFR2}
	对于\cref{model:CentralizedStandardizedLinearModel}，回归方程的显著性检验的统计量$F$与复相关系数$R^2$有如下关系：
	\begin{equation*}
		F=\frac{R^2}{1-R^2}\frac{n-p}{p-1}
	\end{equation*}
\end{theorem}
\begin{proof}
	由\cref{theo:LinearRegressionModelTSH}(1)中$\operatorname{SSE}_c$的公式可得：
	\begin{align*}
		F&=\frac{\hat{\alpha}_I^T\tilde{X}_c^Ty/(p-1)}{\operatorname{SSE}_c/(n-p)}=\frac{\hat{\alpha}_I^T\tilde{X}_c^Ty/[\operatorname{RSS}_{\text{总}}(p-1)]}{[(y-\overline{y})^T(y-\overline{y})-\hat{\alpha}_I^T\tilde{X}_c^Ty]/[\operatorname{RSS}_{\text{总}}(n-p)]} \\
		&=\frac{R^2/(p-1)}{(1-R^2)/(n-p)}=\frac{R^2}{1-R^2}\frac{n-p}{p-1}
	\end{align*}
	类似可得标准化模型时的结果。
\end{proof}

\subsection{模型选择}
\begin{theorem}
	当全模型正确时：
	\begin{enumerate}
		\item 剔除一部分自变量后，可使得剩余的那部分自变量的回归系数的LSE的方差减小，但此时的估计一般为有偏估计。若被剔除的自变量对因变量影响较小，则可使得剩余的那部分自变量的回归系数的LSE的MSE减小；
		\item 若用选模型作预测，预测一般是有偏的，但预测偏差的方差减小。若被剔除的自变量对因变量影响较小，则可使得预测的MSE减小。
	\end{enumerate}
\end{theorem}
\subsubsection{RMSq准则}
\subsubsection{Cp准则}
\subsubsection{AIC准则}
\subsubsection{模型选择方法}
\begin{method}[Best subset selection]
	对所有可能的自变量组合拟合线性回归模型并选定一个模型指标，选择指标最优的变量组合。
\end{method}
\begin{method}[Forward Selection]
	从不包含任何自变量的模型开始，选择与因变量相关系数的绝对值最大且引入模型后能够通过回归系数的显著性检验的自变量，将之添加入模型中，迭代至终止条件，如达到最大自变量个数或没有自变量能够通过回归系数的显著性检验等。
\end{method}
\begin{method}[Backward Selection]
	从全模型开始，选择回归系数的显著性检验中$t$统计量绝对值最小的变量，若它显著，则终止，认为最优模型为全模型；若它不显著，则删除该变量。迭代至终止条件，如达到最小自变量个数或所有剩余的自变量都能够通过回归系数的显著性检验。
\end{method}
\begin{method}[Stepwise]
	从不包含任何自变量的模型开始，选择与因变量相关系数的绝对值最大且引入模型后能够通过回归系数的显著性检验的自变量，将之添加入模型中。在剩余自变量中选择与因变量相关系数的绝对值最大且引入模型后能够通过回归系数的显著性检验的自变量，将之添加入模型中。对这两个自变量进行回归系数的显著性检验，删除不显著的变量，再从排除这两个自变量后剩余的自变量中按照前述规则添加一个自变量，然后按照前述规则删除相应的变量。迭代至终止条件。
\end{method}

\subsection{回归诊断}
\subsubsection{残差分析}
\begin{definition}
	称以某种残差为纵坐标、其它量为横坐标的散点图为\gls{ResidualPlot}。
\end{definition}
\begin{definition}
	当模型为\cref{model:NormalLinearRegressionModel}时，将：
	\begin{equation*}
		r_i=\frac{\hat{\varepsilon}_i}{\sqrt{\hat{\sigma}^2(1-p_{ii})}}
	\end{equation*}
	称为\gls{StudentizedResidual}。由\cref{prop:ehat}(1)可知它相当于是对$\hat{\varepsilon}$进行标准化然后用$\sigma^2$的估计$\hat{\sigma}^2$进行替换后得到的值。
\end{definition}
\begin{note}
	应用上可以近似地认为$r_i$相互独立且服从$\operatorname{N}(0,1)$。若将$\hat{\varepsilon}$与$\hat{y}$画作二维散点图，以$\hat{y}$为横坐标、$r_i$为纵坐标，点的纵坐标应大致分布于$[-2,2]$，且由\cref{prop:ehat}(3)和\cref{prop:MultiNormal}(2)(8)可得这些点不应呈现出任何趋势（$\hat{y}$与$\hat{\varepsilon}_i$独立）。若不满足上述现象，则此时模型有问题，可能是漏掉了重要的回归自变量，也可能是数据不满足\cref{model:NormalLinearRegressionModel}中的假设。在数据不满足\cref{model:NormalLinearRegressionModel}中的假设时，一个解决方案是对$y$实施变换，使得变换后得到的向量与回归自变量之间存在线性相关关系，同时让误差也符合假设，一个在$y$值全部为正数时使用广泛的变换就是下述的Box-Cox变换。
\end{note}
\subsubsection{Box-Cox变换}
\begin{definition}
	Box-Cox变换是对各分量都为正数的$y$进行如下变换：
	\begin{equation*}
		y_i^{(\lambda)}=
		\begin{cases}
			\dfrac{y_i^{\lambda}-1}{\lambda},&\lambda\ne0 \\
			\ln y_i,&\lambda=0
		\end{cases}
	\end{equation*}
	其中$\lambda$是一个待定的变换参数。
\end{definition}
\begin{note}
	可以看出Box-Cox变换是一个变换族，包括了对数变换、平方根变换和倒数变换等。确定$\lambda$的值的方式为极大似然估计法，选择$\lambda$值使得$y^{(\lambda)}\sim\operatorname{N}(X\beta,\sigma^2I_n)$最有可能发生，这一方法的选择是很直观的。实际操作时若$y$有分量非正，可通过平移将其变换为全部为正值的情况，然后再进行Box-Cox变换。下面解释实际计算。\par
	对于固定的$\lambda$，$\beta$和$\sigma^2$的似然函数为：
	\begin{equation*}
		L(\beta,\sigma^2,\lambda)=\frac{1}{(2\pi)^{\frac{n}{2}}(\sigma^2)^\frac{n}{2}}\exp\left(-\frac{1}{2\sigma^2}||y^{(\lambda)}-X\beta||^2\right)|\mathbf{J}|
	\end{equation*}
	$\mathbf{J}$为变换的Jacobi行列式：
	\begin{equation*}
		\mathbf{J}=\left|\frac{\dif y_i^{(\lambda)}}{\dif y_i}\right|=\prod_{i=1}^{n}y_i^{\lambda-1}
	\end{equation*}
	由\cref{prop:NormalLinearModel}(1)可知此时$\beta$和$\sigma^2$的极大似然估计为：
	\begin{equation*}
		\hat{\beta}(\lambda)=(X^TX)^{-1}X^Ty^{(\lambda)},\quad\hat{\sigma}^2(\lambda)=\frac{1}{n}\operatorname{SSE}(\lambda)
	\end{equation*}
	对应的对数似然函数最大值为：
	\begin{equation*}
		\ln L_{\text{max}}(\lambda)=-\frac{n}{2}\ln2\pi-\frac{n}{2}\ln\frac{\operatorname{SSE}(\lambda)}{n}-\frac{1}{2}\frac{n}{\operatorname{SSE}(\lambda)}\operatorname{SSE}(\lambda)+\ln\mathbf{J}
	\end{equation*}
	由\cref{prop:ehat}(2)可知略去与$\lambda$无关的项之后的对数似然函数最大值为：
	\begin{align*}
		\ln L_{\text{max}}(\lambda)&=-\frac{n}{2}\ln \operatorname{SSE}(\lambda)+\sum_{i=1}^{n}\ln y_i^{\lambda-1}=-\frac{n}{2}\ln\{[y^{(\lambda)}]^T(I_n-P_X)y^{(\lambda)}\}+\frac{n}{2}\ln\mathbf{J}^{\frac{2}{n}} \\
		&=-\frac{n}{2}\ln\left\{\frac{[y^{(\lambda)}]^T}{\mathbf{J}^{\frac{1}{n}}}(I_n-P_X)\frac{y^{(\lambda)}}{\mathbf{J}^{\frac{1}{n}}}\right\}
	\end{align*}
	可以看出$-\dfrac{n}{2}$也可略去，因为对数函数是单调函数，所以它也可以略去，于是只需解：
	\begin{equation*}
		\lambda=\underset{\lambda}{\arg\min}\frac{[y^{(\lambda)}]^T}{\mathbf{J}^{\frac{1}{n}}}(I_n-P_X)\frac{y^{(\lambda)}}{\mathbf{J}^{\frac{1}{n}}}
	\end{equation*}
	记$z^{(\lambda)}=\dfrac{y^{(\lambda)}}{\mathbf{J}^{\frac{1}{n}}}$，则：
	\begin{equation*}
		z_i^{(\lambda)}=
		\begin{cases}
			\dfrac{y_i^{\lambda}-1}{\lambda\left(\prod\limits_{i=1}^{n}y_i\right)^{\frac{\lambda-1}{n}}},&\lambda\ne0 \\
			\dfrac{\ln y_i}{\left(\prod\limits_{i=1}^{n}y_i\right)^{\frac{\lambda-1}{n}}},&\lambda=0
		\end{cases}
	\end{equation*}
	$\lambda$的解析表达式一般来讲很难得到，但我们可以求解数值解，即对不同的$\lambda$计算$z^{(\lambda)}$和$I_n-P_X$。为了便于计算机计算，可预先计算的量为：
	\begin{equation*}
		I_n-P_X,\quad\sqrt[n]{\prod\limits_{i=1}^{n}y_i}
	\end{equation*}
\end{note}
\begin{algorithm}[H]
	\caption{Box-Cox 变换的计算机求解}
	\begin{algorithmic}[1]
		\Require 观测向量 $y = (y_1,\dots,y_n)^T$，设计矩阵 $X$，候选参数集合 $\Lambda$
		\Ensure 最优参数 $\hat{\lambda}$和变换后的向量$y^{(\lambda)}$
		
		\State \textbf{预处理:} 若存在 $y_i \le 0$，则对 $y$ 作平移: $y \gets y + c$，其中 $c > -\min(y_i)$
		
		\State \textbf{计算预备量:} 
		\Statex \quad 1) $I_n - P_X$
		\Statex \quad 2) 几何平均数 $g = \sqrt[n]{\prod\limits_{i=1}^n y_i}$
		
		\For{$\lambda \in \Lambda$}
		\If{$\lambda \ne 0$}
		\State 计算变换向量$z^{(\lambda)}$，其中：
		\begin{equation*}
			z_i^{(\lambda)}=\dfrac{y_i^\lambda-1}{\lambda g^{\lambda-1}},\;i=1,2,\dots,n
		\end{equation*}
		\Else
		\State 计算变换向量$z^{(\lambda)}$，其中：
		\begin{equation*}
			z_i^{(\lambda)}=\dfrac{\ln y_i}{g^{\lambda-1}},\;i=1,2,\dots,n
		\end{equation*}
		\EndIf
		
		\State 计算误差平方和:
		\[
		\operatorname{SSE}(\lambda) = [z^{(\lambda)}]^T (I_n - P_X) z^{(\lambda)}
		\]
		\EndFor
		
		\State \textbf{选择最优参数:}
		\[
		\hat{\lambda} = \arg\min_{\lambda \in \Lambda}\operatorname{SSE}(\lambda)
		\]
		\State \Return $\hat{\lambda}, \; y^{(\hat{\lambda})}$
	\end{algorithmic}
\end{algorithm}
\subsubsection{影响分析}
影响分析即为探查对估计或预测有较大影响的数据。
\begin{definition}
	定义第$i$个样本\textbf{Cook统计量}为：
	\begin{equation*}
		D_i=\frac{(\hat{\beta}-\hat{\beta}_{(i)})^TX^TX(\hat{\beta}-\hat{\beta}_{(i)})}{p\hat{\sigma}^2},\quad i=1,2,\dots,n
	\end{equation*}
	其中$\hat{\beta}_{(i)}$表示删除第$i$个样本后拟合得到的回归系数。
\end{definition}
\begin{note}
	从\cref{theo:NormalLinearModelConfidenceEllipsoid}可以看出：
	\begin{equation*}
		\left\{\beta:\frac{(\hat{\beta}-\beta)X^TX(\hat{\beta}-\beta)}{p\hat{\sigma}^2}\leqslant\operatorname{F}_{p,n-p}(\alpha)\right\}
	\end{equation*}
	是拒绝假设$\beta_0=\beta_1=\cdots=\beta_{p-1}=0$后$\beta$置信水平为$1-\alpha$的置信椭球。当$\beta$被换为$\hat{\beta}_{(i)}$时，$D_i$的值就对应着$\hat{\beta}_{(i)}$在$\hat{\beta}$置信椭球上的置信水平，$D_i$的值越大，$\alpha$就越大，置信水平就越小，也就是说$D_i$越大$\hat{\beta}_{(i)}$对应在$\hat{\beta}$的置信水平越小的置信椭球，加入第$i$个样本后得到的$\hat{\beta}$对应的置信水平越大，这往往代表着$\hat{\beta}$离样本中心越远（置信水平越大往往置信域越大，而置信域中心的点一般是均值的估计），即第$i$个样本对估计或预测造成的影响越大。\par
	上述公式的一个问题是需要计算$n+1$次回归的结果，下面的结论给出了一个非常便捷的Cook统计量的计算公式。
\end{note}
\begin{theorem}\label{theo:CookStatistic}
	$D_i$有如下计算公式：
	\begin{equation*}
		D_i=\frac{1}{p}\left(\frac{p_{ii}}{1-p_{ii}}\right)r_i^2,\quad i=1,2,\dots,n
	\end{equation*}
	其中$r_i$是学生化残差，$p_{ii}$是矩阵$P_X$的第$i$个主对角元。
\end{theorem}
\begin{proof}
	注意到恒等式（两边同乘$A-uv^T$即可）：
	\begin{equation*}
		(A-uv^T)^{-1}=A^{-1}+\frac{A^{-1}uv^TA^{-1}}{1-u^TA^{-1}v}
	\end{equation*}
	其中$A\in M_{n}(K)$且可逆，$u,v\in M_{n\times 1}(K)$。记$X_{(i)}$为去除第$i$个样本后的设计阵，$x_i^T$为$X$的第$i$行，由\cref{prop:MatrixMultiplication}(1)的第四种理解可得：
	\begin{equation*}
		(X_{(i)}^TX_{(i)})^{-1}=(X^TX-x_ix_i^T)^{-1}=(X^TX)^{-1}+\frac{(X^TX)^{-1}x_ix_i^T(X^TX)^{-1}}{1-p_{ii}}
	\end{equation*}
	将上式两边同乘$X^Ty$，记$y_{(i)}$为$y$去掉第$i$个分量后的向量，利用$X^Ty=X_{(i)}^Ty_{(i)}+y_ix_i$可得：
	\begin{gather*}
		(X_{(i)}^TX_{(i)})^{-1}X^Ty=(X^TX)^{-1}X^Ty+\frac{(X^TX)^{-1}x_ix_i^T(X^TX)^{-1}X^Ty}{1-p_{ii}} \\
		(X_{(i)}^TX_{(i)})^{-1}X_{(i)}^Ty_{(i)}+(X_{(i)}^TX_{(i)})^{-1}y_ix_i=\hat{\beta}+\frac{(X^TX)^{-1}x_ix_i^T\hat{\beta}}{1-p_{ii}} \\
		\hat{\beta}_{(i)}+(X_{(i)}^TX_{(i)})^{-1}y_ix_i=\hat{\beta}+\frac{(X^TX)^{-1}x_ix_i^T\hat{\beta}}{1-p_{ii}}
	\end{gather*}
	再将原式两边同乘$x_i$可得：
	\begin{align*}
		&(X_{(i)}^TX_{(i)})^{-1}x_i=(X^TX)^{-1}x_i+\frac{(X^TX)^{-1}x_ix_i^T(X^TX)^{-1}x_i}{1-p_{ii}} \\
		=&\frac{(1-p_{ii})(X^TX)^{-1}x_i+(X^TX)^{-1}x_ix_i^T(X^TX)^{-1}x_i}{1-p_{ii}} \\
		=&\frac{[1-x_i^T(X^TX)^{-1}x_i](X^TX)^{-1}x_i+(X^TX)^{-1}x_ix_i^T(X^TX)^{-1}x_i}{1-p_{ii}} \\
		=&\frac{(X^TX)^{-1}x_i-x_i^T(X^TX)^{-1}x_i(X^TX)^{-1}x_i+(X^TX)^{-1}x_ix_i^T(X^TX)^{-1}x_i}{1-p_{ii}}=\frac{(X^TX)^{-1}x_i}{1-p_{ii}}
	\end{align*}
	代入到上一组公式中可得：
	\begin{gather*}
		\hat{\beta}_{(i)}+\frac{(X^TX)^{-1}x_iy_i}{1-p_{ii}}=\hat{\beta}+\frac{(X^TX)^{-1}x_ix_i^T\hat{\beta}}{1-p_{ii}} \\
		\hat{\beta}_{(i)}+\frac{(X^TX)^{-1}x_iy_i}{1-p_{ii}}=\hat{\beta}+\frac{(X^TX)^{-1}x_i\hat{y}_i}{1-p_{ii}} \\
		\hat{\beta}-\hat{\beta}_{(i)}=\frac{(X^TX)^{-1}x_i(y_i-\hat{y}_i)}{1-p_{ii}}=\frac{(X^TX)^{-1}x_i\hat{\varepsilon}_i}{1-p_{ii}} 
	\end{gather*}
	将上式代入到Cook统计量的定义中，由\cref{prop:Transpose}(4)和\cref{prop:InvertibleMatrix}(12)可得到：
	\begin{align*}
		&D_i=\left[\frac{(X^TX)^{-1}x_i\hat{\varepsilon}_i}{1-p_{ii}}\right]^T(X^TX)\left[\frac{(X^TX)x_i\hat{\varepsilon}_i}{1-p_{ii}}\right]\frac{1}{p\hat{\sigma}^2} \\
		=&\frac{1}{p}\frac{x_i^T(X^TX)^{-1}X^TX(X^TX)^{-1}x_i}{1-p_{ii}}\frac{\hat{\varepsilon}_i^2}{\hat{\sigma}^2(1-p_{ii})} \\
		=&\frac{1}{p}\frac{x_i^T(X^TX)^{-1}x_i}{1-p_{ii}}r_i^2=\frac{1}{p}\frac{p_{ii}}{1-p_{ii}}r_i^2\qedhere
	\end{align*}
\end{proof}
\subsubsection{异常点检测}
\begin{note}
	异常点检测即检测数据中是否存在某一个点不符合该组数据的模式，直观上来想一个残差值或学生化残差值较大的点是一个异常点，从建模的角度来看，异常点的特征构成不同于其它点，由此我们引入下述的均值漂移线性回归模型。判断一个点是不是异常点应是一个假设检验问题，我们需要找到一种方法来进行这种检验。我们只讨论数据中最多仅有一个异常点的情况，由于异常点会使得回归方程变形，所以多个异常点的识别是一个很复杂的问题。
\end{note}
\begin{definition}\label{model:MeanShiftLinearRegressionModel}
	称：
	\begin{equation*}
		y=(X,d_j)(\beta^T,\eta)^T,\quad\varepsilon\sim\operatorname{N}_n(\mathbf{0},\sigma^2I_n)
	\end{equation*}
	为\gls{MeanShiftLinearRegressionModel}，其中$d_j$为第$j$个分量为$1$其余分量为$0$的$n$维向量，$(X,d_j)$列满秩。
\end{definition}
\begin{theorem}\label{theo:MeanShiftLinearRegressionModel}
	对于\cref{model:MeanShiftLinearRegressionModel}，有：
	\begin{enumerate}
		\item $\beta$的LSE$\beta^*$和$\eta$的LSE$\eta^*$分别为：
		\begin{equation*}
			\beta^*=\hat{\beta}_{(j)},\quad\eta^*=\frac{\hat{\varepsilon}_j}{1-p_{jj}}
		\end{equation*}
		$\hat{\beta}_{(j)}$为\cref{model:LinearRegressionModel}去除第$j$组数据后得到的$\beta$的LSE，$p_{ii}$是矩阵$P_X$的第$i$个主对角元。；
		\item 对于假设检验问题$H_0:\eta=0,\;H_1:\eta\ne0$，检验统计量和拒绝域为：
		\begin{equation*}
			F=\frac{(n-p-1)r_j^2}{n-p-r_j^2},\quad\{F;F>\operatorname{F}_{1,n-p-1}(1-\alpha)\}
		\end{equation*}
		或：
		\begin{equation*}
			t=\left[\frac{(n-p-1)r_j^2}{n-p-r_j^2}\right]^{\frac{1}{2}},\quad\{t:|t|>\operatorname{t}_{n-p-1}\left(1-\frac{\alpha}{2}\right)\}
		\end{equation*}
		其中$r_i$是学生化残差。
	\end{enumerate}
\end{theorem}
\begin{proof}
	(1)由\cref{theo:LinearRegressionModel}(1)可知$(X,d_j)^T(X,d_j)$可逆，于是此时的正则方程变为：
	\begin{equation*}
		\begin{pmatrix}
			\beta^* \\
			\eta^*
		\end{pmatrix}=[(X,d_j)^T(X,d_j)]^{-1}(X,d_j)^Ty=
		\begin{pmatrix}
			X^TX & x_j \\
			x_j^T & 1
		\end{pmatrix}^{-1}
		\begin{pmatrix}
			X^Ty \\
			y_j
		\end{pmatrix}
	\end{equation*}
	由\cref{theo:LinearRegressionModel}(1)可知$X^TX$可逆，设$x_j^T$为$X$的第$j$行，于是根据\cref{prop:InvertibleMatrix}(17)可得：
	\begin{align*}
	&\begin{pmatrix}
		\beta^* \\
		\eta^*
	\end{pmatrix} \\
	=&\begin{pmatrix}
		(X^TX)^{-1}+(X^TX)^{-1}x_j[1-x_j^T(X^TX)^{-1}x_j]^{-1}x_j^T(X^TX)^{-1} & -(X^TX)^{-1}x_j[1-x_j^T(X^TX)^{-1}x_j]^{-1} \\
		-[1-x_j^T(X^TX)^{-1}x_j]^{-1}x_j^T(X^TX)^{-1} & [1-x_j^T(X^TX)^{-1}x_j]^{-1}
	\end{pmatrix}
	\begin{pmatrix}
		X^Ty \\
		y_j
	\end{pmatrix} \\
	=&
	\begin{pmatrix}
		(X^TX)^{-1}X^Ty+(X^TX)^{-1}x_j[1-x_j^T(X^TX)^{-1}x_j]^{-1}x_j^T(X^TX)^{-1}X^Ty-(X^TX)^{-1}x_j[1-x_j^T(X^TX)^{-1}x_j]^{-1}y_j \\
		-[1-x_j^T(X^TX)^{-1}x_j]^{-1}x_j^T(X^TX)^{-1}X^Ty+[1-x_j^T(X^TX)^{-1}x_j]^{-1}y_j
	\end{pmatrix} \\
	=&
	\begin{pmatrix}
		\hat{\beta}+(X^TX)^{-1}x_j\dfrac{1}{1-p_{jj}}x_j^T\hat{\beta}-(X^TX)^{-1}x_j\dfrac{1}{1-p_{jj}}y_j \\
		-\dfrac{1}{1-p_{jj}}x_j^T\hat{\beta}+\dfrac{1}{1-p_{jj}}y_j
	\end{pmatrix} \\
	=&
	\begin{pmatrix}
		\hat{\beta}+\dfrac{1}{1-p_{jj}}(X^TX)^{-1}x_j\hat{y}_j-\dfrac{1}{1-p_{jj}}(X^TX)^{-1}x_jy_j \\
		-\dfrac{1}{1-p_{jj}}\hat{y}_j+\dfrac{1}{1-p_{jj}}y_j
	\end{pmatrix} \\
	=&
	\begin{pmatrix}
		\hat{\beta}-\dfrac{1}{1-p_{jj}}(X^TX)^{-1}x_j\hat{\varepsilon}_j \\
		\dfrac{\hat{\varepsilon}_j}{1-p_{jj}}
	\end{pmatrix}
	\end{align*}
	由\cref{theo:CookStatistic}证明中倒数第二组公式即可得出结论。\par
	(2)可以发现此时的假设检验问题即为在\cref{theo:NormalLinearModelHypothesisTesting}中取：
	\begin{equation*}
		A=
		\begin{pmatrix}
			\mathbf{0} & \mathbf{0} \\
			\mathbf{0} & 1
		\end{pmatrix}
	\end{equation*}
	和$b=\mathbf{0}$时的情况，此时$\operatorname{SSE}_A=y^Ty-\hat{\beta}^TX^Ty$，由(1)、\cref{prop:Transpose}(4)、\cref{prop:InvertibleMatrix}(12)、\cref{theo:SSESSEACalculate}和\cref{theo:VarianceOfErrorTerm}可得：
	\begin{align*}
		&\operatorname{SSE}=y^Ty-\Big((\beta^*)^T,\eta^*\Big)(X,d_j)^Ty=y^Ty-(\beta^*)^TX^Ty-\eta^*d_j^Ty \\
		=&y^Ty-\left[\hat{\beta}-\frac{1}{1-p_{jj}}(X^TX)^{-1}x_j\hat{\varepsilon}_j\right]^TX^Ty-\frac{\hat{\varepsilon}_j}{1-p_{jj}}y_j \\
		=&y^Ty-\hat{\beta}^TX^Ty+\frac{\hat{\varepsilon}_j}{1-p_{jj}}x_j^T(X^TX)^{-1}X^Ty-\frac{\hat{\varepsilon}_j}{1-p_{jj}}y_j \\
		=&y^Ty-\hat{\beta}^TX^Ty+\frac{\hat{\varepsilon}_j}{1-p_{jj}}x_j^T\hat{\beta}-\frac{\hat{\varepsilon}_j}{1-p_{jj}}y_j \\
		=&y^Ty-\hat{\beta}^TX^Ty+\frac{\hat{\varepsilon}_j}{1-p_{jj}}\hat{y}_j-\frac{\hat{\varepsilon}_j}{1-p_{jj}}y_j \\
		=&y^Ty-\hat{\beta}^TX^Ty-\frac{\hat{\varepsilon}_j^2}{1-p_{jj}}=(n-p)\hat{\sigma}^2-\frac{\hat{\varepsilon}_j^2}{1-p_{jj}}
	\end{align*}
	于是可得：
	\begin{equation*}
		\operatorname{SSE}_A-\operatorname{SSE}=y^Ty-\hat{\beta}^TX^Ty-y^Ty+\hat{\beta}^TX^Ty+\frac{\hat{\varepsilon}_j^2}{1-p_{jj}}=\frac{\hat{\varepsilon}_j^2}{1-p_{jj}}
	\end{equation*}
	由\cref{theo:NormalLinearModelHypothesisTesting}(4)可得：
	\begin{equation*}
		\frac{\operatorname{SSE}_A-\operatorname{SSE}}{\operatorname{SSE}/(n-p-1)}=\dfrac{\dfrac{(n-p-1)\hat{\varepsilon}_j^2}{1-p_{jj}}}{(n-p)\hat{\sigma}^2-\dfrac{\hat{\varepsilon}_j^2}{1-p_{jj}}}=\dfrac{\dfrac{(n-p-1)\hat{\varepsilon}_j^2}{\hat{\sigma}^2(1-p_{jj})}}{n-p-\dfrac{\hat{\varepsilon}_j^2}{\hat{\sigma}^2(1-p_{jj})}}=\frac{(n-p-1)r_j^2}{n-p-r_j^2}\sim\operatorname{F}_{1,n-p-1}
	\end{equation*}
	拒绝域为$\{F;F>\operatorname{F}_{1,n-p-1}(1-\alpha)\}$。由\cref{prop:FDistribution}(2)可得$t$检验时的情况。
\end{proof}

\subsection{复共线性与解决方案}
\begin{note}
	由\cref{theo:LinearRegressionModel}(1)可知$X^TX$正定且可逆，于是存在正交矩阵$Q$和$\seq{\lambda}{p}$使得：
	\begin{equation*}
		X^TX=Q^T\operatorname{diag}\{\seq{\lambda}{p}\}Q
	\end{equation*}
	其中$\lambda_i>0$。由\cref{prop:MSE}(1)、\cref{prop:EstimableFunction}(4)(5)、\cref{prop:Trace}(2)、\cref{prop:InvertibleMatrix}(11)、\cref{prop:OrthogonalUnitaryMatrix}(1)和\cref{prop:Trace}(3)可得：
	\begin{align*}
		\operatorname{MSE}(\hat{\beta})&=\operatorname{tr}[\sigma^2(X^TX)^{-1}]=\sigma^2\operatorname{tr}(X^TX)^{-1}=\sigma^2\operatorname{tr}
		\left(Q^T\operatorname{diag}\left\{\frac{1}{\lambda_1},\frac{1}{\lambda_2},\dots,\frac{1}{\lambda_{p}}\right\}Q\right) \\
		&=\sigma^2\operatorname{tr}
		\left(\operatorname{diag}\left\{\frac{1}{\lambda_1},\frac{1}{\lambda_2},\dots,\frac{1}{\lambda_{p}}\right\}QQ^T\right)=\sigma^2\sum_{i=1}^{p}\frac{1}{\lambda_i}
	\end{align*}
	当$X^TX$的一个特征根很小的时候，会导致$\operatorname{MSE}(\hat{\beta})=\operatorname{tr}\operatorname{Cov}(\hat{\beta})$很大。\par
	从另一个角度来看，由\cref{prop:MSE}(2)可得：
	\begin{equation*}
		\operatorname{E}(\hat{\beta}^T\hat{\beta})=\beta^T\beta+\sigma^2\sum_{i=1}^{p}\frac{1}{\lambda_i}
	\end{equation*}
	当$X^TX$的一个特征根很小的时候，会导致$\operatorname{E}(\hat{\beta}^T\hat{\beta})$比$\beta^T\beta$大很多，也就是$\hat{\beta}$的平均长度比$\beta$大很多。\par
	综上所述，当$X^TX$的一个特征根很小的时候，线性回归模型的最小二乘估计不再是一个好的估计。\par
	设$\lambda$是$X^TX$的一个特征值，$\alpha$是其对应的特征向量。若$\lambda\approx0$，则：
	\begin{gather*}
		X^TX\alpha=\lambda\alpha\approx\mathbf{0} \\
		\alpha^TX^TX\alpha=\lambda\alpha^T\alpha\approx0
	\end{gather*}
	也即$||X\alpha||\approx0$，$X\alpha\approx\mathbf{0}$，这等价于$X$的各列之间存在近似的线性相关关系，我们称之为\gls{Multicollinearity}关系。\par
	度量复共线性严重程度的一个重要量是$X^TX$的条件数\info{数值分析条件数}：
	\begin{enumerate}
		\item $k<100$：复共线性程度很小；
		\item $100\leqslant k\leqslant1000$：存在中等程度或较强的复共线性；
		\item $k>1000$：存在严重的复共线性。
	\end{enumerate}
	另一个可以度量复共线性严重程度的指标为\gls{VIF}：定义$\beta_i$的方差膨胀因子为$(X^TX)^{-1}(i,i)$，于是方差膨胀因子越大$\operatorname{MSE}(\hat{\beta})$越大。还可以证明\info{VIF等于$1-R^2$的倒数}。
\end{note}
\subsubsection{岭回归}
\begin{note}
	由前述，复共线性程度严重时会导致$\hat{\beta}$的平均长度比$\beta$大很多，为了缓解这一问题，根据\info{花书模型空间与正则项}，可以将最小二乘法的目标函数$||y-X\beta||^2$修改为：
	\begin{equation*}
		Q(\alpha)=||y-X\beta||^2+\lambda||\beta||^2
	\end{equation*}
	其中$\lambda$为正则强度。类似\cref{theo:LSELinearModel}可知此时的正则方程为：
	\begin{equation*}
		(X^TX+\lambda I_{p})\beta=X^Ty
	\end{equation*}
	显然$X^TX+\lambda I_{p}$是一个实对称阵，由\info{矩阵多项式的特征值}和\cref{theo:LinearRegressionModel}(1)可知它还是一个正定阵，根据\cref{theo:PositiveDefinite}(6)可得$X^TX+\lambda I_{p}$可逆，于是$\beta$的估计值$\hat{\beta}(\lambda)=(X^TX+\lambda I_{p})^{-1}X^Ty$。\par
	从这个结果来看，修改后的目标函数将会选择长度更小的估计，同时也增大了$X^TX$的特征值，从而缓解复共线性问题。显然此时的$\hat{\beta}$已经不再是$\beta$的无偏估计，从\cref{prop:MSE}(1)可知，它有可能会选择均方误差更小的估计。
\end{note}
\begin{definition}
	对于\cref{model:CentralizedStandardizedLinearModel}，称：
	\begin{equation*}
		\hat{\beta}(\lambda)=(X^TX+\lambda I_p)^{-1}X^Ty
	\end{equation*}
	为$\beta$的\gls{RidgeEstimate}。由\cref{theo:LinearRegressionModel}(1)，存在正交矩阵$Q$和$\seq{\lambda}{p}$使得：
	\begin{equation*}
		\tilde{X}^T\tilde{X}=Q^T\operatorname{diag}\{\seq{\lambda}{p-1}\}Q
	\end{equation*}
	其中$\lambda_i>0$。记$Y=\tilde{X}Q^T,\;\delta_I=Q\beta_I$，于是模型可改写为：
	\begin{equation*}
		y=\beta_0\mathbf{1}_n+Y\delta_I+\varepsilon,\quad\operatorname{E}(\varepsilon)=\mathbf{0},\quad\operatorname{Cov}(\varepsilon)=\sigma^2I_n
	\end{equation*}
	称上述形式为\textbf{典则形式}（canonical form），$\delta$为\textbf{典则回归系数}。
\end{definition}
\begin{property}\label{prop:RidgeEstimate}
	中心化模型的岭估计具有如下性质：
	\begin{enumerate}
		\item $\hat{\alpha}_0=\overline{y},\;\hat{\delta}(\lambda)=Q\hat{\alpha}_I(\lambda)=\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1}Y^Ty,\;\operatorname{Cov}[\hat{\delta}(\lambda)]=\sigma^2\operatorname{diag}\left\{\frac{\lambda_1}{(\lambda_1+\lambda)^2},\frac{\lambda_2}{(\lambda_2+\lambda)^2},\dots,\frac{\lambda_{p-1}}{(\lambda_{p-1}+\lambda)^2}\right\}$；
		\item $\operatorname{MSE}[\hat{\delta}(\lambda)]=\operatorname{MSE}[\hat{\alpha}_I(\lambda)]$；
		\item 存在$\lambda>0$使得$\operatorname{MSE}[\hat{\delta}(\lambda)]<\operatorname{MSE}(\hat{\delta})$；
	\end{enumerate}
\end{property}
\begin{proof}
	(1)显然$Y=\tilde{X}_cQ^T$也是中心化的，所以第一式成立。由正则方程、\cref{prop:Transpose}(4)、\cref{prop:InvertibleMatrix}(11)和\cref{prop:OrthogonalUnitaryMatrix}(1)可得：
	\begin{align*}
		&\hat{\delta}(\lambda)=(Y^TY+\lambda I_{p-1})^{-1}Y^Ty=(Q\tilde{X}_c^T\tilde{X}_cQ^T+\lambda QI_{p-1}Q^T)^{-1}Q\tilde{X}_c^Ty \\
		=&Q(\tilde{X}_c^T\tilde{X}_c+\lambda I_{p-1})^{-1}Q^TQ\tilde{X}_c^Ty=Q(\tilde{X}_c^T\tilde{X}_c+\lambda I_{p-1})^{-1}\tilde{X}_c^Ty=Q\hat{\alpha}_I(\lambda)
	\end{align*}
	注意到：
	\begin{equation*}
		Y^TY=Q\tilde{X}_c^T\tilde{X}_cQ^T=\operatorname{diag}\{\seq{\lambda}{p-1}\}
	\end{equation*}
	所以：
	\begin{equation*}
		\hat{\delta}(\lambda)=\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1}Y^Ty
	\end{equation*}
	根据\cref{prop:CovMat}(3)和\cref{prop:InvertibleMatrix}(12)可得：
	\begin{align*}
		&\operatorname{Cov}[\hat{\delta}(\lambda)]=\operatorname{Cov}[\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1}Y^Ty] \\
		=&\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1}Y^T\sigma^2I_n[\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1}Y^T]^T \\
		=&\sigma^2\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1}Y^TY\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1} \\
		=&\sigma^2\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1}\operatorname{diag}\{\seq{\lambda}{p-1}\} \\
		&\cdot\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1} \\
		=&\sigma^2\operatorname{diag}\left\{\frac{\lambda_1}{(\lambda_1+\lambda)^2},\frac{\lambda_2}{(\lambda_2+\lambda)^2},\dots,\frac{\lambda_{p-1}}{(\lambda_{p-1}+\lambda)^2}\right\}
	\end{align*}\par
	(2)由(1)、\cref{prop:MSE}(1)、\cref{prop:CovMat}(3)、\cref{prop:Trace}(3)、\cref{prop:MeasurableIntegral}(5)和\cref{prop:Transpose}(4)可得：
	\begin{align*}
		\operatorname{MSE}[\hat{\delta}(\lambda)]&=\operatorname{MSE}[Q\hat{\alpha}_I(\lambda)]=\operatorname{tr}\operatorname{Cov}[Q\hat{\alpha}_I(\lambda)] \\
		&\quad+\{\operatorname{E}[Q\hat{\alpha}_I(\lambda)]-Q\alpha_I(\lambda)\}^T\{\operatorname{E}[Q\hat{\alpha}_I(\lambda)]-Q\alpha_I(\lambda)\} \\
		&=\operatorname{tr}\{Q\operatorname{Cov}[\hat{\alpha}_I(\lambda)]Q^T\}+\{\operatorname{E}[\hat{\alpha}_I(\lambda)]-\alpha_I(\lambda)\}^TQ^TQ\{\operatorname{E}[\hat{\alpha}_I(\lambda)]-\alpha_I(\lambda)\} \\
		&=\operatorname{tr}\operatorname{Cov}[\hat{\alpha}_I(\lambda)]+\{\operatorname{E}[\hat{\alpha}_I(\lambda)]-\alpha_I(\lambda)\}^T\{\operatorname{E}[\hat{\alpha}_I(\lambda)]-\alpha_I(\lambda)\}=\operatorname{MSE}[\hat{\alpha}_I(\lambda)]
	\end{align*}\par
	(3)由(1)可得：
	\begin{align*}
		&\operatorname{E}[\hat{\delta}(\lambda)]=\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1}Y^T\operatorname{E}(y) \\
		=&\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1}Y^T(\alpha_0\mathbf{1}_n+Y\delta) \\
		=&\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1}Y^TY\delta \\
		=&\left(\operatorname{diag}\left\{\seq{\lambda}{p-1}\right\}+\lambda I_{p-1}\right)^{-1}\operatorname{diag}\{\seq{\lambda}{p-1}\}\delta \\
		=&\left(\frac{\lambda_1}{\lambda_1+\lambda}\delta_1,\frac{\lambda_2}{\lambda_2+\lambda}\delta_2,\dots,\frac{\lambda_{p-1}}{\lambda_{p-1}+\lambda}\delta_{p-1}\right)^T
	\end{align*}
	所以：
	\begin{gather*}
		\operatorname{E}[\hat{\delta}(\lambda)]-\delta=\left(-\frac{\lambda}{\lambda_1+\lambda}\delta_1,-\frac{\lambda}{\lambda_2+\lambda}\delta_2,\dots,-\frac{\lambda}{\lambda_{p-1}+\lambda}\delta_{p-1}\right)^T
	\end{gather*}
	于是由\cref{prop:MSE}(1)、(1)、\cref{prop:Trace}(2)可得：
	\begin{gather*}
		\operatorname{MSE}[\hat{\delta}(\lambda)]=\operatorname{tr}\operatorname{Cov}[\hat{\delta}(\lambda)]+||\operatorname{E}[\hat{\delta}(\lambda)]-\delta||^2=\sigma^2\sum_{i=1}^{p-1}\frac{\lambda_i}{(\lambda_i+\lambda)^2}+\sum_{i=1}^{p-1}\frac{\lambda^2\delta_i^2}{(\lambda_i+\lambda)^2} \\
		\begin{aligned}
			\frac{\dif\operatorname{MSE}[\hat{\delta}(\lambda)]}{\dif\lambda}&=-2\sigma^2\sum_{i=1}^{p-1}\frac{\lambda_i}{(\lambda_i+\lambda)^3}+2\lambda\sum_{i=1}^{p-1}\frac{\delta_i^2}{(\lambda_i+\lambda)^2}-2\sum_{i=1}^{p-1}\frac{\lambda^2\delta_i^2}{(\lambda_i+\lambda)^3} \\
			&=-2\sigma^2\sum_{i=1}^{p-1}\frac{\lambda_i}{(\lambda_i+\lambda)^3}+2\lambda\sum_{i=1}^{p-1}\frac{\lambda_i\delta_i^2}{(\lambda_i+\lambda)^3}=\sum_{i=1}^{p-1}(\lambda\delta_i^2-\sigma^2)\frac{2\lambda_i}{(\lambda_i+\lambda)^3}
		\end{aligned}
	\end{gather*}
	当$\lambda>0$但较小时，由上式看出此时可以有$\operatorname{MSE}[\hat{\delta}(\lambda)]$关于$\lambda$的导数小于$0$，所以存在$\lambda^*>0$，当$0<\lambda<\lambda^*$时$\operatorname{MSE}[\hat{\delta}(\lambda)]$是$\lambda$的单调减函数，也就是说在这个区间上$\operatorname{MSE}[\hat{\delta}(\lambda)]<\operatorname{MSE}(\hat{\delta})$。
\end{proof}
\begin{note}
	理论说明了存在较小的$\lambda$使得岭估计优于最小二乘估计，下面给出两个选择参数$\lambda$的方法。\par
	\textbf{(1)Hoerl-Kennard公式：}由\cref{prop:RidgeEstimate}(3)，考虑方程：
	\begin{equation*}
		\sum_{i=1}^{p-1}(\lambda\delta_i^2-\sigma^2)\frac{2\lambda_i}{(\lambda_i+\lambda)^3}
	\end{equation*}
	若$\lambda\delta_i^2-\sigma^2<0,\;i=1,2,\dots,p-1$，则此时$\operatorname{MSE}[\hat{\delta}(\lambda)]$仍然小于$\operatorname{MSE}(\hat{\delta})$，所以可取：
	\begin{equation*}
		\lambda=\max_i\frac{\hat{\sigma}^2}{\hat{\delta}_i^2}
	\end{equation*}\par
	\textbf{(2)岭迹法：}将$\hat{\delta}_1(\lambda),\hat{\delta}_2(\lambda),\dots,\hat{\delta}_{p-1}(\lambda)$随着$\lambda$变化的趋势画在同一张图上，选择$\lambda$值使得各个回归系数的估计值大体上稳定并且估计值的符号较为合理（根据实际意义判断），同时还需使得$\operatorname{SSE}$上升不太多。
\end{note}