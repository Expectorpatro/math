\section{正态线性模型}
\subsection{参数估计}
\begin{definition}\label{model:NormalLinearModel}
	称以下模型为\gls{NormalLinearModel}：
	\begin{equation*}
		\begin{cases}
			y=X\beta+\varepsilon \\
			\varepsilon\sim\operatorname{N}_n(\mathbf{0},\sigma^2I_n)
		\end{cases}
	\end{equation*}
	其中$y$为$n\times 1$观测向量，$X$为$n\times p$设计矩阵，$\beta$为$p\times 1$未知参数向量，$\varepsilon$为随机误差，$\sigma^2$为误差方差。
\end{definition}
\begin{property}\label{prop:NormalLinearModel}
	对于\cref{model:NormalLinearModel}，设$c^T\beta$为可估函数，则：
	\begin{enumerate}
		\item LS估计$c^T\hat{\beta}$是$c^T\beta$的MLE，$\tilde{\sigma}^2=\dfrac{n-r}{n}\hat{\sigma}^2$是$\sigma^2$的MLE。若模型在\cref{theo:ConstraintLinearModel}的约束下，则LS估计$c^T\hat{\beta}_A$是$c^T\beta$的MLE，$\tilde{\sigma}_A^2=\dfrac{n-r+k}{n}\hat{\sigma}_A^2$是$\sigma^2$的MLE；
		\item $c^T\hat{\beta}\sim N[c^T\beta,\sigma^2c^T(X^TX)^-c]$，$\dfrac{(n-r)\hat{\sigma}^2}{\sigma^2}=\dfrac{SSE}{\sigma^2}\sim\chi_{n-r}^2$；
		\item $c^T\hat{\beta}$与$\hat{\sigma}^2$相互独立；
		\item $T_1=y^Ty,\;T_2=X^Ty$为完全充分统计量；
		\item $c^T\hat{\beta}$是$c^T\beta$唯一的MVUE，$\hat{\sigma}^2$为$\sigma^2$唯一的MVUE；
		\item $\hat{e}\sim\operatorname{N}_n[\mathbf{0},\sigma^2(I_n-P_X)]$。
	\end{enumerate}
\end{property}
\begin{proof}
	(1)对于\cref{model:NormalLinearModel}，其似然函数为：
	\begin{equation*}
		L(\beta,\sigma^2)=\frac{1}{(2\pi)^{\frac{n}{2}}(\sigma^2)^\frac{n}{2}}\exp\left(-\frac{1}{2\sigma^2}||y-X\beta||^2\right)
	\end{equation*}
	于是对数似然函数为：
	\begin{equation*}
		\ln L(\beta,\sigma^2)=-\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}||y-X\beta||^2
	\end{equation*}
	固定$\sigma^2$时，由最小二乘法原理可知：
	\begin{equation*}
		||y-X\hat{\beta}||^2=\min||y-X\beta||^2
	\end{equation*}
	当$\beta=\hat{\beta}$时有：
	\begin{equation*}
		\ln L(\hat{\beta},\sigma^2)=-\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}||y-X\hat{\beta}||^2
	\end{equation*}
	由极值点的必要条件可知：
	\begin{gather*}
		\frac{\dif\ln L(\hat{\beta},\sigma^2)}{\dif \sigma^2}=-\frac{n}{2\sigma^2}+\frac{||y-X\hat{\beta}||^2}{2\sigma^4} \\
		\tilde{\sigma}^2=\frac{1}{n}||y-X\hat{\beta}||^2
	\end{gather*}
	时对数似然函数取极值，注意到：
	\begin{gather*}
		\frac{\dif^2\ln L(\hat{\beta},\sigma^2)}{\dif (\sigma^2)^2}=\frac{n}{2\sigma^4}-\frac{||y-X\hat{\beta}||^2}{\sigma^6} \\
		\begin{aligned}
			\frac{\dif^2\ln L(\hat{\beta},\sigma^2)}{\dif (\sigma^2)^2}\Big|_{\sigma^2=\tilde{\sigma}^2}&=\frac{n^3}{2||y-X\hat{\beta}||^4}-\frac{||y-X\hat{\beta}||^2n^3}{||y-X\hat{\beta}||^6} \\
			&=\frac{n^3}{2||y-X\hat{\beta}||^4}-\frac{n^3}{||y-X\hat{\beta}||^4}=-\frac{n^3}{2||y-X\hat{\beta}||^4}<0
		\end{aligned}
	\end{gather*}
	于是此处取极大值。因为：
	\begin{equation*}
		\tilde{\sigma}^2=\frac{1}{n}||y-X\hat{\beta}||^2=\frac{n-r}{n}\hat{\sigma}^2
	\end{equation*}
	所以$\tilde{\sigma}^2$是$\sigma^2$的MLE。由上可知$\hat{\beta}$是$\beta$的MLE，根据\cref{prop:MLE}(1)可得$c^T\hat{\beta}$是$c^T\beta$的MLE。\par
	约束条件下的情况与上述证明过程类似。\par
	(2)因为$\varepsilon\sim\operatorname{N}_n(\mathbf{0},\sigma^2I_n)$，而$c^T\hat{\beta}=c^T(X^TX)^-X^Ty=c^T(X^TX)^-X^T(X\beta+\varepsilon)$。因为$c^T\beta$是可估函数，所以由\cref{prop:EstimableFunction}(1)可知存在$\alpha$使得$c=X^T\alpha$，根据\cref{prop:A-}(7)可知：
	\begin{equation*}
		c^T(X^TX)^-X^TX\beta=c^T\beta
	\end{equation*}
	由\cref{prop:A-}(6)(7)可知：
	\begin{equation*}
		c^T(X^TX)^-X^T[c^T(X^TX)^-X^T]^T=c^T(X^TX)^-X^TX(X^TX)^-c=c^T(X^TX)^-c
	\end{equation*}
	于是由\cref{prop:MultiNormal}(2)可得：
	\begin{equation*}
		c^T\hat{\beta}\sim\operatorname{N}[c^T\beta,c^T(X^TX)^-c]
	\end{equation*}\par
	因为$(I_n-P_X)X=\mathbf{0}$，根据\cref{prop:OrthogonalProjectionMat}(4)可得$I_n-P_X$是对称阵，所以由\cref{prop:ehat}(2)可知：
	\begin{align*}
		\frac{n-r}{\sigma^2}\hat{\sigma}^2&=\frac{\hat{e}^T\hat{e}}{\sigma^2}=\frac{y^T(I_n-P_X)y}{\sigma^2} \\
		&=\frac{(X\beta+\varepsilon)^T(I_n-P_X)(X\beta+\varepsilon)}{\sigma^2} \\
		&=\frac{(X\beta+\varepsilon)^T(I_n-P_X)X\beta+(X\beta+\varepsilon)^T(I_n-P_X)\varepsilon}{\sigma^2} \\
		&=\frac{(X\beta+\varepsilon)^T(I_n-P_X)\varepsilon}{\sigma^2}=\frac{\beta^TX^T(I_n-P_X)\varepsilon+\varepsilon^T(I_n-P_X)\varepsilon}{\sigma^2} \\
		&=\frac{\beta^T[(I_n-P_X)^TX]^T\varepsilon+\varepsilon^T(I_n-P_X)\varepsilon}{\sigma^2}=\frac{\varepsilon^T(I_n-P_X)\varepsilon}{\sigma^2}
	\end{align*}
	因为$\varepsilon\sim\operatorname{N}_n(\mathbf{0},\sigma^2I_n)$，由\cref{theo:MatNormalLinearTransform}可知$\dfrac{\varepsilon}{\sigma}\sim\operatorname{N}_n(\mathbf{0},I_n)$。根据\cref{prop:OrthogonalProjectionMat}(4)可知$I_n-P_X$是对称幂等阵，由\cref{prop:IdempotentMat}(3)和\cref{prop:OrthogonalProjectionMat}(1)可得$\operatorname{rank}(I_n-P_X)=n-r$，于是根据\cref{theo:XAXChi2}可得：
	\begin{equation*}
		\frac{n-r}{\sigma^2}\hat{\sigma}^2=\frac{\varepsilon^T(I_n-P_X)\varepsilon}{\sigma^2}\sim\chi_{n-r}^2
	\end{equation*}\par
	(3)由\cref{prop:ehat}(2)可知：
	\begin{equation*}
		c^T\hat{\beta}=c^T(X^TX)^-X^Ty,\quad\hat{\sigma}^2=\frac{y^T(I_n-P_X)y}{n-r}
	\end{equation*}
	由\cref{prop:OrthogonalProjectionMat}(4)可知$I_n-P_X$为对称阵，所以$\dfrac{I_n-P_X}{n-r}$也是对称阵。因为：
	\begin{align*}
		c^T(X^TX)^-X^T\frac{I_n-P_X}{n-r}&=\frac{1}{n-r}c^T(X^TX)^-X^T(I_n-P_X) \\
		&=\frac{1}{n-r}c^T(X^TX)^-[(I_n-P_X)X]^T=\mathbf{0}
	\end{align*}
	由\cref{theo:BXXAXIndependent}可知$c^T\hat{\beta}$与$\hat{\sigma}^2$独立。\par
	(4)\par
	(5)\par
	(6)由\cref{prop:ehat}(1)立即可得。
\end{proof}

\subsection{假设检验}
\begin{theorem}\label{theo:NormalLinearModelHypothesisTesting}
	对于\cref{model:NormalLinearModel}，假设：
	\begin{equation*}
		A\beta=b,\quad A\in M_{k\times p}(K),\quad\operatorname{rank}(A)=k,\quad\mathcal{M}(A^T)\subseteq\mathcal{M}(X^T)
	\end{equation*}
	且$A\beta=b$相容，则：
	\begin{enumerate}
		\item 似然比检验$H_0:A\beta=b,\;H_1:A\beta\ne b$的似然比为：
		\begin{equation*}
			\lambda(y)=\left(\frac{SSE_A}{SSE}\right)^\frac{n}{2}
		\end{equation*}
		\item $\dfrac{SSE_A-SSE}{\sigma^2}\sim\chi^2_{k,\alpha}$，其中：
		\begin{equation*}
			\alpha=\frac{(A\beta-b)^T[A(X^TX)^-A^T]^{-1}(A\beta-b)}{\sigma^2}
		\end{equation*}
		\item $SSE_A-SSE$与$SSE$相互独立；
		\item 当$A\beta=b$为真时，
		\begin{equation*}
			F=\frac{(SSE_A-SSE)/k}{SSE/(n-r)}\sim F_{k,n-r}
		\end{equation*}
		且上式左侧为$\lambda(y)$的单调增函数；
		\item 似然比检验$H_0:A\beta=b,\;H_1:A\beta\ne b$置信度为$1-\alpha$的拒绝域为$\{F:F>F_{k,n-r}(\alpha)\}$。
	\end{enumerate}
\end{theorem}
\begin{proof}
	(1)由\cref{prop:NormalLinearModel}(1)可知：
	\begin{gather*}
		\begin{aligned}
			\sup_{\beta,\sigma}L(\beta,\sigma^2;y)&=L(\hat{\beta},\tilde{\sigma}^2;y)=(2\pi)^{-\frac{n}{2}}(\tilde{\sigma}^2)^{-\frac{n}{2}}\exp\left(-\frac{||y-X\hat{\beta}||^2}{2\tilde{\sigma}^2}\right) \\
			&=(2\pi)^{-\frac{n}{2}}\left(\frac{||y-X\hat{\beta}||^2}{n}\right)^{-\frac{n}{2}}\exp\left(-\frac{n||y-X\hat{\beta}||^2}{2||y-X\hat{\beta}||^2}\right) \\
			&=(2\pi)^{-\frac{n}{2}}\left(\frac{||y-X\hat{\beta}||^2}{n}\right)^{-\frac{n}{2}}\exp\left(-\frac{n}{2}\right)=\left(\frac{2\pi e}{n}\right)^{-\frac{n}{2}}||y-X\hat{\beta}||^{-n}
		\end{aligned} \\
		\sup_{A\beta=b,\sigma^2}L(\beta,\sigma^2;y)=L(\hat{\beta}_A,\tilde{\sigma}_A^2;y)=\left(\frac{2\pi e}{n}\right)^{-\frac{n}{2}}||y-X\hat{\beta}_A||^{-n}
	\end{gather*}
	于是：
	\begin{equation*}
		\lambda(y)=\frac{L(\hat{\beta},\tilde{\sigma}^2;y)}{L(\hat{\beta}_A,\tilde{\sigma}_A^2;y)}=\left(\frac{SSE_A}{SSE}\right)^\frac{n}{2}
	\end{equation*}\par
	(2)根据\cref{prop:A-}(6)(7)和\cref{prop:OrthogonalProjectionMat}(4)以及$(I_n-P_X)X=\mathbf{0}$对$SSE_A$作分解：
	\begin{align*}
		SSE_A&=||y-X\hat{\beta}_A||^2=\Big\|y-X\{\hat{\beta}-(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b)\}\Big\|^2 \\
		&=\Big\|y-X\hat{\beta}+X(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b)\Big\|^2 \\
		&=||y-X\hat{\beta}||^2+2(y-X\hat{\beta})^TX(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&\quad+\{X(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b)\}^T \\
		&\quad\cdot X(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&=||y-X\hat{\beta}||^2+2[y-X(X^TX)^-X^Ty]^TX(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&\quad+(A\hat{\beta}-b)^T[A(X^TX)^-A^T]^{-1}A(X^TX)^-X^T \\
		&\quad\cdot X(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&=||y-X\hat{\beta}||^2+2[(I_n-P_X)y]^TX(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&\quad+(A\hat{\beta}-b)^T[A(X^TX)^-A^T]^{-1}A(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&=||y-X\hat{\beta}||^2+2y^T(I_n-P_X)X(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&\quad+(A\hat{\beta}-b)^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&=||y-X\hat{\beta}||^2+(A\hat{\beta}-b)^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b)
	\end{align*}
	所以有：
	\begin{equation*}
		SSE_A-SSE=(A\hat{\beta}-b)^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b)
	\end{equation*}
	由\cref{prop:NormalLinearModel}(2)可知：
	\begin{equation*}
		A\hat{\beta}-b\sim\operatorname{N}_k[A\beta-b,\sigma^2A(X^TX)^-A^T]
	\end{equation*}
	根据\info{$A(X^TX)^-A^T$的正定性}可知$A(X^TX)^-A^T$存在平方根阵及平方根阵的逆矩阵，由\cref{prop:MultiNormal}(2)可知：
	\begin{equation*}
		\frac{[A(X^TX)^-A^T]^{-\frac{1}{2}}}{\sigma}(A\hat{\beta}-b)\sim\operatorname{N}_k\{[A(X^TX)^-A^T]^{-\frac{1}{2}}(A\beta-b),I_n\}
	\end{equation*}
	于是由$\chi^2$分布的定义可得：
	\begin{equation*}
		\frac{SSE_A-SSE}{\sigma^2}=\frac{\{[A(X^TX)^-A^T]^{-\frac{1}{2}}(A\hat{\beta}-b)\}^T[A(X^TX)^-A^T]^{-\frac{1}{2}}(A\hat{\beta}-b)}{\sigma^2}\sim\chi_{k,\alpha}^2
	\end{equation*}
	其中：
	\begin{align*}
		\alpha&=\{[A(X^TX)^-A^T]^{-\frac{1}{2}}(A\beta-b)\}^T[A(X^TX)^-A^T]^{-\frac{1}{2}}(A\beta-b) \\
		&=(A\beta-b)^T[A(X^TX)^-A^T]^{-1}(A\beta-b)
	\end{align*}\par
	(3)由\cref{prop:ehat}(2)可知$SSE=y^T(I_n-P_X)y$，由(2)的过程可得：
	\begin{align*}
		SSE_A-SSE&=(A\hat{\beta}-b)^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-b) \\
		&=[A(X^TX)^-X^Ty-b]^T[A(X^TX)^-A^T]^{-1}[A(X^TX)^-X^Ty-b] \\
		&=y^TX(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}A(X^TX)^-X^Ty \\
		&\quad-2b^T[A(X^TX)^-A^T]^{-1}A(X^TX)^-X^Ty+b^T[A(X^TX)^-A^T]^{-1}b
	\end{align*}
	因为$(I_n-P_X)X=\mathbf{0}$，由\cref{prop:OrthogonalProjectionMat}(4)可得：
	\begin{gather*}
		(I_n-P_X)X(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}A(X^TX)^-X^T=\mathbf{0} \\
		\begin{aligned}
			&2b^T[A(X^TX)^-A^T]^{-1}A(X^TX)^-X^T(I_n-P_X) \\
			=&2b^T[A(X^TX)^-A^T]^{-1}A(X^TX)^-[(I_n-P_X)X]^T=\mathbf{0}
		\end{aligned}
	\end{gather*}
	于是根据\cref{theo:BXXAXIndependent}和\cref{theo:XAXXBXIndependent}可知$SSE_A-SSE$与$SSE$独立。\par
	(4)当$A\beta=b$为真时，由(2)可知$\dfrac{SSE_A-SSE}{\sigma^2}\sim\chi^2_{k}$。根据(3)和\cref{prop:NormalLinearModel}(2)可得：
	\begin{equation*}
		\frac{(SSE_A-SSE)/(k\sigma^2)}{SSE/[(n-r)\sigma^2]}=\frac{(SSE_A-SSE)/k}{SSE/(n-r)}\sim F_{k,n-r}\qedhere
	\end{equation*}
	由(1)可得：
	\begin{equation*}
		\frac{(SSE_A-SSE)/k}{SSE/(n-r)}=\frac{n-r}{k}[\lambda^{\frac{2}{n}}(y)-1]
	\end{equation*}
	所以它是$\lambda(y)$的单调增函数。\par
	(5)由(1)(4)可立即得出。
\end{proof}

\subsection{置信域}
\subsubsection{置信椭球}
\begin{theorem}\label{theo:NormalLinearModelConfidenceEllipsoid}
	对于\cref{model:NormalLinearModel}，若不能接受假设$A\beta=\mathbf{0}$，则$A\beta$置信度为$1-\alpha$的置信椭球为：
	\begin{equation*}
		\{A\beta:(A\beta-A\hat{\beta})^T[A(X^TX)^-A^T]^{-1}(A\beta-A\hat{\beta})\leqslant k\hat{\sigma}^2F_{k,n-r}(\alpha)\}
	\end{equation*}
\end{theorem}
\begin{proof}
	由\cref{prop:NormalLinearModel}(2)可知：
	\begin{equation*}
		A\hat{\beta}\sim\operatorname{N}[A\beta,\sigma^2A(X^TX)^-A^T]
	\end{equation*}
	所以：
	\begin{equation*}
		\frac{A\hat{\beta}-A\beta}{\sigma}\sim\operatorname{N}[\mathbf{0},A(X^TX)^-A^T]
	\end{equation*}
	因为：
	\begin{equation*}
		[A(X^TX)^-A^T]^{-1}A(X^TX)^-A^T[A(X^TX)^-A^T]^{-1}=[A(X^TX)^-A^T]^{-1}
	\end{equation*}
	所以由\cref{theo:XAXChi2}可知：
	\begin{equation*}
		\frac{(A\hat{\beta}-A\beta)^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-A\beta)}{\sigma^2}\sim\chi_{k}^2
	\end{equation*}
	根据\cref{prop:NormalLinearModel}(2)(3)可得：
	\begin{align*}
		&\frac{(A\hat{\beta}-A\beta)^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-A\beta)}{k\sigma^2}\Big/\frac{(n-r)\hat{\sigma}^2}{(n-r)\sigma^2} \\
		=&\frac{(A\hat{\beta}-A\beta)^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-A\beta)}{k\hat{\sigma}^2}\sim F_{k,n-r}
	\end{align*}
	所以对任意的$0<\alpha<1$，有：
	\begin{equation*}
		P\left[\frac{(A\hat{\beta}-A\beta)^T[A(X^TX)^-A^T]^{-1}(A\hat{\beta}-A\beta)}{k\hat{\sigma}^2}\leqslant F_{k,n-r}(\alpha)\right]=1-\alpha
	\end{equation*}
	即$A\beta$置信度为$1-\alpha$的置信椭球为：
	\begin{equation*}
		\{A\beta:(A\beta-A\hat{\beta})^T[A(X^TX)^-A^T]^{-1}(A\beta-A\hat{\beta})\leqslant k\hat{\sigma}^2F_{k,n-r}(\alpha)\}\qedhere
	\end{equation*}
\end{proof}
\begin{note}
	这里其实是一个未知方差构造$F$分布的思想。
\end{note}
\subsubsection{Scheffe置信区间}
\begin{theorem}\label{theo:ScheffeCI}
	对于\cref{model:NormalLinearModel}，对任何可估函数$l^T\beta$，其中$l\in\mathcal{M}(A^T)$且$l\ne\mathbf{0}$，其置信度为$1-\alpha$的同时置信区间为：
	\begin{equation*}
		l^T\hat{\beta}\pm[k\hat{\sigma}^2F_{k,n-r}(\alpha)l^T(X^TX)^-l]^{\frac{1}{2}}
	\end{equation*}
\end{theorem}
\begin{proof}
	由\cref{theo:NormalLinearModelConfidenceEllipsoid}、\info{$A(X^TX)^-A^T$的正定性}与\cref{ineq:cauchy-schiwarz-rayleigh}可知：
	\begin{align*}
		1-\alpha&=P\left[(A\beta-A\hat{\beta})^T[A(X^TX)^-A^T]^{-1}(A\beta-A\hat{\beta})\leqslant k\hat{\sigma}^2F_{k,n-r}(\alpha)\right] \\
		&=P\left\{\sup_{b\ne\mathbf{0}}\frac{[(A\hat{\beta}-A\beta)^Tb]^2}{b^TA(X^TX)^-A^Tb}\leqslant k\hat{\sigma}^2F_{k,n-r}(\alpha)\right\} \\
		&=P\left\{\frac{[(A\hat{\beta}-A\beta)^Tb]^2}{b^TA(X^TX)^-A^Tb}\leqslant k\hat{\sigma}^2F_{k,n-r}(\alpha),\;\text{对任意的}b\ne\mathbf{0}\right\} \\ 
		&=P\left\{|(A\hat{\beta}-A\beta)^Tb|\leqslant [k\hat{\sigma}^2F_{k,n-r}(\alpha)b^TA(X^TX)^-A^Tb]^{\frac{1}{2}},\;\text{对任意的}b\ne\mathbf{0}\right\} \\
		&=P\left\{|\hat{\beta}^TA^Tb-\beta^TA^Tb|\leqslant [k\hat{\sigma}^2F_{k,n-r}(\alpha)b^TA(X^TX)^-A^Tb]^{\frac{1}{2}},\;\text{对任意的}b\ne\mathbf{0}\right\}
	\end{align*}
	记$A^Tb=l$，因为$\mathcal{M}(A^T)\subseteq\mathcal{M}(X^T)$，由\cref{prop:EstimableFunction}(1)可知$l^T\beta$也是一个可估函数，于是有：
	\begin{equation*}
		1-\alpha=P\left\{|l^T\hat{\beta}-l^T\beta|\leqslant [k\hat{\sigma}^2F_{k,n-r}(\alpha)l^T(X^TX)^-l]^{\frac{1}{2}},\;\text{对任意的}l\in\mathcal{M}(A^T)\text{且}l\ne\mathbf{0}\right\}\qedhere
	\end{equation*}
\end{proof}
\subsubsection{Bonferroni置信区间}
\begin{theorem}\label{theo:BonferroniCI}
	对于\cref{model:NormalLinearModel}，记$A$的行分别为$\seq{a^T}{k}$，则$a_
	i^T\beta$置信度为$1-\alpha$的Bonferroni置信区间为：
	\begin{equation*}
		a_i^T\hat{\beta}\pm t_{n-r}\left(\frac{\alpha}{2k}\right)[\hat{\sigma}^2a_i^T(X^TX)^-a_i]^{\frac{1}{2}}
	\end{equation*}
\end{theorem}
\begin{proof}
	由\cref{theo:NormalLinearModelConfidenceEllipsoid}可得当$k=1$时有：
	\begin{align*}
		1-\alpha&=P\left\{(a_i^T\beta-a_i^T\hat{\beta})^T[a_i^T(X^TX)^-a_i]^{-1}(a_i^T\beta-a_i^T\hat{\beta})\leqslant\hat{\sigma}^2F_{1,n-r}(\alpha)\right\} \\
		&=P\left\{(a_i^T\beta-a_i^T\hat{\beta})^2\leqslant\hat{\sigma}^2F_{1,n-r}(\alpha)[a_i^T(X^TX)^-a_i]\right\}
	\end{align*}
	由\cref{prop:FDistribution}(2)可知$F_{1,n-r}=t^2_{n-r}$，因为服从$t$分布的变量可取负值而服从$F$分布的变量只能为正值，所以上式把平方变成绝对值时应修改对应的$\alpha$为$\dfrac{a}{2}$，即此时：
	\begin{equation*}
		1-\alpha=P\left\{|a_i^T\beta-a_i^T\hat{\beta}|\leqslant t_{n-r}\left(\frac{\alpha}{2}\right)[\hat{\sigma}^2a_i^T(X^TX)^-a_i]^{\frac{1}{2}}\right\}
	\end{equation*}
	由Bonferroni校正法可得出结论。
\end{proof}
\subsubsection{比较}
\begin{derivation}
	Scheffe区间与Bonferroni区间哪个更好？由二者的公式可以看出只需选择：
	\begin{equation*}
		\min\left\{[k\hat{\sigma}^2F_{k,n-r}(\alpha)]^{\frac{1}{2}},\hat{\sigma}t_{n-r}\left(\frac{\alpha}{2k}\right)\right\}
	\end{equation*}
	对应的方法即能得到更短的置信区间。
\end{derivation}

\subsection{区间预测}
\begin{theorem}
	延续\ref{def:LinearModelForcast}处的定义，假设$\varepsilon\sim\operatorname{N}(0,\sigma^2I_n),\;\varepsilon_0\sim\operatorname{N}(0,\sigma^2I_m)$，且有
	\begin{equation*}
		\operatorname{Cov}[(y,y_0)^T]=\operatorname{Cov}[(\varepsilon,\varepsilon_0)^T]=\sigma^2
		\begin{pmatrix}
			I_n & V^T \\
			V & I_m 
		\end{pmatrix}
	\end{equation*}
	则$y_0^{(i)}$的Scheffe置信区间与Bonferroni置信区间分别为：
	\begin{equation*}
		\hat{y}_0^{(i)}\pm[m\hat{\sigma}^2F_{m,n-r}(\alpha)T_{ii}]^{\frac{1}{2}},\quad
		\hat{y}_0^{(i)}\pm t_{n-r}\left(\frac{\alpha}{2m}\right)\hat{\sigma}(T_{ii})^{\frac{1}{2}}
	\end{equation*}
	其中$y_0^{(i)},\hat{y}_0^{(i)}$表示的是$y_0,\hat{y}_0$的第$i$个分量，$\hat{y}_0$由\cref{theo:PECorrelatedLinearModelForcast}给出，$T_{ii}$表示矩阵$T$的$i,i$元，$T=(X_0-VX)(X^TX)^-(X_0-VX)^T+I_m-VV^T$。
\end{theorem}
\begin{proof}
	由\cref{theo:PECorrelatedLinearModelForcast}、\cref{prop:MultiNormal}(2)、\cref{prop:CovMat}(3)(5)和\cref{prop:A-}(6)(7)(5)可得：
	\begin{equation*}
		\hat{y}_0=X_0\hat{\beta}+V(y-X\hat{\beta})\sim\operatorname{N}_m(X_0\beta,\sigma^2VV^T)
	\end{equation*}
	\begin{align*}
		\operatorname{Cov}(\hat{y}_0)&=\operatorname{Cov}(Cy)=C\operatorname{Cov}(y)C^T=\sigma^2CC^T \\
		&=\sigma^2[X_0(X^TX)^-X^T+V-VX(X^TX)^-X^T] \\
		&\quad\cdot[X(X^TX)^-X_0^T+V^T-X(X^TX)^-X^TV^T] \\
		&=\sigma^2[X_0(X^TX)^-X^TX(X^TX)^-X_0^T+X_0(X^TX)^-X^TV^T \\
		&\quad-X_0(X^TX)^-X^TX(X^TX)^-X^TV^T+VX(X^TX)^-X_0^T \\
		&\quad+VV^T-VX(X^TX)^-X^TV^T-VX(X^TX)^-X^TX(X^TX)^-X_0^T \\
		&\quad-VX(X^TX)^-X^TV^T+VX(X^TX)^-X^TX(X^TX)^-X^TV^T] \\
		&=\sigma^2[X_0(X^TX)^-X_0^T+X_0(X^TX)^-X^TV^T-X_0(X^TX)^-X^TV^T \\
		&\quad+VX(X^TX)^-X_0^T+VV^T-VX(X^TX)^-X^TV^T \\
		&\quad-VX(X^TX)^-X_0^T-VX(X^TX)^-X^TV^T+VX(X^TX)^-X^TV^T] \\
		&=\sigma^2[X_0(X^TX)^-X_0^T+VV^T-VX(X^TX)^-X^TV^T]
	\end{align*}
	\begin{align*}
		\operatorname{Cov}(\hat{y}_0,y_0)
		&=\operatorname{Cov}[X_0\hat{\beta}+V(y-X\hat{\beta}),y_0]=\operatorname{Cov}[Vy+(X_0-VX)\hat{\beta},y_0] \\
		&=V\operatorname{Cov}(y,y_0)+(X_0-VX)\operatorname{Cov}(\hat{\beta},y_0) \\
		&=V\operatorname{Cov}(y,y_0)+(X_0-VX)\operatorname{Cov}[(X^TX)^-X^Ty,y_0] \\
		&=V\operatorname{Cov}(y,y_0)+(X_0-VX)(X^TX)^-X^T\operatorname{Cov}\operatorname{Cov}(y,y_0) \\
		&=[V+(X_0-VX)(X^TX)^-X^T]\operatorname{Cov}(y,y_0) \\
		&=\sigma^2[V+(X_0-VX)(X^TX)^-X^T]V^T
	\end{align*}
	令$A=X_0(X^TX)^-X_0^T+VV^T-VX(X^TX)^-X^TV^T,\;B=[V+(X_0-VX)(X^TX)^-X^T]V^T$，所以：
	\begin{equation*}
		\begin{pmatrix}
			\hat{y}_0 \\
			y_0
		\end{pmatrix}\sim\operatorname{N}_{m+n}\left[
		\begin{pmatrix}
			X_0\beta \\
			X_0\beta
		\end{pmatrix},\sigma^2
		\begin{pmatrix}
			A & B \\
			B^T & I_m
		\end{pmatrix}
		\right]
	\end{equation*}
	由\cref{prop:MultiNormal}(2)可知：
	\begin{equation*}
		\hat{y}_0-y_0=
		(1,-1)
		\begin{pmatrix}
			\hat{y}_0 \\
			y_0
		\end{pmatrix}\sim\operatorname{N}_m\{
		\mathbf{0},\sigma^2[(X_0-VX)(X^TX)^-(X_0-VX)+I_m-VV^T]\}
	\end{equation*}\par
	令$T=(X_0-VX)(X^TX)^-(X_0-VX)^T+I_m-VV^T$，仿照\cref{theo:NormalLinearModelConfidenceEllipsoid}中的推导能得到此时$\hat{y}_0-y_0$置信度为$1-\alpha$的置信椭球为：
	\begin{equation*}
		[\hat{y}_0-y_0:(\hat{y}_0-y_0)^T(T)^{-1}(\hat{y}_0-y_0)\leqslant m\hat{\sigma}^2F_{m,n-r}(\alpha)]
	\end{equation*}
	其中$T$的可逆性由\info{T的可逆性证明}保证。\par
	由上述置信椭球，仿照\cref{theo:ScheffeCI}的推导，在其中取$b$为标准基向量即可得到$y_0^{(i)}$的Scheffe置信区间为：
	\begin{equation*}
		\hat{y}_0^{(i)}\pm[m\hat{\sigma}^2F_{m,n-r}(\alpha)T_{ii}]^{\frac{1}{2}}
	\end{equation*}\par
	同理可得到$y_0^{(i)}$的Bonferroni置信区间为：
	\begin{equation*}
		\hat{y}_0^{(i)}\pm t_{n-r}\left(\frac{\alpha}{2m}\right)\hat{\sigma}(T_{ii})^{\frac{1}{2}}\qedhere
	\end{equation*}
\end{proof}





