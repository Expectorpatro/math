\section{正态线性模型}
\subsection{参数估计}
\begin{definition}\label{model:NormalLinearModel}
	称以下模型为\gls{NormalLinearModel}：
	\begin{equation*}
		\begin{cases}
			y=X\beta+\varepsilon \\
			\varepsilon\sim\operatorname{N}_n(\mathbf{0},\sigma^2I_n)
		\end{cases}
	\end{equation*}
	其中$y$为$n\times 1$观测向量，$X$为$n\times p$设计矩阵，$\beta$为$p\times 1$未知参数向量，$\varepsilon$为随机误差，$\sigma^2$为误差方差。
\end{definition}
\begin{property}\label{prop:NormalLinearModel}
	对于\cref{model:NormalLinearModel}，设$c^{\top}\beta$为可估函数，则：
	\begin{enumerate}
		\item LS估计$c^{\top}\hat{\beta}$是$c^{\top}\beta$的MLE，$\tilde{\sigma}^2=\dfrac{n-r}{n}\hat{\sigma}^2$是$\sigma^2$的MLE。若模型在\cref{theo:ConstraintLinearModel}的约束下，则LS估计$c^{\top}\hat{\beta}_A$是$c^{\top}\beta$的MLE，$\tilde{\sigma}_A^2=\dfrac{n-r+k}{n}\hat{\sigma}_A^2$是$\sigma^2$的MLE；
		\item $c^{\top}\hat{\beta}\sim N[c^{\top}\beta,\sigma^2c^{\top}(X^{\top}X)^-c]$，$\dfrac{(n-r)\hat{\sigma}^2}{\sigma^2}=\dfrac{\operatorname{SSE}}{\sigma^2}\sim\chi_{n-r}^2$；
		\item $c^{\top}\hat{\beta}$与$\hat{\sigma}^2$相互独立；
		\item $T_1=y^{\top}y,\;T_2=X^{\top}y$为$y$的分布族的完全充分统计量；
		\item 若$c^{\top}\beta$和$\sigma^2$的估计的损失函数是估计量的严格凸函数，则$c^{\top}\hat{\beta}$是$c^{\top}\beta$在几乎处处的意义下唯一的UMRUE，$\hat{\sigma}^2$为$\sigma^2$在几乎处处的意义下唯一的UMRUE；
		\item $\hat{e}\sim\operatorname{N}_n[\mathbf{0},\sigma^2(I_n-P_X)]$。
	\end{enumerate}
\end{property}
\begin{proof}
	(1)对于\cref{model:NormalLinearModel}，其似然函数为：
	\begin{equation*}
		L(\beta,\sigma^2)=\frac{1}{(2\pi)^{\frac{n}{2}}(\sigma^2)^\frac{n}{2}}\exp\left(-\frac{1}{2\sigma^2}||y-X\beta||^2\right)
	\end{equation*}
	于是对数似然函数为：
	\begin{equation*}
		\ln L(\beta,\sigma^2)=-\frac{n}{2}\ln2\pi-\frac{n}{2}\ln\sigma^2-\frac{1}{2\sigma^2}||y-X\beta||^2
	\end{equation*}
	固定$\sigma^2$时，由最小二乘法原理可知：
	\begin{equation*}
		||y-X\hat{\beta}||^2=\min||y-X\beta||^2
	\end{equation*}
	当$\beta=\hat{\beta}$时有：
	\begin{equation*}
		\ln L(\hat{\beta},\sigma^2)=-\frac{n}{2}\ln2\pi-\frac{n}{2}\ln\sigma^2-\frac{1}{2\sigma^2}||y-X\hat{\beta}||^2
	\end{equation*}
	由极值点的必要条件可知：
	\begin{gather*}
		\frac{\dif\ln L(\hat{\beta},\sigma^2)}{\dif \sigma^2}=-\frac{n}{2\sigma^2}+\frac{||y-X\hat{\beta}||^2}{2\sigma^4} \\
		\tilde{\sigma}^2=\frac{1}{n}||y-X\hat{\beta}||^2
	\end{gather*}
	时对数似然函数取极值，注意到：
	\begin{gather*}
		\frac{\dif^2\ln L(\hat{\beta},\sigma^2)}{\dif (\sigma^2)^2}=\frac{n}{2\sigma^4}-\frac{||y-X\hat{\beta}||^2}{\sigma^6} \\
		\begin{aligned}
			\frac{\dif^2\ln L(\hat{\beta},\sigma^2)}{\dif (\sigma^2)^2}\Big|_{\sigma^2=\tilde{\sigma}^2}&=\frac{n^3}{2||y-X\hat{\beta}||^4}-\frac{||y-X\hat{\beta}||^2n^3}{||y-X\hat{\beta}||^6} \\
			&=\frac{n^3}{2||y-X\hat{\beta}||^4}-\frac{n^3}{||y-X\hat{\beta}||^4}=-\frac{n^3}{2||y-X\hat{\beta}||^4}<0
		\end{aligned}
	\end{gather*}
	于是此处取极大值。因为：
	\begin{equation*}
		\tilde{\sigma}^2=\frac{1}{n}||y-X\hat{\beta}||^2=\frac{n-r}{n}\hat{\sigma}^2
	\end{equation*}
	所以$\tilde{\sigma}^2$是$\sigma^2$的MLE。由上可知$\hat{\beta}$是$\beta$的MLE，根据\cref{prop:MLE}(1)可得$c^{\top}\hat{\beta}$是$c^{\top}\beta$的MLE。\par
	约束条件下的情况与上述证明过程类似。\par
	(2)因为$\varepsilon\sim\operatorname{N}_n(\mathbf{0},\sigma^2I_n)$，而$c^{\top}\hat{\beta}=c^{\top}(X^{\top}X)^-X^{\top}y=c^{\top}(X^{\top}X)^-X^{\top}(X\beta+\varepsilon)$。因为$c^{\top}\beta$是可估函数，所以由\cref{prop:EstimableFunction}(1)可知存在$\alpha$使得$c=X^{\top}\alpha$，根据\cref{prop:A-}(7)可知：
	\begin{equation*}
		c^{\top}(X^{\top}X)^-X^{\top}X\beta=c^{\top}\beta
	\end{equation*}
	由\cref{prop:A-}(5)(7)可知：
	\begin{equation*}
		c^{\top}(X^{\top}X)^-X^{\top}[c^{\top}(X^{\top}X)^-X^{\top}]^{\top}=c^{\top}(X^{\top}X)^-X^{\top}X(X^{\top}X)^-c=c^{\top}(X^{\top}X)^-c
	\end{equation*}
	于是由\cref{prop:MultiNormal}(2)可得：
	\begin{equation*}
		c^{\top}\hat{\beta}\sim\operatorname{N}[c^{\top}\beta,c^{\top}(X^{\top}X)^-c]
	\end{equation*}\par
	因为$(I_n-P_X)X=\mathbf{0}$，根据\cref{prop:OrthogonalProjectionMat}(7)(3)可得$I_n-P_X$是对称阵，所以由\cref{prop:ehat}(2)、\cref{prop:OrthogonalProjectionMat}(8)和\cref{prop:Transpose}(4)可知：
	\begin{align*}
		\frac{n-r}{\sigma^2}\hat{\sigma}^2&=\frac{\hat{e}^{\top}\hat{e}}{\sigma^2}=\frac{y^{\top}(I_n-P_X)y}{\sigma^2} \\
		&=\frac{(X\beta+\varepsilon)^{\top}(I_n-P_X)(X\beta+\varepsilon)}{\sigma^2} \\
		&=\frac{(X\beta+\varepsilon)^{\top}(I_n-P_X)X\beta+(X\beta+\varepsilon)^{\top}(I_n-P_X)\varepsilon}{\sigma^2} \\
		&=\frac{(X\beta+\varepsilon)^{\top}(I_n-P_X)\varepsilon}{\sigma^2}=\frac{\beta^{\top}X^{\top}(I_n-P_X)\varepsilon+\varepsilon^{\top}(I_n-P_X)\varepsilon}{\sigma^2} \\
		&=\frac{\beta^{\top}[(I_n-P_X)^{\top}X]^{\top}\varepsilon+\varepsilon^{\top}(I_n-P_X)\varepsilon}{\sigma^2}=\frac{\varepsilon^{\top}(I_n-P_X)\varepsilon}{\sigma^2}
	\end{align*}
	因为$\varepsilon\sim\operatorname{N}_n(\mathbf{0},\sigma^2I_n)$，由\cref{prop:MultiNormal}(2)可知$\dfrac{\varepsilon}{\sigma}\sim\operatorname{N}_n(\mathbf{0},I_n)$。根据\cref{prop:OrthogonalProjectionMat}(7)(3)可知$I_n-P_X$是对称幂等阵，由\cref{prop:IdempotentMat}(5)和\cref{prop:OrthogonalProjectionMat}(1)可得$\operatorname{rank}(I_n-P_X)=n-r$，于是根据\cref{theo:XAXChi2}可得：
	\begin{equation*}
		\frac{n-r}{\sigma^2}\hat{\sigma}^2=\frac{\varepsilon^{\top}(I_n-P_X)\varepsilon}{\sigma^2}\sim\chi_{n-r}^2
	\end{equation*}\par
	(3)由\cref{prop:ehat}(2)可知：
	\begin{equation*}
		c^{\top}\hat{\beta}=c^{\top}(X^{\top}X)^-X^{\top}y,\quad\hat{\sigma}^2=\frac{y^{\top}(I_n-P_X)y}{n-r}
	\end{equation*}
	因为$c^{\top}\beta$是可估函数，所以存在$\alpha$使得$c=X^{\top}\alpha$，由\cref{prop:Transpose}(4)、\cref{prop:MatrixMultiplication}(2)和\cref{prop:OrthogonalProjectionMat}(8)可得：
	\begin{align*}
		c^{\top}(X^{\top}X)^-X^{\top}\operatorname{Cov}(y)\frac{I_n-P_X}{n-r}&=\frac{\sigma^2}{n-r}\alpha^{\top}X(X^{\top}X)^-X^{\top}(I_n-P_X) \\
		&=\frac{\sigma^2}{n-r}\alpha^{\top}P_X(I_n-P_X)=\mathbf{0}
	\end{align*}
	由\cref{theo:BXXAXIndependent}可知$c^{\top}\hat{\beta}$与$\hat{\sigma}^2$独立。\par
	(4)由\cref{prop:MultiNormal}(1)(2)和\cref{prop:Transpose}(4)可知观测向量$y$的概率函数为：
	\begin{align*}
		&p(y)=(2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2}(y-X\beta)^{\top}\frac{1}{\sigma^2}(y-X\beta)\right] \\
		=&(2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[-\frac{1}{2\sigma^2}(y^{\top}y-y^{\top}X\beta-\beta^{\top}X^{\top}y-\beta^{\top}X^{\top}X\beta)\right] \\
		=&(2\pi\sigma^2)^{-\frac{n}{2}}\exp(\beta^{\top}X^{\top}X\beta)\exp\left(-\frac{1}{2\sigma^2}y^{\top}y-\frac{\beta^{\top}}{\sigma^2}X^{\top}y\right)
	\end{align*}
	所以$y$的分布族为指数族，自然参数为$\left(-\dfrac{1}{2\sigma^2},-\dfrac{\beta^{\top}}{\sigma^2}\right)$，自然参数空间为$\mathbb{R}^{-}\times\mathbb{R}^{p}$，所以它是满秩的。由\cref{prop:ExponentialFamily}(8)可知$T_1=y^{\top}y,T_2=X^{\top}y$是分布族的完全充分统计量。\par
	(5)由\cref{prop:EstimableFunction}(4)可知$c^{\top}\hat{\beta}=c^{\top}(X^{\top}X)^-X^{\top}y$是$c^{\top}\beta$的无偏估计，根据(4)可得$c^{\top}\hat{\beta}$是完全充分统计量$X^{\top}y$的函数，由\cref{theo:Lehmann-Scheffe}可知$c^{\top}\hat{\beta}$是$c^{\top}\beta$在几乎处处的意义下唯一的UMRUE。由\cref{theo:VarianceOfErrorTerm}和\cref{prop:ehat}(2)可知$\hat{\sigma}^2=\dfrac{\operatorname{SSE}}{n-r}=\dfrac{y^{\top}y-y^{\top}X(X^{\top}X)^-X^{\top}y}{n-r}$是$\sigma^2$的无偏估计，根据(4)可得$\hat{\sigma}^2$是完全充分统计量$y^{\top}y$和$X^{\top}y$的函数，由\cref{theo:Lehmann-Scheffe}可知$\hat{\sigma}^2$是$\sigma^2$在几乎处处的意义下唯一的UMRUE。\par
	(6)由\cref{prop:ehat}(1)和\cref{prop:MultiNormal}(2)立即可得。
\end{proof}

\subsection{假设检验}
\begin{theorem}\label{theo:NormalLinearModelHypothesisTesting}
	对于\cref{model:NormalLinearModel}，假设：
	\begin{equation*}
		A\beta=b,\quad A\in M_{k\times p}(K),\quad\operatorname{rank}(A)=k,\quad\mathcal{M}(A^{\top})\subseteq\mathcal{M}(X^{\top})
	\end{equation*}
	且$A\beta=b$相容，则：
	\begin{enumerate}
		\item 似然比检验$H_0:A\beta=b,\;H_1:A\beta\ne b$的似然比为：
		\begin{equation*}
			\lambda(y)=\left(\frac{\operatorname{SSE}_A}{\operatorname{SSE}}\right)^\frac{n}{2}
		\end{equation*}
		\item $\dfrac{\operatorname{SSE}_A-\operatorname{SSE}}{\sigma^2}\sim\chi^2_{k,\alpha}$，其中：
		\begin{equation*}
			\alpha=\frac{(A\beta-b)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\beta-b)}{\sigma^2}
		\end{equation*}
		\item $\operatorname{SSE}_A-\operatorname{SSE}$与$\operatorname{SSE}$相互独立；
		\item 当$A\beta=b$为真时，
		\begin{equation*}
			F=\frac{(\operatorname{SSE}_A-\operatorname{SSE})/k}{\operatorname{SSE}/(n-r)}\sim\operatorname{F}_{k,n-r}
		\end{equation*}
		且上式左侧为$\lambda(y)$的单调增函数；
		\item 似然比检验$H_0:A\beta=b,\;H_1:A\beta\ne b$置信水平为$1-\alpha$的拒绝域为$\{F:F>\operatorname{F}_{k,n-r}(1-\alpha)\}$；
	\end{enumerate}
\end{theorem}
\begin{proof}
	(1)由\cref{prop:NormalLinearModel}(1)可知：
	\begin{gather*}
		\begin{aligned}
			\sup_{\beta,\sigma}L(\beta,\sigma^2;y)&=L(\hat{\beta},\tilde{\sigma}^2;y)=(2\pi)^{-\frac{n}{2}}(\tilde{\sigma}^2)^{-\frac{n}{2}}\exp\left(-\frac{||y-X\hat{\beta}||^2}{2\tilde{\sigma}^2}\right) \\
			&=(2\pi)^{-\frac{n}{2}}\left(\frac{||y-X\hat{\beta}||^2}{n}\right)^{-\frac{n}{2}}\exp\left(-\frac{n||y-X\hat{\beta}||^2}{2||y-X\hat{\beta}||^2}\right) \\
			&=(2\pi)^{-\frac{n}{2}}\left(\frac{||y-X\hat{\beta}||^2}{n}\right)^{-\frac{n}{2}}\exp\left(-\frac{n}{2}\right)=\left(\frac{2\pi e}{n}\right)^{-\frac{n}{2}}||y-X\hat{\beta}||^{-n}
		\end{aligned} \\
		\sup_{A\beta=b,\sigma^2}L(\beta,\sigma^2;y)=L(\hat{\beta}_A,\tilde{\sigma}_A^2;y)=\left(\frac{2\pi e}{n}\right)^{-\frac{n}{2}}||y-X\hat{\beta}_A||^{-n}
	\end{gather*}
	于是：
	\begin{equation*}
		\lambda(y)=\frac{L(\hat{\beta},\tilde{\sigma}^2;y)}{L(\hat{\beta}_A,\tilde{\sigma}_A^2;y)}=\left(\frac{\operatorname{SSE}_A}{\operatorname{SSE}}\right)^\frac{n}{2}
	\end{equation*}\par
	(2)根据\cref{prop:Transpose}(4)、\cref{prop:InvertibleMatrix}(12)、\cref{prop:A-}(5)(7)和\cref{prop:OrthogonalProjectionMat}(7)(3)对$\operatorname{SSE}_A$作分解：
	\begin{align*}
		\operatorname{SSE}_A&=||y-X\hat{\beta}_A||^2=\Big\|y-X\{\hat{\beta}-(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b)\}\Big\|^2 \\
		&=\Big\|y-X\hat{\beta}+X(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b)\Big\|^2 \\
		&=\operatorname{SSE}+2(y-X\hat{\beta})^{\top}X(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b) \\
		&\quad+\{X(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b)\}^{\top} \\
		&\quad\cdot X(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b) \\
		&=\operatorname{SSE}+2[y-X(X^{\top}X)^-X^{\top}y]^{\top}X(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b) \\
		&\quad+(A\hat{\beta}-b)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}A(X^{\top}X)^-X^{\top} \\
		&\quad\cdot X(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b) \\
		&=\operatorname{SSE}+2[(I_n-P_X)y]^{\top}X(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b) \\
		&\quad+(A\hat{\beta}-b)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}A(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b) \\
		&=\operatorname{SSE}+2y^{\top}(I_n-P_X)X(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b) \\
		&\quad+(A\hat{\beta}-b)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b) \\
		&=\operatorname{SSE}+(A\hat{\beta}-b)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b)
	\end{align*}
	所以有：
	\begin{equation*}
		\operatorname{SSE}_A-\operatorname{SSE}=(A\hat{\beta}-b)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b)
	\end{equation*}
	由\cref{prop:NormalLinearModel}(2)和\cref{prop:MultiNormal}(2)可知：
	\begin{equation*}
		A\hat{\beta}-b\sim\operatorname{N}_k[A\beta-b,\sigma^2A(X^{\top}X)^-A^{\top}]
	\end{equation*}
	根据\info{$A(X^{\top}X)^-A^{\top}$的正定性}可知$A(X^{\top}X)^-A^{\top}$存在平方根阵及平方根阵的逆矩阵，由\cref{prop:MultiNormal}(2)可知：
	\begin{equation*}
		\frac{[A(X^{\top}X)^-A^{\top}]^{-\frac{1}{2}}}{\sigma}(A\hat{\beta}-b)\sim\operatorname{N}_k\left\{\frac{[A(X^{\top}X)^-A^{\top}]^{-\frac{1}{2}}}{\sigma}(A\beta-b),I_k\right\}
	\end{equation*}
	于是由$\chi^2$分布的定义可得：
	\begin{equation*}
		\frac{\operatorname{SSE}_A-\operatorname{SSE}}{\sigma^2}=\frac{\{[A(X^{\top}X)^-A^{\top}]^{-\frac{1}{2}}(A\hat{\beta}-b)\}^{\top}[A(X^{\top}X)^-A^{\top}]^{-\frac{1}{2}}(A\hat{\beta}-b)}{\sigma^2}\sim\chi_{k,\alpha}^2
	\end{equation*}
	其中：
	\begin{align*}
		\alpha&=(\sigma^2)^{-1}\{[A(X^{\top}X)^-A^{\top}]^{-\frac{1}{2}}(A\beta-b)\}^{\top}[A(X^{\top}X)^-A^{\top}]^{-\frac{1}{2}}(A\beta-b) \\
		&=(\sigma^2)^{-1}(A\beta-b)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\beta-b)
	\end{align*}\par
	(3)由\cref{prop:ehat}(2)可知$\operatorname{SSE}=y^{\top}(I_n-P_X)y$，由(2)的过程和\cref{prop:Transpose}(4)可得：
	\begin{align*}
		\operatorname{SSE}_A-\operatorname{SSE}&=(A\hat{\beta}-b)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-b) \\
		&=[A(X^{\top}X)^-X^{\top}y-b]^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}[A(X^{\top}X)^-X^{\top}y-b] \\
		&=y^{\top}X(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}A(X^{\top}X)^-X^{\top}y \\
		&\quad-2b^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}A(X^{\top}X)^-X^{\top}y+b^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}b \\
		&=y^{\top}By-Cy+c
	\end{align*}
	因为$(I_n-P_X)X=\mathbf{0}$，由\cref{prop:OrthogonalProjectionMat}(7)(3)和\cref{prop:Transpose}(4)可得：
	\begin{gather*}
		(I_n-P_X)B=(I_n-P_X)X(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}A(X^{\top}X)^-X^{\top}=\mathbf{0} \\
		\begin{aligned}
			&C(I_n-P_X)=2b^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}A(X^{\top}X)^-X^{\top}(I_n-P_X) \\
			=&2b^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}A(X^{\top}X)^-[(I_n-P_X)X]^{\top}=\mathbf{0}
		\end{aligned}
	\end{gather*}
	于是根据\cref{theo:BXXAXIndependent}和\cref{theo:XAXXBXIndependent}可知$\operatorname{SSE}_A-\operatorname{SSE}$与$\operatorname{SSE}$独立。\par
	(4)当$A\beta=b$为真时，由(2)可知$\dfrac{\operatorname{SSE}_A-\operatorname{SSE}}{\sigma^2}\sim\chi^2_{k}$。根据(3)和\cref{prop:NormalLinearModel}(2)可得：
	\begin{equation*}
		\frac{(\operatorname{SSE}_A-\operatorname{SSE})/(k\sigma^2)}{\operatorname{SSE}/[(n-r)\sigma^2]}=\frac{(\operatorname{SSE}_A-\operatorname{SSE})/k}{\operatorname{SSE}/(n-r)}\sim\operatorname{F}_{k,n-r}
	\end{equation*}
	由(1)可得：
	\begin{equation*}
		\frac{(\operatorname{SSE}_A-\operatorname{SSE})/k}{\operatorname{SSE}/(n-r)}=\frac{n-r}{k}[\lambda^{\frac{2}{n}}(y)-1]
	\end{equation*}
	所以它是$\lambda(y)$的单调增函数。\par
	(5)由(1)(4)可立即得出。
\end{proof}

\subsection{置信域}
\subsubsection{置信椭球}
\begin{theorem}\label{theo:NormalLinearModelConfidenceEllipsoid}
	对于\cref{model:NormalLinearModel}，若不能接受假设$A\beta=\mathbf{0}$，则$A\beta$置信水平为$1-\alpha$的置信椭球为：
	\begin{equation*}
		\{A\beta:(A\beta-A\hat{\beta})^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\beta-A\hat{\beta})\leqslant k\hat{\sigma}^2F_{k,n-r}(1-\alpha)\}
	\end{equation*}
\end{theorem}
\begin{proof}
	由\cref{prop:NormalLinearModel}(2)可知：
	\begin{equation*}
		A\hat{\beta}\sim\operatorname{N}[A\beta,\sigma^2A(X^{\top}X)^-A^{\top}]
	\end{equation*}
	所以：
	\begin{equation*}
		\frac{A\hat{\beta}-A\beta}{\sigma}\sim\operatorname{N}[\mathbf{0},A(X^{\top}X)^-A^{\top}]
	\end{equation*}
	因为：
	\begin{equation*}
		[A(X^{\top}X)^-A^{\top}]^{-1}A(X^{\top}X)^-A^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}=[A(X^{\top}X)^-A^{\top}]^{-1}
	\end{equation*}
	所以由\cref{theo:XAXChi2}可知：
	\begin{equation*}
		\frac{(A\hat{\beta}-A\beta)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-A\beta)}{\sigma^2}\sim\chi_{k}^2
	\end{equation*}
	根据\cref{prop:NormalLinearModel}(2)(3)可得：
	\begin{align*}
		&\frac{(A\hat{\beta}-A\beta)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-A\beta)}{k\sigma^2}\Big/\frac{(n-r)\hat{\sigma}^2}{(n-r)\sigma^2} \\
		=&\frac{(A\hat{\beta}-A\beta)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-A\beta)}{k\hat{\sigma}^2}\sim F_{k,n-r}
	\end{align*}
	所以对任意的$0<\alpha<1$，有：
	\begin{equation*}
		P\left[\frac{(A\hat{\beta}-A\beta)^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\hat{\beta}-A\beta)}{k\hat{\sigma}^2}\leqslant F_{k,n-r}(1-\alpha)\right]=1-\alpha
	\end{equation*}
	即$A\beta$置信水平为$1-\alpha$的置信椭球为：
	\begin{equation*}
		\{A\beta:(A\beta-A\hat{\beta})^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\beta-A\hat{\beta})\leqslant k\hat{\sigma}^2F_{k,n-r}(1-\alpha)\}\qedhere
	\end{equation*}
\end{proof}
\begin{note}
	这里其实是一个未知方差构造$F$分布的思想。
\end{note}
\subsubsection{Scheffe置信区间}
\begin{theorem}\label{theo:ScheffeCI}
	对于\cref{model:NormalLinearModel}，对任何可估函数$l^{\top}\beta$，其中$l\in\mathcal{M}(A^{\top})$且$l\ne\mathbf{0}$，其置信水平为$1-\alpha$的同时置信区间为：
	\begin{equation*}
		l^{\top}\hat{\beta}\pm[k\hat{\sigma}^2F_{k,n-r}(1-\alpha)l^{\top}(X^{\top}X)^-l]^{\frac{1}{2}}
	\end{equation*}
\end{theorem}
\begin{proof}
	由\cref{theo:NormalLinearModelConfidenceEllipsoid}、\info{$A(X^{\top}X)^-A^{\top}$的正定性}与\cref{ineq:cauchy-schiwarz-rayleigh}可知：
	\begin{align*}
		1-\alpha&=P\left[(A\beta-A\hat{\beta})^{\top}[A(X^{\top}X)^-A^{\top}]^{-1}(A\beta-A\hat{\beta})\leqslant k\hat{\sigma}^2F_{k,n-r}(1-\alpha)\right] \\
		&=P\left\{\sup_{b\ne\mathbf{0}}\frac{[(A\hat{\beta}-A\beta)^{\top}b]^2}{b^{\top}A(X^{\top}X)^-A^{\top}b}\leqslant k\hat{\sigma}^2F_{k,n-r}(1-\alpha)\right\} \\
		&=P\left\{\frac{[(A\hat{\beta}-A\beta)^{\top}b]^2}{b^{\top}A(X^{\top}X)^-A^{\top}b}\leqslant k\hat{\sigma}^2F_{k,n-r}(1-\alpha),\;\text{对任意的}b\ne\mathbf{0}\right\} \\ 
		&=P\left\{|(A\hat{\beta}-A\beta)^{\top}b|\leqslant [k\hat{\sigma}^2F_{k,n-r}(1-\alpha)b^{\top}A(X^{\top}X)^-A^{\top}b]^{\frac{1}{2}},\;\text{对任意的}b\ne\mathbf{0}\right\} \\
		&=P\left\{|\hat{\beta}^{\top}A^{\top}b-\beta^{\top}A^{\top}b|\leqslant [k\hat{\sigma}^2F_{k,n-r}(1-\alpha)b^{\top}A(X^{\top}X)^-A^{\top}b]^{\frac{1}{2}},\;\text{对任意的}b\ne\mathbf{0}\right\}
	\end{align*}
	记$A^{\top}b=l$，因为$\mathcal{M}(A^{\top})\subseteq\mathcal{M}(X^{\top})$，由\cref{prop:EstimableFunction}(1)可知$l^{\top}\beta$也是一个可估函数，于是有：
	\begin{equation*}
		1-\alpha=P\left\{|l^{\top}\hat{\beta}-l^{\top}\beta|\leqslant [k\hat{\sigma}^2F_{k,n-r}(1-\alpha)l^{\top}(X^{\top}X)^-l]^{\frac{1}{2}},\;\text{对任意的}l\in\mathcal{M}(A^{\top})\text{且}l\ne\mathbf{0}\right\}\qedhere
	\end{equation*}
\end{proof}
\subsubsection{Bonferroni置信区间}
\begin{theorem}\label{theo:BonferroniCI}
	对于\cref{model:NormalLinearModel}，记$A$的行分别为$\seq{a^{\top}}{k}$，则$a_
	i^{\top}\beta$置信水平为$1-\alpha$的Bonferroni置信区间为：
	\begin{equation*}
		a_i^{\top}\hat{\beta}\pm t_{n-r}\left(1-\frac{\alpha}{2k}\right)[\hat{\sigma}^2a_i^{\top}(X^{\top}X)^-a_i]^{\frac{1}{2}}
	\end{equation*}
\end{theorem}
\begin{proof}
	由\cref{theo:NormalLinearModelConfidenceEllipsoid}可得当$k=1$时有：
	\begin{align*}
		1-\alpha&=P\left\{(a_i^{\top}\beta-a_i^{\top}\hat{\beta})^{\top}[a_i^{\top}(X^{\top}X)^-a_i]^{-1}(a_i^{\top}\beta-a_i^{\top}\hat{\beta})\leqslant\hat{\sigma}^2F_{1,n-r}(1-\alpha)\right\} \\
		&=P\left\{(a_i^{\top}\beta-a_i^{\top}\hat{\beta})^2\leqslant\hat{\sigma}^2F_{1,n-r}(1-\alpha)[a_i^{\top}(X^{\top}X)^-a_i]\right\}
	\end{align*}
	由\cref{prop:FDistribution}(2)可知$F_{1,n-r}=t^2_{n-r}$，因为服从$\operatorname{t}$分布的变量可取负值而服从$\operatorname{F}$分布的变量只能为正值，所以上式把平方变成绝对值时应修改对应的$\alpha$为$\dfrac{a}{2}$，即此时：
	\begin{equation*}
		1-\alpha=P\left\{|a_i^{\top}\beta-a_i^{\top}\hat{\beta}|\leqslant t_{n-r}\left(1-\frac{\alpha}{2}\right)[\hat{\sigma}^2a_i^{\top}(X^{\top}X)^-a_i]^{\frac{1}{2}}\right\}
	\end{equation*}
	由Bonferroni校正法可得出结论。
\end{proof}
\subsubsection{比较}
\begin{derivation}
	Scheffe区间与Bonferroni区间哪个更好？由二者的公式可以看出只需选择：
	\begin{equation*}
		\min\left\{[k\hat{\sigma}^2F_{k,n-r}(1-\alpha)]^{\frac{1}{2}},\hat{\sigma}t_{n-r}\left(1-\frac{\alpha}{2k}\right)\right\}
	\end{equation*}
	对应的方法即能得到更短的置信区间。
\end{derivation}

\subsection{区间预测}
\begin{theorem}
	延续\ref{def:LinearModelForcast}处的定义，假设$\varepsilon\sim\operatorname{N}(0,\sigma^2I_n),\;\varepsilon_0\sim\operatorname{N}(0,\sigma^2I_m)$，且有
	\begin{equation*}
		\operatorname{Cov}[(y,y_0)^{\top}]=\operatorname{Cov}[(\varepsilon,\varepsilon_0)^{\top}]=\sigma^2
		\begin{pmatrix}
			I_n & V^{\top} \\
			V & I_m 
		\end{pmatrix}
	\end{equation*}
	则$y_0^{(i)}$的Scheffe置信区间与Bonferroni置信区间分别为：
	\begin{equation*}
		\hat{y}_0^{(i)}\pm[m\hat{\sigma}^2F_{m,n-r}(1-\alpha)T_{ii}]^{\frac{1}{2}},\quad
		\hat{y}_0^{(i)}\pm t_{n-r}\left(1-\frac{\alpha}{2m}\right)\hat{\sigma}(T_{ii})^{\frac{1}{2}}
	\end{equation*}
	其中$y_0^{(i)},\hat{y}_0^{(i)}$表示的是$y_0,\hat{y}_0$的第$i$个分量，$\hat{y}_0$由\cref{theo:PECorrelatedLinearModelForcast}给出，$T_{ii}$表示矩阵$T$的$i,i$元，$T=(X_0-VX)(X^{\top}X)^-(X_0-VX)^{\top}+I_m-VV^{\top}$。
\end{theorem}
\begin{proof}
	由\cref{theo:PECorrelatedLinearModelForcast}、\cref{prop:MultiNormal}(2)、\cref{prop:CovMat}(3)(5)和\cref{prop:A-}(5)(7)(5)可得：
	\begin{equation*}
		\hat{y}_0=X_0\hat{\beta}+V(y-X\hat{\beta})\sim\operatorname{N}_m(X_0\beta,\sigma^2VV^{\top})
	\end{equation*}
	\begin{align*}
		\operatorname{Cov}(\hat{y}_0)&=\operatorname{Cov}(Cy)=C\operatorname{Cov}(y)C^{\top}=\sigma^2CC^{\top} \\
		&=\sigma^2[X_0(X^{\top}X)^-X^{\top}+V-VX(X^{\top}X)^-X^{\top}] \\
		&\quad\cdot[X(X^{\top}X)^-X_0^{\top}+V^{\top}-X(X^{\top}X)^-X^{\top}V^{\top}] \\
		&=\sigma^2[X_0(X^{\top}X)^-X^{\top}X(X^{\top}X)^-X_0^{\top}+X_0(X^{\top}X)^-X^{\top}V^{\top} \\
		&\quad-X_0(X^{\top}X)^-X^{\top}X(X^{\top}X)^-X^{\top}V^{\top}+VX(X^{\top}X)^-X_0^{\top} \\
		&\quad+VV^{\top}-VX(X^{\top}X)^-X^{\top}V^{\top}-VX(X^{\top}X)^-X^{\top}X(X^{\top}X)^-X_0^{\top} \\
		&\quad-VX(X^{\top}X)^-X^{\top}V^{\top}+VX(X^{\top}X)^-X^{\top}X(X^{\top}X)^-X^{\top}V^{\top}] \\
		&=\sigma^2[X_0(X^{\top}X)^-X_0^{\top}+X_0(X^{\top}X)^-X^{\top}V^{\top}-X_0(X^{\top}X)^-X^{\top}V^{\top} \\
		&\quad+VX(X^{\top}X)^-X_0^{\top}+VV^{\top}-VX(X^{\top}X)^-X^{\top}V^{\top} \\
		&\quad-VX(X^{\top}X)^-X_0^{\top}-VX(X^{\top}X)^-X^{\top}V^{\top}+VX(X^{\top}X)^-X^{\top}V^{\top}] \\
		&=\sigma^2[X_0(X^{\top}X)^-X_0^{\top}+VV^{\top}-VX(X^{\top}X)^-X^{\top}V^{\top}]
	\end{align*}
	\begin{align*}
		\operatorname{Cov}(\hat{y}_0,y_0)
		&=\operatorname{Cov}[X_0\hat{\beta}+V(y-X\hat{\beta}),y_0]=\operatorname{Cov}[Vy+(X_0-VX)\hat{\beta},y_0] \\
		&=V\operatorname{Cov}(y,y_0)+(X_0-VX)\operatorname{Cov}(\hat{\beta},y_0) \\
		&=V\operatorname{Cov}(y,y_0)+(X_0-VX)\operatorname{Cov}[(X^{\top}X)^-X^{\top}y,y_0] \\
		&=V\operatorname{Cov}(y,y_0)+(X_0-VX)(X^{\top}X)^-X^{\top}\operatorname{Cov}\operatorname{Cov}(y,y_0) \\
		&=[V+(X_0-VX)(X^{\top}X)^-X^{\top}]\operatorname{Cov}(y,y_0) \\
		&=\sigma^2[V+(X_0-VX)(X^{\top}X)^-X^{\top}]V^{\top}
	\end{align*}
	令$A=X_0(X^{\top}X)^-X_0^{\top}+VV^{\top}-VX(X^{\top}X)^-X^{\top}V^{\top},\;B=[V+(X_0-VX)(X^{\top}X)^-X^{\top}]V^{\top}$，所以：
	\begin{equation*}
		\begin{pmatrix}
			\hat{y}_0 \\
			y_0
		\end{pmatrix}\sim\operatorname{N}_{m+n}\left[
		\begin{pmatrix}
			X_0\beta \\
			X_0\beta
		\end{pmatrix},\sigma^2
		\begin{pmatrix}
			A & B \\
			B^{\top} & I_m
		\end{pmatrix}
		\right]
	\end{equation*}
	由\cref{prop:MultiNormal}(2)可知：
	\begin{equation*}
		\hat{y}_0-y_0=
		(1,-1)
		\begin{pmatrix}
			\hat{y}_0 \\
			y_0
		\end{pmatrix}\sim\operatorname{N}_m\{
		\mathbf{0},\sigma^2[(X_0-VX)(X^{\top}X)^-(X_0-VX)+I_m-VV^{\top}]\}
	\end{equation*}\par
	令$T=(X_0-VX)(X^{\top}X)^-(X_0-VX)^{\top}+I_m-VV^{\top}$，仿照\cref{theo:NormalLinearModelConfidenceEllipsoid}中的推导能得到此时$\hat{y}_0-y_0$置信水平为$1-\alpha$的置信椭球为：
	\begin{equation*}
		[\hat{y}_0-y_0:(\hat{y}_0-y_0)^{\top}(T)^{-1}(\hat{y}_0-y_0)\leqslant m\hat{\sigma}^2F_{m,n-r}(1-\alpha)]
	\end{equation*}
	其中$T$的可逆性由\info{T的可逆性证明}保证。\par
	由上述置信椭球，仿照\cref{theo:ScheffeCI}的推导，在其中取$b$为标准基向量即可得到$y_0^{(i)}$的Scheffe置信区间为：
	\begin{equation*}
		\hat{y}_0^{(i)}\pm[m\hat{\sigma}^2F_{m,n-r}(1-\alpha)T_{ii}]^{\frac{1}{2}}
	\end{equation*}\par
	同理可得到$y_0^{(i)}$的Bonferroni置信区间为：
	\begin{equation*}
		\hat{y}_0^{(i)}\pm t_{n-r}\left(1-\frac{\alpha}{2m}\right)\hat{\sigma}(T_{ii})^{\frac{1}{2}}\qedhere
	\end{equation*}
\end{proof}







