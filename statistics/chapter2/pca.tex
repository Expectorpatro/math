\section{主成分分析}

\gls{PCA}的目的是：对数据进行一个线性变换，在最大程度保留原始信息的前提下去除数据中彼此相关的信息。反映在变量上就是说，对所有的变量进行一个线性变换，使得变换后得到的变量彼此之间不相关，并且是所有可能的线性变换中方差最大的一些变量（我们认为方差体现了信息量的大小）。

\subsection{总体主成分分析}
\begin{definition}
	设$\mathbf{X}$是一个$n$维随机向量，其均值向量为$\mu=(\seq{\mu}{n})$、协方差矩阵为$\Sigma=(\sigma_{ij}),\;i,j=1,2,\dots,n$。对$\mathbf{X}$进行一个线性变换$\mathcal{T}$得到一个$n$维随机向量$\mathbf{Y}=\seq{\mathbf{Y}}{n}$，$\mathcal{T}$的矩阵为：
	\begin{equation*}
		A=
		\begin{pmatrix}
			\alpha_1 \\
			\alpha_2 \\
			\vdots \\
			\alpha_n
		\end{pmatrix}
	\end{equation*}
	若：
	\begin{enumerate}
		\item $\operatorname{Cov}(\mathbf{Y})$是一个对角矩阵，即$\operatorname{Cov}(\mathbf{Y}_i,\mathbf{Y}_j)=0,\;i\ne j$；
		\item $\mathbf{Y}_1$是所有对$\mathbf{X}$进行线性变换后得到的随机变量中方差最大的随机变量，$\mathbf{Y}_2$是与$\mathbf{Y}_1$不相关的所有对$\mathbf{X}$进行线性变换后得到的随机变量中方差第二大的随机变量，以此类推。
	\end{enumerate}
	则分别称$\mathbf{Y}_1,\mathbf{Y}_2,\dots,\mathbf{Y}_n$是第一、第二、……、第$n$主成分。
\end{definition}
这一定义是否足够？
\begin{theorem}
	若不对$\mathcal{T}$的矩阵$A$作出相应的限制，对$\mathbf{X}$进行线性变换后得到的$\mathbf{Y}_i,\;i=1,2,\dots,n$的方差可以任意大。
\end{theorem}
\begin{proof}
	由\cref{prop:CovMat}(3)可知：
	\begin{equation*}
		\operatorname{Var}(\mathbf{Y}_i)=\operatorname{Cov}(\mathbf{Y})_{(i,i)}=\operatorname{Cov}(A\mathbf{X})_{(i,i)}=(A\Sigma A^T)_{i,i}=\alpha_i\Sigma\alpha_i^T
	\end{equation*}
	若$\operatorname{Var}(\mathbf{Y}_i)>0$，取矩阵$B=kA$，$\mathbf{Z}=kA\mathbf{X}$，则：
	\begin{equation*}
		\operatorname{Var}(\mathbf{Y}_i)=(k\alpha_i)\Sigma(k\alpha_i)^T=k^2\alpha_i\Sigma\alpha_i
	\end{equation*}
	改变$k$的值，即可对$\mathbf{Y}_i,\;i=1,2,\dots,n$的方差进行任意的放缩。
\end{proof}
因此，我们需要对$A$进行相应的限制，在这里我们人为地选择要求$A$是一个正交矩阵，也就是让$\alpha_i\alpha_i^T=1$。
\begin{definition}
	设$\mathbf{X}$是一个$n$维随机向量，其均值向量为$\mu=(\seq{\mu}{n})$、协方差矩阵为$\Sigma=(\sigma_{ij}),\;i,j=1,2,\dots,n$。对$\mathbf{X}$进行一个线性变换$\mathcal{T}$得到一个$n$维随机向量$\mathbf{Y}=\seq{\mathbf{Y}}{n}$，$\mathcal{T}$的矩阵为：
	\begin{equation*}
		A=
		\begin{pmatrix}
			\alpha_1 \\
			\alpha_2 \\
			\vdots \\
			\alpha_n
		\end{pmatrix}
	\end{equation*}
	若：
	\begin{enumerate}
		\item $AA^T=I$；
		\item $\operatorname{Cov}(\mathbf{Y})$是一个对角矩阵，即$\operatorname{Cov}(\mathbf{Y}_i,\mathbf{Y}_j)=0,\;i\ne j$；
		\item $\mathbf{Y}_1$是所有对$\mathbf{X}$进行线性变换后得到的随机变量中方差最大的随机变量，$\mathbf{Y}_2$是与$\mathbf{Y}_1$不相关的所有对$\mathbf{X}$进行线性变换后得到的随机变量中方差第二大的随机变量，以此类推。
	\end{enumerate}
	则分别称$\mathbf{Y}_1,\mathbf{Y}_2,\dots,\mathbf{Y}_n$是第一、第二、……、第$n$主成分。
\end{definition}
\begin{theorem}\label{theo:PCA}
	设$\mathbf{X}$是一个$n$维随机向量，$\Sigma$是其协方差矩阵，$\Sigma$的特征值\footnote{若特征多项式有重根，则标准正交化特征向量组不唯一，主成分也不唯一。}从大到小记作$\seq{\lambda}{n}$，$\seq{\varphi}{n}$为对应的标准正交化特征向量，则$\mathbf{X}$的第$i$个主成分以及其方差为：
	\begin{equation*}
		\mathbf{Y}_i=\varphi_i^T\mathbf{X},\;\operatorname{Var}(\mathbf{Y}_i)=\varphi_i^T\Sigma\varphi_i=\lambda_i
	\end{equation*}
\end{theorem}
\begin{proof}
	考虑到：
	\begin{equation*}
		\operatorname{Var}(\mathbf{Y}_i)=\alpha_i\Sigma\alpha_i^T,\quad
		\operatorname{Cov}(\mathbf{Y}_i,\mathbf{Y}_j)=\alpha_i\Sigma\alpha_j^T
	\end{equation*}
	求解主成分的过程即为求解：
	\begin{gather*}
		\alpha_i=\arg\max\alpha_i\Sigma\alpha_i^T \\
		\operatorname{s.t.}
		\begin{cases}
			||\alpha_i||=1,\;&i=1,2,\dots,n\\
			\alpha_i\Sigma\alpha_j=0,\;&j<i
		\end{cases}
	\end{gather*}
	由\cref{theo:maxminxAx/xx}可知上述结论成立。
\end{proof}
\begin{definition}
	将第$i$个主成分$\mathbf{Y}_i$与变量$\mathbf{X}_j$的相关系数$\rho(\mathbf{Y}_i,\mathbf{X}_j)$称为\gls{FactorLoading}。可推得：
	\begin{equation*}
		\rho(\mathbf{Y}_i,\mathbf{X}_j)=\frac{\sqrt{\lambda_i}\alpha_{ij}}{\sqrt{\sigma_{jj}}},\;i,j=1,2,\dots,n
	\end{equation*}
\end{definition}
\begin{derivation}\label{der:FactorLoading}
	由相关系数的定义：
	\begin{align*}
		\rho(\mathbf{Y}_i,\mathbf{X}_j)
		&=\frac{\operatorname{Cov}(\mathbf{Y}_i,\mathbf{X}_j)}{\sqrt{\operatorname{Var}(\mathbf{Y}_i)\operatorname{Var}(\mathbf{X}_j)}}=\frac{\operatorname{Cov}(\alpha_i\mathbf{X},e_j^T\mathbf{X})}{\sqrt{\lambda_i\sigma_{jj}}} \\
		&=\frac{\alpha_i\Sigma e_j}{\sqrt{\lambda_i\sigma_{jj}}}=\frac{e_j^T\Sigma\alpha_i}{\sqrt{\lambda_i\sigma_{jj}}}=\frac{e_j^T\lambda_i\alpha_i}{\sqrt{\lambda_i\sigma_{jj}}}=\frac{\sqrt{\lambda_i}\alpha_{ij}}{\sqrt{\sigma_{jj}}}
	\end{align*}
\end{derivation}
\begin{property}
	总体主成分具有如下性质：
	\begin{enumerate}
		\item $\operatorname{Cov}(\mathbf{Y})=\operatorname{diag}\{\seq{\lambda}{n}\}$；
		\item $\mathbf{Y}$的方差之和等于$\mathbf{X}$的方差之和，即$\sum\limits_{i=1}^{n}\lambda_i=\sum\limits_{i=1}^{n}\sigma_{ii}$；
		\item 第$i$个主成分与原变量的因子负荷量满足：
		\begin{equation*}
			\sum_{j=1}^{n}\sigma_{jj}\rho^2(\mathbf{Y}_i,\mathbf{X}_j)=\lambda_i
		\end{equation*}
		\item 原变量的第$j$个分量与所有主成分的因子负荷量满足：
		\begin{equation*}
			\sum_{i=1}^{n}\rho^2(\mathbf{Y}_i,\mathbf{X}_j)=1
		\end{equation*}
	\end{enumerate}
\end{property}
\begin{proof}
	(1)由\cref{theo:PCA}直接可得。\par
	(2)由\cref{prop:CovMat}(3)和\cref{prop:Trace}(3)可得：
	\begin{align*}
		\sum_{i=1}^{n}\operatorname{Var}(\mathbf{Y}_i)
		&=\operatorname{tr}[\operatorname{Cov}(\mathbf{Y})]=\operatorname{tr}[\operatorname{Cov}(A\mathbf{X})]=\operatorname{tr}(A\Sigma A^T) \\
		&=\operatorname{tr}(\Sigma A^TA)=\operatorname{tr}(\Sigma)=\sum_{i=1}^{n}\operatorname{Var}(\mathbf{X}_i)
	\end{align*}\par
	(3)由\cref{der:FactorLoading}可得：
	\begin{equation*}
		\sum_{j=1}^{n}\sigma_{jj}\rho^2(\mathbf{Y}_i,\mathbf{X}_j)=\sum_{j=1}^{n}\lambda_i\alpha_{ij}^2=\lambda_i\alpha_i\alpha_i^T=\lambda_i
	\end{equation*}\par
	(4)因为$A$是正交矩阵，所以$A$可逆，于是$\mathbf{X}$可以表示为$\seq{\mathbf{Y}}{n}$的线性组合，所以二者的复相关系数为$1$。由\info{复相关系数性质}可直接得出结论。
\end{proof}
\begin{definition}
	称第$i$个主成分$\mathbf{Y}_i$的方差与所有主成分方差之和为$\mathbf{Y}_i$的方差贡献率，记为$\eta_i$，即：
	\begin{equation*}
		\eta_i=\frac{\lambda_i}{\sum\limits_{j=1}^{n}\lambda_j}
	\end{equation*}
	将：
	\begin{equation*}
		\frac{\sum\limits_{i=1}^{k}\lambda_i}{\sum\limits_{i=1}^{n}\lambda_i}
	\end{equation*}
	称为主成分$\seq{\mathbf{Y}}{k}$的累计方差贡献率。
\end{definition}
\begin{definition}
	称主成分$\seq{\mathbf{Y}}{k}$与变量$\mathbf{X}_j$之间的复相关系数的平方$R^2$为$\seq{\mathbf{Y}}{k}$对$\mathbf{X}_j$的贡献率，其计算公式为：
	\begin{equation*}
		R^2=\sum_{i=1}^{k}\frac{\lambda_i\alpha_{ij}^2}{\sigma_{ii}}
	\end{equation*}
\end{definition}
\begin{derivation}
	由\cref{der:FactorLoading}和\info{链接复相关系数的性质}直接可得。
\end{derivation}
由前述，我们一般通过选择主成分的个数来实现对数据的降维，即选择主成分的个数使它们的累计方差贡献率达到一定比例（一般为$85\%$）。

\subsection{样本主成分分析}
假设对$n$维随机变量$\mathbf{X}$进行$m$次独立观测，得到$m$个$n$维样本$\seq{x}{m}$。在样本主成分分析中，我们使用样本来估计$\mathbf{X}$的协方差矩阵，即：
\begin{equation*}
	S=(s_{ij}),\;s_{ij}=\frac{1}{m-1}\sum_{k=1}^{m}(x_{ki}-\hat{\mathbf{X}}_i)(x_{kj}-\hat{\mathbf{X}}_j),\;i,j=1,2,\dots,n
\end{equation*}
其中：
\begin{equation*}
	\hat{\mathbf{X}}_i=\frac{1}{m}\sum_{j=1}^{m}x_{ji},\;i=1,2,\dots,n
\end{equation*}
其余步骤与总体主成分分析一致。

\subsection{注意事项}
\subsubsection{多重共线性问题}
当原始变量出现多重共线性时，PCA的效果会受到影响，这是因为重复的信息在方差占比中重复进行了计算。我们可以通过计算协方差矩阵的最小特征值\info{写线性模型的时候再把多重共线性推导链接过来}来判断是否出现多重共线性的情况。若最小特征值趋于$0$，则需要对纳入研究的变量进行考察与筛选。
\subsubsection{相关矩阵导出主成分}
上面我们都是对协方差矩阵的特征值分解进行计算，但在现实中，我们可能会对数据进行标准化处理来消除量纲带来的影响，注意到标准化后数据的协方差矩阵即为相关矩阵，此时将相关矩阵作对应的特征值分解即可。但需要注意：标准化后\textbf{各变量方差相等均为$1$，损失了部分信息}，所以会使得\textbf{标准化后的各变量在对主成分构成中的作用趋于相等}。因此，取值范围在同量级的数据建议使用协方差矩阵直接求解主成分，若变量之间数量级差异较大，再使用相关矩阵求解主成分。
\subsubsection{具体算法}
\begin{algorithm}[H]
	\caption{主成分分析（PCA）}
	\begin{algorithmic}[1]
		\State \textbf{Input:} 原始数据矩阵 $\mathbf{X}$
		\State \textbf{Output:} 主成分得分 $\mathbf{Y}$，选定的主成分个数 $k$
		
		\If{变量量纲差异明显}
		\State 标准化：$\mathbf{Z} = \dfrac{X - \mu}{\sigma}$
		\Else
		\State 保留原始数据 $\mathbf{Z} = \mathbf{X}$
		\EndIf
		
		\If{存在多重共线性（最小特征值 $\approx 0$）}
		\State 删除或合并相关变量
		\Else
		\State 保留原始变量
		\EndIf
		
		\State 计算协方差矩阵：$\Sigma = \text{cov}(\mathbf{Z})$
		\State 对 $\Sigma$ 进行特征值分解：$\Sigma = A \Lambda A^T$
		\State 初始化 $k \gets 1$, 累计贡献率 $\gets 0$
		
		\Repeat
		\State 选择第 $k$ 个主成分
		\State 更新累计贡献率
		\If{累计贡献率 $< $ 阈值}
		\State $k \gets k + 1$
		\EndIf
		\Until{累计贡献率 $\ge$ 阈值}
		
		\State 计算主成分得分：$\mathbf{Y} = \mathbf{Z} A_k$（$A_k$维矩阵$A$的前$k$行构成的矩阵）
		\State 输出 $\mathbf{Y}$ 和 $k$
	\end{algorithmic}
\end{algorithm}








