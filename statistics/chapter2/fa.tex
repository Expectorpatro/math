\section{因子分析}

\begin{definition}
	设$\mathbf{X}$是一个可观测的$m$维随机向量，$\operatorname{E}(\mathbf{X})=\boldsymbol{\mu},\;\operatorname{Cov}(\mathbf{X})=\Sigma=(\sigma_{ij})$。\gls{FactorAnalysis}的数学模型为：
	\begin{gather*}
		\mathbf{X}=\boldsymbol{\mu}+AF+\varepsilon \\
		\begin{cases}
			\operatorname{E}(F)=\mathbf{0},\;\operatorname{Cov}(F)=I_n \\
			\operatorname{E}(\varepsilon)=\mathbf{0},\;\operatorname{Cov}(\varepsilon)=D=\operatorname{diag}\{\seq{\sigma^2}{m}\} \\
			\operatorname{Cov}(F,\varepsilon)=\mathbf{0}
		\end{cases}
	\end{gather*}
	其中$F=(\seq{f}{n})^T$是不可观测的$n$维随机变量，$\varepsilon$是不可观测的$m$维随机变量，分别称$F$和$\varepsilon$为\gls{CommonFactor}和\gls{SpecificFactor}。$A=(a_{ij})$是一个非随机矩阵，$a_{ij}$表示公共因子$f_j$、随机变量$\mathbf{X}_i$的因子载荷。$a_{1j},a_{2j},\dots,a_{ij}$中至少有两个不为$0$，否则可将$f_i$并入到$\varepsilon_i$中去；$\varepsilon_i$也仅出现在$\mathbf{X}_i$的表达式中。
\end{definition}
\begin{property}
	上述因子分析模型具有如下性质：
	\begin{enumerate}
		\item $\Sigma=AA^T+D$；
		\item 模型不受单位影响。若$\mathbf{X}^*=C\mathbf{X}$，则有：
		\begin{equation*}
			\mathbf{Y}=C\boldsymbol{\mu}+CAF+C\varepsilon=\boldsymbol{\mu}^*+A^*F+\varepsilon^*
		\end{equation*}
		\item 因子载荷不唯一；
		\item $\operatorname{Cov}(\mathbf{X},F)=A$，即$\operatorname{Cov}(\mathbf{X}_i,F_j)=a_{ij}$
		\item 令$h_i^2=\sum\limits_{j=1}^{n}a_{ij}^2$，则有：
		\begin{equation*}
			\operatorname{Var}(\mathbf{X}_i)=\sigma_{ii}=\sum_{j=1}^{n}a_{ij}^2+\sigma_i^2=h_i^2+\sigma_i^2,\;i=1,2,\dots,m
		\end{equation*}
		\item 令$g_j^2=\sum\limits_{i=1}^{m}a_{ij}^2$，则有：
		\begin{equation*}
			\sum_{i=1}^{m}\operatorname{Var}(\mathbf{X}_i)=\sum_{j=1}^{n}g_j^2+\sum_{i=1}^{n}\sigma_i^2
		\end{equation*}
	\end{enumerate}
\end{property}
\begin{proof}
	(1)由\cref{prop:CovMat}(3)(4)(5)可得：
	\begin{align*}
		\Sigma&=\operatorname{Cov}(\mathbf{X})=\operatorname{Cov}(\boldsymbol{\mu}+AF+\varepsilon,\boldsymbol{\mu}+AF+\varepsilon) \\
		&=\operatorname{Cov}(\boldsymbol{\mu},\boldsymbol{\mu}+AF+\varepsilon)+\operatorname{Cov}(AF,\boldsymbol{\mu}+AF+\varepsilon)+\operatorname{Cov}(\varepsilon,\boldsymbol{\mu}+AF+\varepsilon) \\
		&=\operatorname{Cov}(AF,\boldsymbol{\mu})+\operatorname{Cov}(AF)+\operatorname{Cov}(AF,\varepsilon)+\operatorname{Cov}(\mathbf{\varepsilon},\boldsymbol{\mu})+\operatorname{Cov}(\varepsilon,AF)+\operatorname{Cov}(\varepsilon) \\
		&=A\operatorname{Cov}(F)A^T+A\operatorname{Cov}(F,\varepsilon)+\operatorname{Cov}(\varepsilon,F)A^T+D \\
		&=AA^T+D
	\end{align*}\par
	(2)显然。\par
	(3)取正交矩阵$Q$，令$A^*=AQ$，$F^*=Q^TF$，由\info{期望的性质}、\cref{prop:CovMat}(3)则依然有：
	\begin{equation*}
		\operatorname{E}(F^*)=Q^T\operatorname{E}(F)=\mathbf{0},\;\operatorname{Cov}(F^*)=Q^T\operatorname{Cov}(F)Q=I_n,\;\mathbf{X}=\boldsymbol{\mu}+A^*F^*+\varepsilon
	\end{equation*}\par
	(4)由\cref{prop:CovMat}
	(5)由(1)即可得到结论。\par
	(6)由\cref{prop:CovMat}(1)、(1)和\cref{prop:Trace}(1)可得：
	\begin{align*}
		\sum_{i=1}^{m}\operatorname{Var}(\mathbf{X}_i)&=\operatorname{tr}[\operatorname{Cov}(\mathbf{X})]=\operatorname{tr}(AA^T+D)=\sum_{i=1}^{m}\sum_{j=1}^{n}a_{ij}^2+\sum_{i=1}^{n}\sigma_i^2 \\
		&=\sum_{j=1}^{n}\sum_{i=1}^{m}a_{ij}^2+\sum_{i=1}^{n}\sigma_i^2=\sum_{j=1}^{n}g_j^2+\sum_{i=1}^{n}\sigma_i^2\qedhere
	\end{align*}
\end{proof}
\begin{definition}
	称$h_i^2$为变量$\mathbf{X}_i$的\gls{CommonVariance}，它反映了公共因子对$\mathbf{X}_i$的方差贡献度。称$\sigma_i^2$为$\mathbf{X}_i$的\gls{SpecificVariance}，它反映了特殊因子$\varepsilon_i$对$\mathbf{X}_i$的方差贡献度。$g_j^2$可视为公共因子$f_j$对$\seq{\mathbf{X}}{m}$的总方差贡献度。
\end{definition}

\subsection{参数估计方法}
\subsubsection{主成分法}
\begin{method}
	设$\mathbf{X}$的协方差矩阵$\Sigma$，它的特征值从大到小依次为$\seq{\lambda}{m}$，对应的单位正交特征向量分别为$\seq{l}{m}$。于是$\Sigma$有分解式：
	\begin{equation*}
		\Sigma=
		\begin{pmatrix}
			l_1 & l_2 & \cdots &l_m
		\end{pmatrix}
		\begin{pmatrix}
			\lambda_1 & 0 & \cdots & 0 \\
			0 & \lambda_2 & \cdots & 0 \\
			\vdots & \vdots & \ddots & \vdots \\
			0 & 0 & \cdots & \lambda_m
		\end{pmatrix}
		\begin{pmatrix}
			l_1^T \\
			l_2^T \\
			\vdots \\
			l_m^T
		\end{pmatrix}
		=\sum_{i=1}^{m}\lambda_il_il_i^T
	\end{equation*}
	由\cref{prop:CovMat}(2)和\cref{theo:PositiveSemidefinite}(3)的第五条可知$\lambda_m\geqslant0$。当最后$m-p$个特征值较小时，$\Sigma$有如下近似：
	\begin{equation*}
		\Sigma=\sum_{i=1}^{m}\lambda_il_il_i^T\approx\sum_{i=1}^{p}\lambda_il_il_i^T+D=AA^T+D
	\end{equation*}
	其中：
	\begin{equation*}
		A=
		\begin{pmatrix}
			\sqrt{\lambda_1}l_1 & \cdots & \sqrt{\lambda_p}l_p
		\end{pmatrix},\;
		D=\operatorname{diag}\{\seq{\sigma^2}{n}\},\;
		\sigma_i^2=\sigma_{ii}-h_i^2
	\end{equation*}
	与PCA一样，一般通过使$\left(\sum\limits_{i=1}^{p}\lambda_i\right)/\left(\sum\limits_{i=1}^{n}\lambda_i\right)$大于一定比例来选择$p$的具体值。
\end{method}
\subsubsection{主因子法}
\begin{method}
	令$AA^T=\Sigma-D$。取$\seq{\hat{\sigma}^2}{m}$为特殊方差的合理初始估计（(1)全零，(2)取$\max\limits_{j\ne i}\sigma_{ij}$），则有：
	\begin{equation*}
		\widehat{AA^T}=
		\begin{pmatrix}
			\sigma_{11}-\hat{\sigma}_1^2 & \sigma_{12} & \cdots & \sigma_{1m} \\
			\sigma_{21} & \sigma_{22}-\hat{\sigma}_2^2 & \cdots & \sigma_{2m} \\
			\vdots & \vdots & \ddots & \vdots \\
			\sigma_{m1} & \sigma_{m2} & \cdots & \sigma_{mm}-\hat{\sigma}_m^2
		\end{pmatrix}
	\end{equation*}
	取$\widehat{AA^T}$前$p$个大于$0$的特征值，从大到小依次为$\seq{\hat{\lambda}}{p}$，对应的单位正交特征向量为$\seq{\hat{l}}{p}$，则有近似的：
	\begin{equation*}
		\hat{A}=
		\begin{pmatrix}
			\sqrt{\hat{\lambda}_1}\hat{l}_1 & \cdots & \sqrt{\hat{\lambda}_p}\hat{l}_p
		\end{pmatrix}
	\end{equation*}
	令$\hat{\sigma}_i^2=\sigma_{ii}-\hat{h}_i^2$，继续上面的迭代过程以得到稳定的近似解。
\end{method}
\begin{algorithm}
	\caption{主因子法求解因子分析}
	\begin{algorithmic}[1]
		\State \textbf{Input:} 协方差矩阵 $\Sigma$，初始特殊方差估计 $\hat{\sigma}^2_1, \ldots, \hat{\sigma}^2_m$，目标因子数 $p$
		\State \textbf{Output:} 因子载荷矩阵估计 $\hat{A}$，特殊方差估计 $\hat{\sigma}_i^2$
		
		\State 初始化 $\hat{\sigma}_i^2$ 为合理值
		\Repeat
		\State 构造矩阵 $\widehat{AA^T} = \Sigma - \operatorname{diag}(\hat{\sigma}_1^2, \ldots, \hat{\sigma}_m^2)$
		\State 对 $\widehat{AA^T}$ 做特征值分解，得到部分特征值 $\hat{\lambda}_1 \geqslant \cdots \geqslant \hat{\lambda}_p$，及对应单位正交特征向量 $\hat{l}_1, \ldots, \hat{l}_p$
		\State 构造因子载荷矩阵估计：
		$
		\hat{A}=(\hat{a}_{ij}) = \begin{pmatrix}
			\sqrt{\hat{\lambda}_1} \hat{l}_1 & \cdots & \sqrt{\hat{\lambda}_p} \hat{l}_p
		\end{pmatrix}
		$
		\State 令 $\hat{h}_i^2 = \sum\limits_{j=1}^p \hat{a}_{ij}^2$，更新 $\hat{\sigma}_i^2 = \sigma_{ii} - \hat{h}_i^2,\;i=1,2,\dots,m$
		\Until{特殊方差估计 $\hat{\sigma}_i^2$ 收敛或达到最大迭代次数}
	\end{algorithmic}
\end{algorithm}
\subsubsection{正态分布假设下的极大似然估计法}
\begin{derivation}
	若假设$F\sim N_n(\mathbf{0},I_n),\;\varepsilon\sim N_m(\mathbf{0},D)$，因为$F$和$\varepsilon$不相关，由\cref{theo:IndependentCorrelationNormal}可知$F$和$\varepsilon$独立。由\cref{theo:MatNormalLinearTransform}和\cref{theo:MultiNormalAdditivity}可得$\mathbf{X}\sim N_m(\boldsymbol{\mu},AA^T+D)$。对$\mathbf{X}$进行简单抽样获得$s$个样本，由\cref{theo:MultiNormalAdditivity}和\cref{theo:MultiNormalLinearTransform}可得这$s$个样本的均值$\bar{\mathbf{X}}\sim N_n\left(\boldsymbol{\mu},\dfrac{1}{s}(AA^T+D)\right)$。若样本均值为$\bar{x}$，则似然函数为：
	\begin{equation*}
		L(A,D)=(2\pi)^{-\frac{m}{2}}|\det[\frac{1}{s}(AA^T+D)]|^{-\frac{1}{2}}\exp\left\{-\frac{1}{2}(\bar{x}-\boldsymbol{\mu})^T\left[\frac{1}{s}(AA^T+D)\right]^{-1}(\bar{x}-\boldsymbol{\mu})\right\}
	\end{equation*}
	对数似然函数省去常数项即为：
	\begin{align*}
		\ln L(A,D)&=-\frac{1}{2}\ln\left\{\left|\det\left[\frac{1}{s}(AA^T+D)\right]\right|\right\}-\frac{1}{2}(\bar{x}-\boldsymbol{\mu})^T\left[\frac{1}{s}(AA^T+D)\right]^{-1}(\bar{x}-\boldsymbol{\mu}) \\
		&=-\frac{1}{2}\ln\left\{\frac{1}{s^m}\left|\det(AA^T+D)\right|\right\}-\frac{s}{2}(\bar{x}-\boldsymbol{\mu})^T(AA^T+D)^{-1}(\bar{x}-\boldsymbol{\mu})
	\end{align*}
\end{derivation}

\subsection{因子旋转}
\begin{align*}
	V_j&=\frac{1}{m}\sum_{i=1}^{m}(d_{ij}^2-\bar{d}_j)^2=\frac{1}{m}\sum_{i=1}^{m}\left(d_{ij}^2-\frac{1}{p}\sum_{i=1}^{m}d_{ij}^2\right) \\
	&=\frac{1}{m}\left[\sum_{i=1}^{m}d_{ij}^4-m\frac{1}{m^2}\left(\sum_{i=1}^{m}d_{ij}^2\right)^2\right] \\
	&=\frac{1}{m^2}\left[m\sum_{i=1}^{m}d_{ij}^4-\frac{1}{m}\left(\sum_{i=1}^{m}d_{ij}^2\right)^2\right] \\
	&=\frac{1}{m^2}\left[m\sum_{i=1}^{m}\frac{a_{ij}^4}{h_i^4}-\frac{1}{m}\left(\sum_{i=1}^{m}\frac{a_{ij}^2}{h_i^2}\right)^2\right]
\end{align*}