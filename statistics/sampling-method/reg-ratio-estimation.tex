\section{回归估计}
\begin{definition}
	在SRS下，利用$\{\seq{Y}{N}\}$和个体特征$\{\seq{X}{N}\}$的线性关系$Y_i=\beta_1X_i+\beta_0$对总体相关信息进行估计的方法被称为\gls{RegEstimate}，当$\beta_0=0$时，也称之为\gls{RatioEstimate}。称该个体特征为\gls{AuxiliaryVariable}。
	\gls{RatioEstimate}使用SRS来估计两个个体特征$\{\seq{Y}{N}\}$与$\{\seq{X}{N}\}$之间的比值：
	\begin{equation*}
		\beta=\frac{\mu_Y}{\mu_X}=\frac{\tau_Y}{\tau_X}
	\end{equation*}
	或者通过估计$\mu_X(\tau_X)$和$\beta$去估计$\mu_Y=\beta\mu_X(\tau_Y=\beta\tau_X)$。
\end{definition}
\begin{note}
	请不要将比例估计看作线性模型，两者不一样，比例估计仅仅只有上述$\beta$的定义式。
\end{note}
\begin{note}
	辅助变量需要满足以下条件：
	\begin{enumerate}
		\item 辅助变量的获得需要简单快捷。如果它的值都很难得到或者得不到那根本没办法作回归估计；
		\item 辅助变量需要和个体值之间存在高度的线性相关性。
	\end{enumerate}
	选择好辅助变量并获取数据后，可以去做辅助变量与样本单元值的线性回归，来检验它们是否满足高度线性相关性（这里其实假设了数据服从正态分布）。在比例估计中我们还要去检验截距是否为$0$。如果线性回归后的截距很小但不显著，我们可以认为满足要求；如果截距较大但不显著，我们认为是样本随机性带来的问题，可以认为截距满足为$0$的条件。
\end{note}
\begin{property}\label{prop:RatioEstimaor}
	若存在$\varepsilon,M>0$使得$\varepsilon<X_i<M$且$|Y_i|<M$，比例估计具有如下性质：
	\begin{enumerate}
		\item $\beta$具有如下点估计：
		\begin{equation*}
			\hat{\beta}=\frac{\hat{\mu}_Y}{\hat{\mu}_X}
		\end{equation*}
		该点估计具有如下性质\footnote{有偏估计我们一般不讨论方差，而去讨论$\operatorname{MSE}$。}\footnote{这里涉及到了一些渐进符号，$\operatorname{O}$的含义与数分中的无穷小记号是一致的，$\operatorname{O}_p$指的是概率收敛的无穷小，感兴趣可参考\cite{vander}第二章第二节中的内容，但请确保你真的懂测度论下的依概率收敛，学测度论之前概率论第四章两大收敛性我是一点不懂的。}：
		 \begin{gather*}
		 	\operatorname{E}(\hat{\beta}-\beta)=-\dfrac{\operatorname{Cov}(\hat{\beta}),\hat{\mu}_X}{\mu_X}=\operatorname{O}_p\left(\frac{1}{n}\right) \\
		 	\operatorname{MSE}(\hat{\beta})=\left(1-\frac{n}{N}\right)\frac{1}{n\mu_X^2}\frac{1}{N-1}\sum_{i=1}^{N}(Y_i-\beta X_i)^2+\operatorname{O}\left(\frac{1}{n^{\frac{3}{2}}}\right)=\operatorname{O}\left(\frac{1}{n}\right)
		 \end{gather*}
	\end{enumerate}
\end{property}
\begin{proof}
	(1)将$\hat{\beta}-\beta$变形：
	\begin{align*}
		\hat{\beta}-\beta&=(\hat{\beta}-\beta)\frac{\mu_X-\hat{\mu}_X+\hat{\mu}_X}{\mu_X}=\frac{(\hat{\beta}-\beta)(\mu_X-\hat{\mu}_X)}{\mu_X}+\frac{(\hat{\beta}-\beta)\hat{\mu}_X}{\mu_X} \\
		&=\frac{(\hat{\beta}-\beta)(\mu_X-\hat{\mu}_X)}{\mu_X}+\frac{\hat{\mu}_Y-\beta\hat{\mu}_X}{\mu_X}
	\end{align*}
	根据\cref{prop:MeasurableIntegral}(5)和\cref{prop:SRS}(3)可知：
	\begin{align*}
		&\operatorname{E}(\hat{\beta}-\beta)=\frac{\operatorname{E}[(\hat{\beta}-\beta)(\mu_X-\hat{\mu}_X)]}{\mu_X}+\frac{\operatorname{E}(\hat{\mu}_Y-\beta\hat{\mu}_X)}{\mu_X} \\
		=&-\frac{\operatorname{E}\{[\hat{\beta}-\operatorname{E}(\hat{\beta})+\operatorname{E}(\hat{\beta})-\beta](\hat{\mu}_X-\mu_X)\}}{\mu_X} \\
		=&-\frac{\operatorname{Cov}(\hat{\beta},\hat{\mu}_X)+[\operatorname{E}(\hat{\beta})-\beta]\operatorname{E}(\hat{\mu}_X-\mu_X)}{\mu_X}=-\frac{\operatorname{Cov}(\hat{\beta},\hat{\mu}_X)}{\mu_X}
	\end{align*}
	由\cref{prop:MeasurableIntegral}(2)(5)、\cref{ineq:cauchy-schiwarz-expectations}和\info{同阶无穷小运算法则}可得：
	\begin{align*}
		&|\operatorname{E}(\hat{\beta}-\beta)|\leqslant\operatorname{E}\left[\left|\frac{(\hat{\beta}-\beta)(\mu_X-\hat{\mu}_X)}{\mu_X}\right|\right]=\operatorname{E}\left[\left|\frac{(\hat{\mu}_Y-\beta\hat{\mu}_X)(\mu_X-\hat{\mu}_X)}{\hat{\mu}_X\mu_X}\right|\right] \\
		<&\frac{1}{\hat{\mu}_X\mu_X}\operatorname{E}[|(\hat{\mu}_Y-\beta\hat{\mu}_X)(\mu_X-\hat{\mu}_X)|]\leqslant\frac{1}{\hat{\mu}_X\mu_X}\sqrt{\operatorname{E}[(\hat{\mu}_Y-\beta\hat{\mu}_X)^2]\operatorname{E}[(\mu_X-\hat{\mu}_X)^2]} \\
		=&\frac{1}{\hat{\mu}_X\mu_X}\sqrt{\operatorname{O}\left(\frac{1}{n}\right)\operatorname{O}\left(\frac{1}{n}\right)}=\operatorname{O}\left(\frac{1}{n}\right)
	\end{align*}
	不对最后一个等式作解释，涉及到了渐进理论的一些东西（连续映射定理、Slutsky定理和概率收敛阶数的性质），对这方面感兴趣的话先学测度论然后可以参考\cite{vander}。由\info{同阶无穷小运算法则}即可得到$\operatorname{E}(\hat{\beta}-\beta)=\operatorname{O}_p\left(\dfrac{1}{n}\right)$。\par
	(2)对$(\hat{\beta}-\beta)^2$进行变形：
	\begin{equation*}
		(\hat{\beta}-\beta)^2=\left(\frac{\hat{\mu}_Y-\beta\hat{\mu}_X}{\hat{\mu}_X}\right)=\frac{(\hat{\mu}_Y-\beta\hat{\mu}_X)^2}{\hat{\mu}_X^2}\frac{\mu_X^2-\hat{\mu}_X^2+\hat{\mu}_X^2}{\mu_X^2}=\frac{(\hat{\mu}_Y-\beta\hat{\mu}_X)^2}{\mu_X^2}-\frac{(\hat{\mu}_Y-\beta\hat{\mu}_X)^2(\hat{\mu}_X^2-\mu_X^2)}{\hat{\mu}_X^2\mu_X^2}
	\end{equation*}
	由\cref{prop:MeasurableIntegral}(5)对上式两边求期望即可得到：
	\begin{equation*}
		\operatorname{E}[(\hat{\beta}-\beta)^2]=\frac{1}{\mu_X^2}\operatorname{E}\left[(\hat{\mu}_Y-\beta\hat{\mu}_X)^2\right]-\operatorname{E}\left[\frac{(\hat{\mu}_Y-\beta\hat{\mu}_X)^2(\hat{\mu}_X^2-\mu_X^2)}{\hat{\mu}_X^2\mu_X^2}\right]
	\end{equation*}
\end{proof}
\begin{property}
	\begin{enumerate}
		\item $\operatorname{Corr}(\hat{\mu}_X,\hat{\mu}_Y)=\operatorname{Corr}(X,Y)$；
		\item 回归估计$\beta_1,\beta_0$有如下点估计：
		\begin{equation*}
			\hat{\beta}_1=\frac{\sum\limits_{i=1}^{N}(X_i-\overline{x})(Y_i-\overline{y})Z_i}{\sum\limits_{i=1}^{N}(X_i-\overline{x})^2Z_i}=\frac{s_y\hat{R}}{s_x},\quad
			\hat{\beta}_0=\overline{y}-\hat{\beta}_1\overline{x}
		\end{equation*}
		该点估计具有如下性质：
		\begin{gather*}
			\beta_1-\hat{\beta}_1=\operatorname{MSE}(\hat{\beta}_1)=\operatorname{O}\left(\frac{1}{n}\right)
		\end{gather*}
	\end{enumerate}
\end{property}
\begin{proof}
	(1)由\cref{prop:CovMat}(3)(5)将协方差进行展开可得：
	\begin{align*}
		&\operatorname{Cov}(\hat{\mu}_X,\hat{\mu}_Y)
		=\operatorname{Cov}\left(\frac{1}{n}\sum_{i=1}^NX_iZ_i,\frac{1}{n}\sum_{j=1}^NY_jZ_j\right) \\
		=&\frac{1}{n^2}\left[\sum_{i=1}^NX_iY_i\operatorname{Var}(Z_i)+\sum_{i=1}^N\sum_{j\ne i}^NX_iY_j\operatorname{Cov}(Z_i,Z_j)\right] \\
		=&\left(1-\frac{n}{N}\right)\frac{1}{nN}\frac{1}{N-1}\left[(N-1)\sum_{i=1}^NX_iY_i-\sum_{i=1}^N\sum_{j\ne i}^NX_iY_j\right] \\
		=&\left(1-\frac{n}{N}\right)\frac{1}{nN}\frac{1}{N-1}\left[(N-1)\sum_{i=1}^NX_iY_i-\left(\sum_{i=1}^N\sum_{j=1}^NX_iY_j-\sum_{i=1}^NX_iY_i\right) \right] \\
		=&\left(1-\frac{n}{N}\right)\frac{1}{nN}\frac{1}{N-1}\left(N\sum_{i=1}^NX_iY_i-\sum_{i=1}^N\sum_{j=1}^NX_iY_j\right) \\
		=&\left(1-\frac{n}{N}\right)\frac{1}{nN}\frac{1}{N-1}\left(N\sum_{i=1}^NX_iY_i-\sum_{i=1}^NX_i\sum_{j=1}^NY_j\right) \\
		=&\left(1-\frac{n}{N}\right)\frac{1}{n}\frac{1}{N-1}\left(\sum_{i=1}^NX_iY_i-N\mu_X\mu_Y\right) \\
		=&\left(1-\frac{n}{N}\right)\frac{1}{n}\frac{1}{N-1}\left(\sum_{i=1}^NX_iY_i-2N\mu_X\mu_Y+N\mu_X\mu_Y\right) \\
		=&\left(1-\frac{n}{N}\right)\frac{1}{n}\frac{1}{N-1}\left(\sum_{i=1}^NX_iY_i-\sum_{i=1}^NX_i\mu_Y-\sum_{i=1}^NY_i\mu_X+N\mu_X\mu_Y\right) \\
		=&\left(1-\frac{n}{N}\right)\frac{1}{n}\frac{1}{N-1}\sum_{i=1}^N(X_iY_i-X_i\mu_Y-Y_i\mu_X+\mu_X\mu_Y) \\
		=&\left(1-\frac{n}{N}\right)\frac{1}{n}\frac{1}{N-1}\sum\limits_{i=1}^N(X_i-\mu_X)(Y_i-\mu_Y) =\left(1-\frac{n}{N}\right)\frac{1}{n}\operatorname{Cov}(X,Y)
	\end{align*}
	根据\cref{prop:SRS}(3)可得：
	\begin{align*}
		\operatorname{Corr}(\bar{x},\bar{y})&=\frac{\operatorname{Cov}(\bar{x},\bar{y})}{\sqrt{\operatorname{Var}(\bar{x})\operatorname{Var}(\bar{y})}} \\
		&=\frac{\left(1-\dfrac{n}{N}\right)\dfrac{1}{n}\operatorname{Cov}(X,Y)}{\sqrt{\left(1-\dfrac{n}{N}\right)^2\dfrac{1}{n^2}\operatorname{Var}(X)\operatorname{Var}(Y)}} \\
		&=\frac{\operatorname{Cov}(X,Y)}{\sqrt{\operatorname{Var}(X)\operatorname{Var}(Y)}} \\
		&=\operatorname{Corr}(X,Y)
	\end{align*}\par
	(2)令：
	\begin{gather*}
		\varepsilon_i=Y_i-\beta_1X_i-\beta_0,\;i=1,2,\dots,N \\ e_j=y_j-\beta_1x_j-\beta_0,\;j=1,2,\dots,n
	\end{gather*}
	
\end{proof}
\section{估计量}

\subsection{回归估计量}
\begin{definition}
	比例估计量有如下计算公式（其中$\mu_X$和$\tau_X$是已知的）：
	\begin{gather*}
		\hat{\mu}_{Y_{reg}}=\hat{B}_1\mu_X+\hat{B}_0=\hat{B}_1(\mu_X-\bar{x})+\bar{y},\quad
		\hat{\tau}_{Y_{reg}}=\hat{B}_1\tau_X+\hat{B}_0
	\end{gather*}
\end{definition}


\subsection{比例估计量}
\begin{definition}
	比例估计量有如下计算公式（其中$\mu_X$和$\tau_X$是已知的）：
	\begin{gather*}
		\hat{B}=\frac{\bar{y}}{\bar{x}}=\frac{\tau_y}{\tau_x} \\
		\hat{\mu}_{Yr}=\hat{B}\mu_X,\quad\hat{\tau}_{Yr}=\hat{B}\tau_X 
	\end{gather*}
\end{definition}
其中$\hat{B}$是比例系数$B$的估计\footnote{这是一个有偏估计！！！}，对于$B$的真实值，应有$B=\frac{\mu_Y}{\mu_X}=\frac{\tau_Y}{\tau_X}$。\footnote{依据辅助变量的选择原则，$X$与$Y$之间需要满足线性关系且截距为$0$，$B$其实就是线性关系中的斜率，同时证明了$Corr(\bar{x},\bar{y})=Corr(X,Y)$，因此$\hat{B}$和$B$既可以由均值来表示也可以由总体总量来表示。}
\section{偏差}
\subsection{回归估计的偏差}
由回归估计量计算公式显然可知回归估计是有偏的。
\subsection{比例估计的偏差}
\begin{theorem}
	比例估计量是有偏的，偏差如下：
	\begin{gather*}
		bias(\hat{\mu}_{Yr})=E(\hat{\mu}_{Yr})-\mu_Y=Cov(-\hat{B},\bar{x}) \\
		bias(\hat{\tau}_{Yr})=E(\hat{\tau}_{Yr})-\tau_Y=Cov(-\hat{B},\bar{x})N
	\end{gather*}
	上式不便于计算，可以用下式进行估计：
	\begin{gather*}
		bias(\hat{\mu}_{Yr})\approx\frac{1}{\mu_X}\left[BVar(\bar{x})-Cov(\bar{x},\bar{y})\right]=\left(1-\frac{n}{N}\right)\frac{1}{n\mu_X}[B\sigma_X^2-Corr(X,Y)\sigma_X\sigma_Y] \\
		bias(\hat{\tau}_{Yr})\approx\frac{\tau_X}{\mu_X^2}\left[BVar(\bar{x})-Cov(\bar{x},\bar{y})\right]=\left(1-\frac{n}{N}\right)\frac{\tau_X}{n\mu_X^2}[B\sigma_X^2-Corr(X,Y)\sigma_X\sigma_Y]
	\end{gather*}
\end{theorem}
\begin{proof}
	将协方差分解：
	\begin{align*}
		Cov(-\hat{B},\bar{x})&=-Cov(\hat{B},\bar{x}) \\
		&=-\left[E(\hat{B}\bar{x})-E(\hat{B})E(\bar{x})\right] \\
		&=-\left[E\left(\frac{\bar{y}}{\bar{x}}\bar{x}\right)-E(\hat{B})\mu_X\right] \\
		&=E(\hat{\mu}_{Yr})-E(\bar{y}) \\
		&=E(\hat{\mu}_{Yr})-\mu_Y
	\end{align*}
	下证近似公式：
	\begin{align*}
		bias(\hat{\mu}_{Yr})
		&=E(\hat{B}\mu_X)-\mu_Y \\
		&=\mu_XE\left(\frac{\bar{y}}{\bar{x}}-\frac{\mu_Y}{\mu_X}\right) \\
		&=\mu_XE\left(\frac{\bar{y}}{\mu_X}\frac{\mu_X}{\bar{x}}-\frac{\mu_Y}{\mu_X}\right) \\
		&=\mu_XE\left(\frac{\bar{y}}{\mu_X}-\frac{\bar{y}}{\mu_X}\frac{\bar{x}-\mu_X}{\bar{x}}-\frac{\mu_Y}{\mu_X}\right) \\
		&=\mu_XE\left(\frac{\bar{y}}{\mu_X}\frac{\mu_X-\bar{x}}{\bar{x}}\right) \\
		&=\mu_XE\left(\frac{\bar{y}}{\mu_X}\frac{\mu_X-\bar{x}}{\bar{x}}\frac{\mu_X}{\bar{x}}\frac{\bar{x}}{\mu_X}\right) \\
		&=\mu_XE\left[\frac{\bar{y}}{\mu_X^2}(\mu_X-\bar{x})\frac{\mu_X}{\bar{x}}\right] \\
		&=\mu_XE\left[\frac{\bar{y}}{\mu_X^2}(\mu_X-\bar{x})\left(1-\frac{\bar{x}-\mu_X}{\bar{x}}\right)\right] \\
		&=\mu_XE\left\{\frac{\bar{y}}{\mu_X^2}\left[(\mu_X-\bar{x})+\frac{(\bar{x}-\mu_X)^2}{\bar{x}}\right]\right\} \\
		&=\mu_XE\left[\frac{-\bar{y}(\bar{x}-\mu_X)}{\mu_X^2}+\frac{\bar{y}(\bar{x}-\mu_X)^2}{\mu_X^2\bar{x}}\right] \\
		&=\frac{1}{\mu_X}E\left[\frac{\bar{x}}{\bar{y}}(\bar{x}-\mu_X)^2-(\bar{x}-\mu_X)\bar{y}\right] \\
		&=\frac{1}{\mu_X}\left\{E[\hat{B}(\bar{x}-\mu_X)^2]-E[\bar{y}(\bar{x}-\mu_X)]\right\} \\
		&=\frac{1}{\mu_X}\left\{E[(\hat{B}-B+B)(\bar{x}-\mu_X)^2]-Cov(\bar{x},\bar{y})\right\} \\
		&=\frac{1}{\mu_X}\left\{E[(\hat{B}-B)(\bar{x}-\mu_X)^2+B(\bar{x}-\mu_X)^2]-Cov(\bar{x},\bar{y})\right\}
	\end{align*}
	由于$E\left[(\hat{B}-B)(\bar{x}-\mu_X)^2\right]$极小（严谨证明不提供），因此:
	\begin{align*}
		bias(\hat{\mu}_{Yr})&\approx\frac{1}{\mu_X}\left\{E[B(\bar{x}-\mu_X)^2]-Cov(\bar{x},\bar{y})\right\} \\
		&=\frac{1}{\mu_X}\left[BVar(\bar{x})-Cov(\bar{x},\bar{y})\right] \qedhere
	\end{align*}
\end{proof}
由此，在以下情况比例估计的偏倚会较小：
\begin{enumerate}
	\item n较大。
	\item $\frac{n}{N}$较大。
	\item $\mu_X$较大。
	\item $\sigma_x$较小。
	\item $R$接近$\pm 1$。
\end{enumerate}
\section{均方误差}

\subsection{回归估计量的均方误差}
\begin{theorem}
	回归估计量的均方误差为：
	\begin{align*}
		MSE(\hat{\mu}_{Y_{reg}})&= 
		E\left\lbrace\big[\bar{y}+\hat{B}_1(\mu_X-\bar{x})-\mu_Y\big]^2\right\rbrace  \\
		&\approx Var(\bar{d}) \\
		&=\left(1-\frac{n}{N}\right)\frac{\sigma_d^2}{n}
	\end{align*}
	其中：
	\begin{gather*}
		d_i=y_i-\big[\mu_Y+B_1(x_i-\mu_X)\big] \\
		\sigma_d^2=\frac{1}{N-1}\sum_{i=1}^N[y_i-\mu_Y-B_1(x_i-\mu_X)]^2=(1-R^2)\sigma_Y^2
	\end{gather*}
\end{theorem}
\begin{proof}
	下求$E(d)$：
	\begin{equation*}
		E(d)=\frac{1}{N}\left[\sum_{i=1}^Ny_i-N\mu_Y-B_1\sum_{i=1}^Nx_i+B_1N\mu_X\right]=0
	\end{equation*}
	因此：
	\begin{align*}
		\sigma_d^2&=\frac{1}{N-1}\sum_{i=1}^N\big[y_i-\mu_Y-B_1(x_i-\mu_X)\big]^2 \\
		&=\frac{1}{N-1}\left[\sum_{i=1}^N(y_i-\mu_Y)^2+\sum_{i=1}^NB_1^2(x_i-\mu_X)^2-\sum_{i=1}^N2B_i(y_i-\mu_Y)(x_i-\mu_X)\right] \\
		&=\sigma_Y^2+B_1^2\sigma_X^2-2B_1R\sigma_X\sigma_Y
	\end{align*}
	而：
	\begin{equation*}
		B_1=\frac{\sigma_YR}{\sigma_X}
	\end{equation*}
	所以：
	\begin{equation*}
		\sigma_d^2
		=\sigma_Y^2+B_1^2\sigma_X^2-2B_1R\sigma_X\sigma_Y 
		=\sigma_Y^2+\frac{\sigma_Y^2R^2}{\sigma_X^2}\sigma_X^2-2\frac{\sigma_YR}{\sigma_X}R\sigma_X\sigma_Y
		=\sigma_Y^2-R^2\sigma_Y^2
		=(1-R^2)\sigma_Y^2\qedhere
	\end{equation*}
\end{proof}
由此，在以下情况$MSE(\hat{\mu}_{Y_{reg}})$较小：
\begin{enumerate}
	\item $n$较大。
	\item $\frac{n}{N}$较大。
	\item $\sigma_Y$较小。
	\item $R$接近$\pm 1$。
\end{enumerate}


\subsection{比例估计量的均方误差}
\begin{theorem}
	比例估计量的均方误差为：
	\begin{align*}
		MSE(\hat{\mu}_{Yr})&\approx E\left[(\bar{y}-B\bar{x})^2\right] \\
		&=\left(1-\frac{n}{N}\right)\frac{\sigma_Y^2-2BR\sigma_X\sigma_Y+B^2\sigma_X^2}{n} \\
		&\approx Var(\hat{\mu}_{Yr})
	\end{align*}
\end{theorem}
证明过程可见David and Sukhatme, 1974。

\section{方差}
\subsection{回归估计的方差}
\begin{theorem}
	回归估计的方差为：
	\begin{equation*}
		Var(\hat{\mu}_{Y_{reg}})=Var(\bar{d})=\left(1-\frac{n}{N}\right)\frac{\sigma_d^2}{n}=\left(1-\frac{n}{N}\right)\frac{(1-R^2)\sigma_Y^2}{n}
	\end{equation*}
\end{theorem}
定义$e_i=y_i-(\hat{B}_1x_i+\hat{B}_0)$可得：
\begin{equation*}
	\widehat{Var}(\hat{\mu}_{Y_{reg}})=\left(1-\frac{n}{N}\right)\frac{s_e^2}{n}
\end{equation*}
这里$s_e^2$可取两种计算公式：
\begin{equation*}
	s_e^2=\frac{1}{n-1}\sum_{i=1}^ne_i^2=\left(1-\frac{n}{N}\right)\frac{(1-\hat{R}^2)s_y^2}{n},\quad
	s_e^2=\frac{1}{n-2}\sum_{i=1}^ne_i^2
\end{equation*}
第二种是考虑回归估计有两个待估参数，自由度为$n-2$，这样子做修正了回归中自由度的问题。\par
由上述总结：
\begin{gather*}
	\widehat{Var_1}(\hat{\mu}_{Y_{reg}})=\left(1-\frac{n}{N}\right)\frac{1}{n}\frac{1}{n-1}\sum_{i=1}^n\left[y_i-(\hat{B}_1x_i+\hat{B}_0)\right]^2 \\
	\widehat{Var_2}(\hat{\mu}_{Y_{reg}})=\left(1-\frac{n}{N}\right)\frac{1}{n}\frac{1}{n-2}\sum_{i=1}^n\left[y_i-(\hat{B}_1x_i+\hat{B}_0)\right]^2 
\end{gather*}

\subsection{比例估计的方差}
由Delta method（见\cref{sec:deltamethod}），注意到关系：
\begin{gather*}
	\hat{B}=\frac{\bar{y}}{\bar{x}}=g(\bar{x},\bar{y}) \\
	\hat{\mu}_{Yr}=\hat{B}\mu_X=g(\hat{B}) \\
	\hat{\tau}_{Yr}=\hat{\mu}_{Yr}\frac{\tau_X}{\mu_X}=\hat{\mu}_{Yr}N
\end{gather*}
可得以下比例估计量的近似方差($R=Corr(X,Y)$)：
\begin{gather*}
	Var(\hat{B})\approx\left(1-\frac{n}{N}\right)\frac{\sigma_Y^2-2BR\sigma_X\sigma_Y+B^2\sigma_X^2}{n\mu_X^2}=\left(1-\frac{n}{N}\right)\frac{\sigma_\varepsilon^2}{n\mu_X^2} \\
	\widehat{Var}(\hat{B})\approx\left(1-\frac{n}{N}\right)\frac{s_y^2-2\hat{B}\hat{R}s_xs_y+\hat{B}^2s_x^2}{n\bar{x}^2}=\left(1-\frac{n}{N}\right)\frac{s_e^2}{n\bar{x}^2} \\
	Var(\hat{\mu}_{Yr})\approx\left(1-\frac{n}{N}\right)\frac{\sigma_Y^2-2BR\sigma_X\sigma_Y+B^2\sigma_X^2}{n}=\left(1-\frac{n}{N}\right)\frac{\sigma_\varepsilon^2}{n}  \\
	\widehat{Var_1}(\hat{\mu}_{Yr})\approx\left(1-\frac{n}{N}\right)\frac{s_y^2-2\hat{B}\hat{R}s_xs_y+\hat{B}^2s_x^2}{n}=\left(1-\frac{n}{N}\right)\frac{s_e^2}{n}  \\
	Var(\hat{\tau}_{Yr})\approx N\left(N-n\right)\frac{\sigma_Y^2-2BR\sigma_X\sigma_Y+B^2\sigma_X^2}{n}=N(N-n)\frac{\sigma_\varepsilon^2}{n} \\
	\widehat{Var_1}(\hat{\tau}_{Yr})\approx N(N-n)\frac{s_y^2-2\hat{B}\hat{R}s_xs_y+\hat{B}^2s_x^2}{n}=N(N-n)\frac{s_e^2}{n}
\end{gather*}
再给出第二种总体均值、总体总量比例估计量抽样分布方差的估计：
\begin{gather*}
	\widehat{Var_2}(\hat{\mu}_{Yr})=\widehat{Var}(\hat{B}\mu_X)=\widehat{Var}(\hat{B})\mu_X^2\approx\left(1-\frac{n}{N}\right)\left(\frac{\mu_X}{\bar{x}}\right)^2\frac{s_e^2}{n} \\
	\widehat{Var_2}(\hat{\tau}_{Yr})\approx N(N-n)\left(\frac{\mu_X}{\bar{x}}\right)^2\frac{s_e^2}{n} 
\end{gather*}
下给出上述公式中所有等式的推导。
\begin{proof}
	从模型的角度，根据MSE的估计来看（最后一行是使用了SRS均值的方差公式）：
	\begin{gather*}
		\begin{aligned}
			Var(\hat{\mu}_{Yr})&\approx MSE(\hat{\mu}_{Yr})
			\approx E\left[(\bar{y}-B\bar{x})^2\right] \\
			&=E\left[\left(\frac{1}{n}\sum_{i=1}^ny_i-B\frac{1}{n}\sum_{i=1}^nx_i\right)^2\right] \\
			&=E\left\{\left[\frac{1}{n}\sum_{i=1}^n(y_i-Bx_i)\right]^2\right\}
		\end{aligned} \\
		\varepsilon_i=y_i-Bx_i,\;\mu_\varepsilon=E(\varepsilon_i)=0 \\
		\begin{aligned}
			Var(\hat{\mu}_{Yr})
			&\approx E\left\{\left[\frac{1}{n}\sum_{i=1}^n(y_i-Bx_i)\right]^2\right\} \\
			&=E\left[(\bar{\varepsilon})^2\right] \\
			&=E\left[(\bar{\varepsilon}-0)^2\right] \\
			&=E\left[(\bar{\varepsilon}-\mu_\varepsilon)^2\right] \\
			&=Var(\bar{\varepsilon}) \\
			&=\left(1-\frac{n}{N}\right)\frac{\sigma_\varepsilon^2}{n}
		\end{aligned}
	\end{gather*}
	对于$\sigma_\varepsilon^2$：
	\begin{align*}
		\sigma_\varepsilon^2
		&=\frac{1}{N-1}\left[\sum_{i=1}^Ny_i^2+B^2\sum_{i=1}^Nx_i^2-2B\sum_{i=1}^Nx_iyi\right] \\
		&=\frac{1}{N-1}\left[\sum_{i=1}^N(y_i-\mu_Y+\mu_Y)^2+B^2\sum_{i=1}^N(x_i-\mu_X+\mu_X)^2-2B\sum_{i=1}^Nx_iyi\right] \\
		&=\frac{1}{N-1}\left[\sum_{i=1}^N(y_i-\mu_Y)^2+2\sum_{i=1}^N(y_i-\mu_Y)\mu_Y+N\mu_Y^2\right.\\
		&\left.\qquad+B^2\sum_{i=1}^N(x_i-\mu_X)^2+2B^2\sum_{i=1}^N(x_i-\mu_X)\mu_X+B^2N\mu_X^2-2B\sum_{i=1}^Nx_iy_i\right] \\
		&=\sigma_Y^2+B^2\sigma_X^2+\frac{1}{N-1}\left[N\mu_Y^2+B^2N\mu_X^2-2B\sum_{i=1}^Nx_iy_i\right] \\
		&=\sigma_Y^2+B^2\sigma_X^2+\frac{1}{N-1}\left[N\mu_Y^2+B^2N\mu_X^2-2B\sum_{i=1}^N(x_i-\mu_X+\mu_X)(y_i-\mu_Y+\mu_Y)\right] \\
		&=\sigma_Y^2+B^2\sigma_X^2+\frac{1}{N-1}\left[N\mu_Y^2+B^2N\mu_X^2-2B\sum_{i=1}^N(x_i-\mu_X)(y_i-\mu_Y)-2BN\mu_X\mu_Y\right] \\
		&=\sigma_Y^2+B^2\sigma_X^2-2BCov(X,Y)+\frac{1}{N-1}\left[N\mu_Y^2+B^2N\mu_X^2-2BN\mu_X\mu_Y\right]
	\end{align*}
	因为：
	\begin{equation*}
		B=\frac{\mu_Y}{\mu_X}
	\end{equation*}
	所以：
	\begin{align*}
		N\mu_Y^2+B^2N\mu_X^2-2BN\mu_X\mu_Y
		&=N\mu_Y^2+\frac{\mu_Y^2}{\mu_X^2}N\mu_X^2-2\frac{\mu_Y}{\mu_X}N\mu_X\mu_Y \\
		&=2N\mu_Y^2-2N\mu_Y^2 \\
		&=0
	\end{align*}
	也就有：
	\begin{equation*}
		\sigma_\varepsilon^2=\sigma_Y^2-2BR\sigma_X\sigma_Y+B^2\sigma_X^2
	\end{equation*}
	令$e_i=y_i-\hat{B}x_i$，将它看作为$\varepsilon_i$的估计，则有：
	\begin{equation*}
		s_e^2 =s_y^2-2\hat{B}\hat{R}s_xs_y+\hat{B}^2s_x^2\qedhere
	\end{equation*}
\end{proof}

\section{置信区间}

\subsection{回归估计的置信区间}
由于比例估计量抽样分布的方差公式中存在未知量，大样本情况下可得如下估计的置信区间：
\begin{gather*}
	\hat{\mu}_{Y_{reg}}\pm u_{1-\frac{\alpha}{2}}\sqrt{\widehat{Var}(\hat{\mu}_{Y_{reg}})}
\end{gather*}

\subsection{比例估计的置信区间}
由于比例估计量抽样分布的方差公式中存在未知量，大样本情况下可得如下估计的置信区间：
\begin{gather*}
	\hat{B}\pm u_{1-\frac{\alpha}{2}}\sqrt{\widehat{Var}(\hat{B})} \\
	\hat{\mu}_{Yr}\pm u_{1-\frac{\alpha}{2}}\sqrt{\widehat{Var}(\hat{\mu}_{Yr})} \\
	\hat{\tau}_{Yr}\pm u_{1-\frac{\alpha}{2}}\sqrt{\widehat{Var}(\hat{\tau}_{Yr})}
\end{gather*}
\section{比例估计样本容量的选择}
\begin{equation*}
	n_r=\frac{Nu^2\sigma_\varepsilon^2}{u^2\sigma_\varepsilon^2+Nd^2}
\end{equation*}
\section{回归估计、比例估计与HT估计的比较}
回归估计与比例估计是有偏的，但它们的方差比HT估计小很多，MSE更小。



