\section{多元正态分布}

\subsection{多元正态分布的定义}
\begin{definition}\label{def:MultiNormal1}
	若一个随机向量$\mathbf{X}=(\mathbf{X}_1,\mathbf{X}_2,\dots,\mathbf{X}_n)^T\in\mathbb{R}^n$满足以下概率密度函数：
	\begin{equation}
		p(\mathbf{X}|\boldsymbol{\mu},\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}(\det\Sigma)^{\frac{1}{2}}}e^{-\frac{1}{2}(\mathbf{X}-\boldsymbol{\mu})^T\Sigma^{-1}(\mathbf{X}-\boldsymbol{\mu})}\notag
	\end{equation}
	则称其为一个正态随机向量，记作$\mathbf{X}\sim N_n(\boldsymbol{\mu},\;\Sigma)$。其中，$\boldsymbol{\mu}=(\seq{\mu}{n})^T\in\mathbb{R}^{n}$，$\Sigma\in M_{n}(\mathbb{R})$且$\Sigma>0$。
\end{definition}
\begin{theorem}
	对于正态随机向量的概率密度函数，$\boldsymbol{\mu}$和$\Sigma$分别是$\mathbf{X}$的均值向量和协方差矩阵。
\end{theorem}
\begin{proof}
	令：
	\begin{equation}
		\mathbf{Y}=\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu})\notag
	\end{equation}\par
	则有$\mathbf{X}=\Sigma^{\frac{1}{2}}\mathbf{Y}+\boldsymbol{\mu}$，由求随机变量函数的分布中的变量变换法可知\footnote{这是由于$\mathbf{X}$到$\mathbf{Y}$的变换是一个线性变换，线性变换让$\mathbf{X}$关于$\mathbf{Y}$有连续偏导数，同时这个线性变换可逆，满足变量变换法的两大条件。}\info{写完变量变换法链接过来}：
	\begin{equation}
		p(\mathbf{Y})=p(\Sigma^{\frac{1}{2}}\mathbf{Y}+\boldsymbol{\mu})|\mathbf{J}|\notag
	\end{equation}\par
	其中$\mathbf{J}$为变换的Jacobi行列式：
	\begin{equation}
		\mathbf{J}=
		\begin{vmatrix}
			\dfrac{\partial \mathbf{X}_1}{\partial \mathbf{Y}_1} & \cdots & \dfrac{\partial \mathbf{X}_1}{\partial \mathbf{Y}_n} \\
			\vdots & \ddots & \vdots \\
			\dfrac{\partial \mathbf{X}_n}{\partial \mathbf{Y}_1} & \cdots & \dfrac{\partial \mathbf{X}_n}{\partial \mathbf{Y}_n}
		\end{vmatrix}
		=\det\Sigma^{\frac{1}{2}}=(\det\Sigma)^{\frac{1}{2}}\notag
	\end{equation}\par
	那么$\mathbf{Y}$的概率密度函数为：
	\begin{equation*}
		p(\mathbf{Y})
		=\frac{1}{(2\pi)^{\frac{n}{2}}}e^{-\frac{1}{2}               \mathbf{Y}^T\mathbf{Y}}
		=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}}e^{-\frac{\mathbf{Y}_i^2}{2}}
	\end{equation*}\par
	对$\mathbf{Y}_i$求边缘分布可得$\mathbf{Y}_i\sim N(0,\;1)$，并且可以发现$\mathbf{Y}$的$n$个分量的联合密度等于每个分量密度函数的乘积，于是$\mathbf{Y}$的各个分量相互独立，所以有：
	\begin{equation*}
		E(\mathbf{Y})=\mathbf{0},\;Cov(\mathbf{Y})=\mathbf{I}
	\end{equation*}
	结合$\mathbf{Y}$与$\mathbf{X}$的关系，由\cref{prop:CovMat}可得：
	\begin{equation*}
		E(\mathbf{X})=\boldsymbol{\mu},\;Cov(\mathbf{X})=\Sigma \qedhere
	\end{equation*}
\end{proof}
\begin{definition}
	正态随机向量$\mathbf{X}$的概率密度函数也可写作：
	\begin{equation*}
		p(\mathbf{X})=\frac{1}{(2\pi)^{\frac{n}{2}}(\det\Sigma)^{\frac{1}{2}}}e^{-\frac{1}{2}\operatorname{tr}[(\mathbf{X}-\boldsymbol{\mu})(\mathbf{X}-\boldsymbol{\mu})^T\Sigma^{-1}]}
	\end{equation*} 
\end{definition}
\begin{proof}
	只需注意到二次型的迹就是自身以及\cref{prop:Trace}：
	\begin{equation*}
		(\mathbf{X}-\boldsymbol{\mu})^T\Sigma^{-1}(\mathbf{X}-\boldsymbol{\mu})=\operatorname{tr}[(\mathbf{X}-\boldsymbol{\mu})^T\Sigma^{-1}(\mathbf{X}-\boldsymbol{\mu})]=\operatorname{tr}[(\mathbf{X}-\boldsymbol{\mu})(\mathbf{X}-\boldsymbol{\mu})^T\Sigma^{-1}]\qedhere
	\end{equation*}
\end{proof}
\subsubsection{多元正态分布的等价定义}
\begin{definition}\label{def:MultiNormal2}
	$\mathbf{X}$为$n$维随机向量。若存在矩阵$A\in M_{n\times r}(\mathbb{R})$使得$\mathbf{X}=A\mathbf{U}+\boldsymbol{\mu}$，其中$\mathbf{U}=(\mathbf{U}_1,\mathbf{U}_2,\dots,\mathbf{U}_r)^T,\;\mathbf{U}_i\sim N(0,1)$且互相独立，$\boldsymbol{\mu}$为$n$维非随机实向量，则称$\mathbf{X}$为服从均值为$\boldsymbol{\mu}$、协方差矩阵为$\Sigma=AA^T$的多元正态向量，记为$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma)$，其中$\Sigma\geqslant0$。若$|\Sigma|=0$，则称此时的分布为\textbf{奇异正态分布}。
\end{definition}
\begin{theorem}
	$\mathbf{X}$是一个随机向量，其协方差矩阵为正定矩阵，则$\mathbf{X}$满足\cref{def:MultiNormal1}的充分必要条件是满足\cref{def:MultiNormal2}，即两种正态分布的定义在随机向量的协方差矩阵是正定矩阵的情形下是等价的。
\end{theorem}
\begin{proof}
	\textbf{(1)充分性：}
	设$\mathbf{X}$满足\cref{def:MultiNormal2}，因为$\mathbf{U}$中的元素服从标准正态分布且彼此独立，所以有：
	\begin{equation*}
		\operatorname{E}(\mathbf{U})=\mathbf{0},\;\operatorname{Cov}(\mathbf{U})=I
	\end{equation*}
	同时：
	\begin{equation*}
		p(\mathbf{U})
		=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}}e^{-\frac{\mathbf{U}_i^2}{2}} \\
		=\frac{1}{(2\pi)^{\frac{n}{2}}[\det \operatorname{Cov}(\mathbf{U})]^{\frac{1}{2}}}e^{-\frac{1}{2}\mathbf{U}^T[\operatorname{Cov}(\mathbf{U})]^{-1}\mathbf{U}}
	\end{equation*}
	因为$\mathbf{X}=A\mathbf{U}+\boldsymbol{\mu}$，由\info{期望的性质}和\cref{prop:CovMat}(3)(4)(5)可得：
	\begin{equation*}
		\operatorname{E}(\mathbf{X})=\boldsymbol{\mu},\;\operatorname{Cov}(\mathbf{X})=AA^T
	\end{equation*}
	因为$\operatorname{Cov}(\mathbf{X})>0$，由\cref{theo:PositiveDefinite}可得$\operatorname{rank}[\operatorname{Cov}(\mathbf{X})]=n$。因为$\operatorname{rank}(AB)\leqslant\min\{\operatorname{rank}(A),\operatorname{rank}(B)\}$\info{写完矩阵的秩做链接}，所以$\operatorname{rank}(A)=n$，即$r=n$，$A$是一个$n$阶可逆矩阵，于是$\mathbf{U}=A^{-1}(\mathbf{X}-\boldsymbol{\mu})$。由求随机变量函数的分布中的变量变换法可知\info{写完变量变换法链接过来}：
	\begin{align*}
		P(\mathbf{X})
		&=P[A^{-1}(\mathbf{X}-\boldsymbol{\mu})]|\mathbf{J}| \\
		&=\frac{1}{(2\pi)^{\frac{n}{2}}\{\det \operatorname{Cov}[A^{-1}(\mathbf{X}-\boldsymbol{\mu})]\}^{\frac{1}{2}}} \\
		&\quad\cdot e^{-\frac{1}{2}[A^{-1}(\mathbf{X}-\boldsymbol{\mu})]^T\{ \operatorname{Cov}[A^{-1}(\mathbf{X}-\boldsymbol{\mu})]\}^{-1}[A^{-1}(\mathbf{X}-\boldsymbol{\mu})]}|\det A^{-1}| \\
		&=\frac{1}{(2\pi)^{\frac{n}{2}}[\det \operatorname{Cov}(\mathbf{X})]^{\frac{1}{2}}}e^{-\frac{1}{2}(\mathbf{X}-\boldsymbol{\mu})^T[\operatorname{Cov}(\mathbf{X})]^{-1}(\mathbf{X}-\boldsymbol{\mu})}
	\end{align*}
	即$\mathbf{X}$满足\cref{def:MatNormal1}。\par
	\textbf{(2)必要性：}设$\mathbf{X}$满足\cref{def:MultiNormal1}，此时只要选择$A=\Sigma^\frac{1}{2}$即可得到$\mathbf{X}$满足\cref{def:MultiNormal2}。
\end{proof}

\subsection{多元正态分布的性质}
\begin{theorem}\label{theo:MultiNormalLinearTransform}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma)$，$\Sigma\geqslant0$，$B\in M_{m\times n}(\mathbb{R}),\;c\in\mathbb{R}^{n}$，则$\mathbf{Y}=B\mathbf{X}+c\sim N(B\boldsymbol{\mu}+c,B\Sigma B^T)$。
\end{theorem}
\begin{proof}
	因为$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma)$，由\cref{def:MultiNormal2}可得，存在$A\in M_{n\times r}(\mathbb{R}),\;\boldsymbol{\mu}\in\mathbb{R}^{n}$使得：
	\begin{equation*}
		\mathbf{X}=A\mathbf{U}+\boldsymbol{\mu},\;AA^T=\Sigma,\;U\sim N(\mathbf{0},I)
	\end{equation*}
	于是：
	\begin{equation*}
		\mathbf{Y}=B(A\mathbf{U}+\boldsymbol{\mu})+c=BA\mathbf{U}+B\boldsymbol{\mu}+c
	\end{equation*}
	注意到$BA(BA)^T=BAA^TB^T=B\Sigma B^T$，由\cref{def:MultiNormal2}可得$\mathbf{Y}\sim N(B\boldsymbol{\mu}+c,B\Sigma B^T)$。
\end{proof}
\begin{corollary}\label{cor:MultiNormalLinearTransform}
	由上述定理可以得到如下推论：
	\begin{enumerate}
		\item 设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma),\;\Sigma>0$，则$\mathbf{Y}=\Sigma^{-\frac{1}{2}}\mathbf{X}\sim N_n(\Sigma^{-\frac{1}{2}}\boldsymbol{\mu},I_n)$；
		\item 设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\sigma^2I_n)$，$Q$为正交矩阵，则$Q\mathbf{X}\sim N_n(Q\boldsymbol{\mu},\sigma^2I_n)$；
		\item 设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma),\;c\in \mathbb{R}^{n}$，则$c^T\mathbf{X}\sim N(c^T\boldsymbol{\mu},c^T\Sigma c)$；
		\item 设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma),\;\boldsymbol{\mu}=(\seq{\mu}{n})^T,\;\Sigma=(\sigma_{ij})$，则$\mathbf{X}_i\sim N(\mu_i,\sigma_{ii}),\;i=1,2,\dots,n$。
		\item 设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma),\;\boldsymbol{\mu}=(\seq{\mu}{n})^T,\;\Sigma=(\sigma_{ij})$，$i_1<i_2<\cdots<i_k$，则有$(\mathbf{X}_{i_1},\mathbf{X}_{i_2},\dots,\mathbf{X}_{i_k})^T\sim N(\boldsymbol{\mu}_0,\Sigma_0)$，其中：
		\begin{equation*}
			\boldsymbol{\mu}_0=
			\begin{pmatrix}
				\mu_{i_1} \\
				\mu_{i_2} \\
				\vdots \\
				\mu_{i_k}
			\end{pmatrix}
			,\quad
			\Sigma_0=
			\begin{pmatrix}
				\sigma_{i_1i_1} & \sigma_{i_1i_2} & \cdots & \sigma_{i_1i_k} \\
				\sigma_{i_2i_1} & \sigma_{i_2i_2} & \cdots & \sigma_{i_2i_k} \\
				\vdots & \vdots & \ddots & \vdots \\
				\sigma_{i_ki_1} & \sigma_{i_ki_2} & \cdots & \sigma_{i_ki_k}
			\end{pmatrix}
		\end{equation*}
	\end{enumerate}
\end{corollary}
\begin{proof}
	(1)由\cref{prop:ReverseSquareRootMat}(3)可知$\Sigma^{-\frac{1}{2}}$是对称阵，所以：
	\begin{equation*}
		\Sigma^{-\frac{1}{2}}\Sigma(\Sigma^{-\frac{1}{2}})^T=\Sigma^{-\frac{1}{2}}\Sigma^{\frac{1}{2}}\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}=I_n
	\end{equation*}\par
	(2)显然：
	\begin{equation*}
		Q\sigma^2IQ^T=\sigma^2QQ^T=\sigma^2I_n
	\end{equation*}\par
	(3)可直接得到；\par
	(4)对$\mathbf{X}_i$，取$c=(0,\dots,0,1,0,\dots,0)^T$，其中$c$的第$i$位为$1$其余全是$0$，于是：
	\begin{equation*}
		c^T\mathbf{X}=\mathbf{X}_i,\;c^T\boldsymbol{\mu}=\mu_i,\;c^T\Sigma c=\sigma_{ii}
	\end{equation*}
	所以$\mathbf{X}_i\sim N(\mu_i,\sigma_{ii}),\;i=1,2,\dots,n$。\par
	(5)取：
	\begin{equation*}
		A=
		\begin{pmatrix}
			e_{i_1}^T \\
			e_{i_2}^T \\
			\vdots \\
			e_{i_k}^T
		\end{pmatrix}
	\end{equation*}
	其中$e_{i_j}$为单位列向量，只在第$i_j$位取$1$，其余位置上元素为$0$，$j=1,2,\dots,k$。于是有：
	\begin{gather*}
		A\boldsymbol{\mu}=(\mu_{i_i},\mu_{i_2},\dots,\mu_{i_k})^T=\mu_0
		\\
		A\Sigma A^T=
		\begin{pmatrix}
			e_{i_1}^T\Sigma e_{i_1} & e_{i_1}^T\Sigma e_{i_2} & \cdots & e_{i_1}^T\Sigma e_{i_k} \\
			e_{i_2}^T\Sigma e_{i_1} & e_{i_2}^T\Sigma e_{i_2} & \cdots & e_{i_2}^T\Sigma e_{i_k} \\
			\vdots & \vdots & \ddots & \vdots \\
			e_{i_k}^T\Sigma e_{i_1} & e_{i_k}^T\Sigma e_{i_2} & \cdots & e_{i_k}^T\Sigma e_{i_k}^T
		\end{pmatrix}
		=
		\begin{pmatrix}
			\sigma_{i_1i_1} & \sigma_{i_1i_2} & \cdots & \sigma_{i_1i_k} \\
			\sigma_{i_2i_1} & \sigma_{i_2i_2} & \cdots & \sigma_{i_2i_k} \\
			\vdots & \vdots & \ddots & \vdots \\
			\sigma_{i_ki_1} & \sigma_{i_ki_2} & \cdots & \sigma_{i_ki_k}
		\end{pmatrix}
		=\Sigma_0
	\end{gather*}
	由上一定理可得$(\mathbf{X}_{i_1},\mathbf{X}_{i_2},\dots,\mathbf{X}_{i_k})^T\sim N(\boldsymbol{\mu}_0,\Sigma_0)$。
\end{proof}
\begin{theorem}\label{theo:c.f.MultiNormal}
	设$\mathbf{X}$是一个随机向量，则$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma)$当且仅当它的特征函数为：
	\begin{equation*}
		\varphi_\mathbf{X}(t)=\exp\left(it^T\boldsymbol{\mu}-\frac{t^T\Sigma t}{2}\right),\;t\in\mathbb{R}^{n}
	\end{equation*}
\end{theorem}
\begin{proof}
	若$X\sim N_n(\boldsymbol{\mu},\Sigma)$，则由\cref{def:MultiNormal2}可知，存在矩阵$A\in M_{n\times r}(\mathbb{R})$使得$\mathbf{X}=A\mathbf{U}+\boldsymbol{\mu}$，其中$\mathbf{U}=(\mathbf{U}_1,\mathbf{U}_2,\dots,\mathbf{U}_r)^T$，$\mathbf{U}_i\sim N(0,1)$且互相独立，$\boldsymbol{\mu}$为$n$维非随机实向量，$\Sigma=AA^T$。由\cref{prop:CharacteristicFunction}(5)可得：
	\begin{equation*}
		\varphi_\mathbf{U}(t)=\prod_{i=1}^n\varphi_{\mathbf{U}_i}(t_i)
		=\prod_{i=1}^ne^{-\frac{t_i^2}{2}}=e^{-\frac{t^Tt}{2}},\;t\in\mathbb{R}^{n}
	\end{equation*}
	于是：
	\begin{align*}
		\varphi_\mathbf{X}(t)
		&=\operatorname{E}(e^{it^T\mathbf{X}})
		=\operatorname{E}[e^{it^T(A\mathbf{U}+\boldsymbol{\mu})}]
		=e^{it\boldsymbol{\mu}}\operatorname{E}(e^{it^TA\mathbf{U}}) \\
		&=e^{it^T\boldsymbol{\mu}}\varphi_\mathbf{U}(A^Tt)
		=e^{it^T\boldsymbol{\mu}}e^{-\frac{t^TAA^Tt}{2}}
		=e^{it^T\boldsymbol{\mu}}e^{-\frac{t^T\Sigma t}{2}}
		=\exp\left(it^T\boldsymbol{\mu}-\frac{t^T\Sigma t}{2}\right)
	\end{align*}
	由\cref{prop:CharacteristicFunction}(6)，概率分布与特征函数之间是一一对应的关系，于是结论成立。
\end{proof}
\begin{theorem}\label{theo:MultiNormal3}
	设$\mathbf{X}$是一个$n$维随机向量，则$\mathbf{X}$服从$n$维多元正态分布的充分必要条件为对于任意的$\alpha\in\mathbb{R}^{n}$，$\alpha^T\mathbf{X}$服从正态分布。
\end{theorem}
\begin{proof}
	\textbf{(1)必要性：}由\cref{cor:MultiNormalLinearTransform}(3)直接得到。\par
	\textbf{(2)充分性：}由\cref{theo:c.f.MultiNormal}可知此时$\alpha^T\mathbf{X}$的特征函数为：
	\begin{equation*}
		\varphi_{\alpha^T\mathbf{X}}(t)=\exp\left(it\mu-\frac{1}{2}t^2\sigma^2\right)
	\end{equation*}
	其中$\mu$和$\sigma^2$分别为$\alpha^T\mathbf{X}$的均值与方差。由\info{期望的性质}和\cref{prop:CovMat}(3)可得：
	\begin{gather*}
		\mu=\operatorname{E}(\alpha^T\mathbf{X})=\alpha^T\operatorname{E}(\mathbf{X}),\;
		\sigma^2=\operatorname{Cov}(\alpha^T\mathbf{X})=\alpha^T\operatorname{Cov}(\mathbf{X})\alpha
	\end{gather*}
	于是有：
	\begin{equation*}
		\varphi_{\alpha^T\mathbf{X}}(t)=\exp\left(it\alpha^T\operatorname{E}(\mathbf{X})-\frac{t\alpha^T\operatorname{Cov}(\mathbf{X})\alpha t}{2}\right)
	\end{equation*}
	由$\alpha$的任意性，上式可写作：
	\begin{equation*}
		\varphi_{\mathbf{X}}(\beta)=\exp\left(i\beta^T\operatorname{E}(\mathbf{X})-\frac{\beta^T\operatorname{Cov}(\mathbf{X})\beta}{2}\right)
	\end{equation*}
	由\cref{prop:CharacteristicFunction}(6)和\cref{theo:c.f.MultiNormal}可知$\mathbf{X}$服从多元正态分布。
\end{proof}
\begin{lemma}\label{lem:MultiNormalConcatIndependent}
	设$\mathbf{X}$和$\mathbf{Y}$分别为$m$维正态随机向量和$n$维正态随机向量且相互独立，则：
	\begin{equation*}
		\mathbf{Z}=
		\begin{pmatrix}
			\mathbf{X} \\
			\mathbf{Y}
		\end{pmatrix}
		\sim N_{m+n}\left(
		\begin{pmatrix}
			\boldsymbol{\mu}_{\mathbf{X}} \\
			\boldsymbol{\mu}_{\mathbf{Y}}
		\end{pmatrix},\;
		\begin{pmatrix}
			\operatorname{Cov}(\mathbf{X}) & \mathbf{0} \\
			\mathbf{0} & \operatorname{Cov}(\mathbf{Y})
		\end{pmatrix}
		\right)
	\end{equation*}
\end{lemma}
\begin{proof}
	由\cref{def:MultiNormal1}即可得到。
\end{proof}
\begin{theorem}\label{theo:MultiNormalAdditivity}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma_\mathbf{X})$和$\mathbf{Y}\sim N_n(\boldsymbol{\nu},\Sigma_\mathbf{Y})$且相互独立，则$\mathbf{X}+\mathbf{Y}\sim N_n(\boldsymbol{\mu}+\boldsymbol{\nu},\Sigma_\mathbf{X}+\Sigma_\mathbf{Y})$。
\end{theorem}
\begin{proof}
	因为：
	\begin{equation*}
		\mathbf{X}+\mathbf{Y}=
		\begin{pmatrix}
			I_n & I_n
		\end{pmatrix}
		\begin{pmatrix}
			\mathbf{X} \\
			\mathbf{Y}
		\end{pmatrix}
	\end{equation*}
	由\cref{lem:MultiNormalConcatIndependent}和\cref{theo:MultiNormalLinearTransform}即可得到结论。
\end{proof}
\begin{theorem}\label{theo:IndependentCorrelationNormal}
	设$\mathbf{X}=(\seq{X}{n})^T\sim N_n(\boldsymbol{\mu},\Sigma)$，其中：
	\begin{gather*}
		\boldsymbol{\mu}=
		\begin{pmatrix} 
			\mu_1 \\ 
			\mu_2 \\ 
			\vdots \\ 
			\mu_n 
		\end{pmatrix},\quad
		\Sigma=
		\begin{pmatrix}
			\sigma_{11} & \sigma_{12} & \cdots & \sigma_{1n} \\
			\sigma_{21} & \sigma_{22} & \cdots & \sigma_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			\sigma_{n1} & \sigma_{n2} & \cdots & \sigma_{nn}
		\end{pmatrix}
	\end{gather*}
	则对任意的$m\leqslant n$，$\mathbf{X}=(\mathbf{X_1},\mathbf{X_2},\dots,\mathbf{X_m})^T$，$\mathbf{X_j}$是$r_j$维随机向量，$\sum\limits_{j=1}^{m}r_j=n$，若$\mathbf{X_j}$不相关，则$\mathbf{X_j}$相互独立，$j=1,2,\dots,m$。即对于服从正态分布的随机变量或随机向量而言，不相关和独立等价。
\end{theorem}
\begin{proof}
	由\cref{cor:MultiNormalLinearTransform}(5)可知$\mathbf{X_j}=(X_{j1},X_{j2},\dots,X_{jr_j})^T\sim N_{r_j}(\mu_j,\Sigma_j)$。由\cref{theo:c.f.MultiNormal}可知：
	\begin{equation*}
		\varphi_\mathbf{X_j}(t_j)=\exp\left(it_j^T\mu_j-\frac{t_j^T\Sigma_j t_j}{2}\right),\;t_j\in\mathbb{R}^{r_j}
	\end{equation*}
	将$\boldsymbol{\mu}$和$\Sigma$写成分块的形式：
	\begin{equation*}
		\boldsymbol{\mu}=
		\begin{pmatrix} 
			\mu_1 \\ 
			\mu_2 \\ 
			\vdots \\ 
			\mu_m 
		\end{pmatrix},\quad
		\Sigma=
		\begin{pmatrix}
			\Sigma_{11} & \Sigma_{12} & \cdots & \Sigma_{1m} \\
			\Sigma_{21} & \Sigma_{22} & \cdots & \Sigma_{2m} \\
			\vdots & \vdots & \ddots & \vdots \\
			\Sigma_{m1} & \Sigma_{m2} & \cdots & \Sigma_{mm}
		\end{pmatrix}
	\end{equation*}
	因为$\mathbf{X_j}$不相关，所以$\operatorname{Cov}(\mathbf{X_j},\mathbf{X_k})=\Sigma_{jk}=\mathbf{0},\;j\ne k,\;j,k=1,2,\dots,m$。于是：
	\begin{equation*}
		\Sigma=
		\begin{pmatrix}
			\Sigma_1 & \mathbf{0} & \cdots & \mathbf{0} \\
			\mathbf{0} & \Sigma_2 & \cdots & \mathbf{0} \\
			\vdots & \vdots & \ddots & \vdots \\
			\mathbf{0} & \mathbf{0} & \cdots & \Sigma_m
		\end{pmatrix}
	\end{equation*}
	由\cref{theo:c.f.MultiNormal}可得：
	\begin{align*}
		\varphi_\mathbf{X}(t)
		&=\exp\left(it\boldsymbol{\mu}-\frac{t^T\Sigma t}{2}\right)
		=\exp\left[i\sum_{j=1}^{m}t_j^T\mu_j-\frac{\sum\limits_{j=1}^{m}t_j^T\Sigma_j t_j}{2}\right] \\
		&=\prod_{j=1}^m\exp\left(it_j\mu_j-\frac{t_j^T\Sigma_j t_j}{2}\right)=\prod_{j=1}^m\varphi_\mathbf{X_j}(t_j)
	\end{align*}
	由\cref{prop:CharacteristicFunction}(5)可知$\mathbf{X_j}$相互独立，$j=1,2,\dots,m$。
\end{proof}
\subsubsection{正态随机向量的二次型}
\begin{theorem}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma),\;\Sigma>0$，$A$为$n$阶非随机实对称阵，则：
	\begin{equation*}
		\operatorname{E}(\mathbf{X}^TA\mathbf{X})=\boldsymbol{\mu}^TA\boldsymbol{\mu}+\operatorname{tr}(A\Sigma),\;
		\operatorname{Var}(\mathbf{X}^TA\mathbf{X})=2\operatorname{tr}[(A\Sigma)^2]+4\boldsymbol{\mu}^TA\Sigma A\boldsymbol{\mu}
	\end{equation*}
\end{theorem}
\begin{proof}
	期望可直接由\cref{theo:ERVQuadraticForm}得到。记$\mathbf{Y}=\Sigma^{-\frac{1}{2}}\mathbf{X}$，由\cref{cor:MultiNormalLinearTransform}(1)可知$\mathbf{Y}\sim N_n(\Sigma^{-\frac{1}{2}}\boldsymbol{\mu},I_n)$，根据\cref{theo:IndependentCorrelationNormal}，$\mathbf{Y}$的各分量相互独立。注意到$\mathbf{Y}$的各分量的三阶中心矩和四阶中心矩分别为$0$和$3$，由\cref{theo:VRVQuadraticForm}、\cref{prop:ReverseSquareRootMat}(3)和\cref{prop:Trace}(3)可得：
	\begin{align*}
		\operatorname{Var}(\mathbf{X}^TA\mathbf{X})
		&=\operatorname{Var}(\mathbf{Y}^T\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\mathbf{Y}) \\
		&=3\sum_{i=1}^{n}(\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})_{ii}^2+2\operatorname{tr}(\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})-3\sum_{i=1}^{n}(\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})_{ii}^2 \\
		&\quad+4\boldsymbol{\mu}^T\Sigma^{-\frac{1}{2}}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\boldsymbol{\mu} \\
		&=2\operatorname{tr}(\Sigma^{\frac{1}{2}}A\Sigma A\Sigma^{\frac{1}{2}})+4\boldsymbol{\mu}^TA\Sigma A\boldsymbol{\mu} \\
		&=2\operatorname{tr}(A\Sigma A\Sigma^{\frac{1}{2}}\Sigma^{\frac{1}{2}})+4\boldsymbol{\mu}^TA\Sigma A\boldsymbol{\mu} \\
		&=2\operatorname{tr}[(A\Sigma)^2]+4\boldsymbol{\mu}^TA\Sigma A\boldsymbol{\mu}\qedhere
	\end{align*}
\end{proof}
\begin{theorem}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma)$，$\Sigma>0$，则$(\mathbf{X}-\boldsymbol{\mu})^T\Sigma^{-1}(\mathbf{X}-\boldsymbol{\mu})\sim\chi_n^2$。
\end{theorem}
\begin{proof}
	因为$\Sigma>0$，所以存在$\Sigma^{-\frac{1}{2}}$。由\cref{cor:MultiNormalLinearTransform}(1)可得：
	\begin{equation*}
		\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu})\sim N_n(\mathbf{0},I_n)
	\end{equation*}
	于是根据\cref{prop:ReverseSquareRootMat}(1)和(3)可得：
	\begin{align*}
		(\mathbf{X}-\boldsymbol{\mu})^T\Sigma^{-1}(\mathbf{X}-\boldsymbol{\mu})&=(\mathbf{X}-\boldsymbol{\mu})^T(\Sigma^{-\frac{1}{2}}\Sigma^{-\frac{1}{2}})(\mathbf{X}-\boldsymbol{\mu}) \\
		&=(\mathbf{X}-\boldsymbol{\mu})^T(\Sigma^{-\frac{1}{2}})^T\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu}) \\
		&=[\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu})]^T\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu})\sim\chi_n^2\qedhere
	\end{align*}
\end{proof}
\begin{theorem}\label{theo:XAXChi2}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},I_n)$，$A$是一个非随机实对称阵，则$\mathbf{X}^TA\mathbf{X}\sim\chi_{r,\boldsymbol{\mu}^TA\boldsymbol{\mu}}^2$的充分必要条件为$A$是一个幂等阵且$\operatorname{rank}(A)=r$。
\end{theorem}
\begin{proof}
	\textbf{(1)充分性：}因为$A$是一个对称幂等阵，由\cref{prop:HermitianIdempotent}(1)可知$A$的特征值只能为$0$或$1$。根据\cref{prop:HermitianMatEigen}(3)可知存在正交矩阵$Q$使得：
	\begin{equation*}
		A=Q^{-1}
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}Q
	\end{equation*}
	令$\mathbf{Y}=Q\mathbf{X}$，由\cref{cor:MultiNormalLinearTransform}(2)可知$\mathbf{Y}\sim N_n(Q\boldsymbol{\mu},I_n)$。对$\mathbf{Y}$和$Q$进行分块：
	\begin{equation*}
		\mathbf{Y}=
		\begin{pmatrix}
			\mathbf{Y_1} \\
			\mathbf{Y_2}
		\end{pmatrix},\;
		Q=
		\begin{pmatrix}
			Q_1 \\
			Q_2
		\end{pmatrix}
	\end{equation*}
	其中$\mathbf{Y_1}$为$r$维随机向量，$Q_1$为$r\times n$矩阵，所以：
	\begin{align*}
		\mathbf{X}^TA\mathbf{X}&=\mathbf{X}^TQ^{-1}
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}Q\mathbf{X}=\mathbf{X}^TQ^T
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}Q\mathbf{X} \\
		&=\mathbf{Y}^T
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}\mathbf{Y}
		=
		\begin{pmatrix}
			\mathbf{Y_1}^T & \mathbf{Y_2}^T
		\end{pmatrix}
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		\begin{pmatrix}
			\mathbf{Y_1} \\
			\mathbf{Y_2}
		\end{pmatrix}
		=\mathbf{Y_1}^T\mathbf{Y_1}\sim\chi_{r,\lambda}^2
	\end{align*}
	其中：
	\begin{equation*}
		\lambda=(Q_1\boldsymbol{\mu})^TQ_1\boldsymbol{\mu}=\boldsymbol{\mu}^TQ_1^TQ_1\boldsymbol{\mu}=\boldsymbol{\mu}^TA\boldsymbol{\mu}
	\end{equation*}
	这是因为\cref{cor:MultiNormalLinearTransform}(5)和：
	\begin{equation*}
		A=Q^{-1}
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}Q
		=Q^T
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}Q
		=
		\begin{pmatrix}
			Q_1^T & Q_2^T
		\end{pmatrix}
			\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		\begin{pmatrix}
			Q_1 \\
			Q_2
		\end{pmatrix}
		=Q_1^TQ_1
	\end{equation*}\par
	\textbf{(2)必要性：}设$\operatorname{rank}(A)=t$。因为$A$是实对称阵，由\cref{prop:HermitianMatEigen}(3)可知存在正交阵$Q$使得：
	\begin{equation*}
		A=Q^{-1}
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		Q
	\end{equation*}
	其中$\varLambda=\operatorname{diag}\{\seq{\lambda}{t}\}$，$\seq{\lambda}{t}$是$A$的非零特征值。若能证得$\lambda_i=1,\;i=1,2,\dots,t$且$t=r$，则$A$是一个幂等阵且$\operatorname{rank}(A)=r$。注意到：
	\begin{equation*}
		\mathbf{X}^TA\mathbf{X}=\mathbf{X}^TQ^T
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		Q\mathbf{X}
	\end{equation*}
	令$\mathbf{Y}=Q\mathbf{X}$，由\cref{cor:MultiNormalLinearTransform}(2)可知$\mathbf{Y}=(\seq{\mathbf{Y}}{n})\sim N_n(Q\boldsymbol{\mu},I_n)$，根据\cref{theo:IndependentCorrelationNormal}可得$\mathbf{Y}_i$之间彼此独立。令$c=Q\boldsymbol{\mu}=(\seq{c}{n})^T$，由\cref{cor:MultiNormalLinearTransform}(5)可知$\mathbf{Y}_i\sim N(c_i,1)$。而：
	\begin{equation*}
		\mathbf{X}^TA\mathbf{X}=\mathbf{Y}^T
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}\mathbf{Y}=\sum_{i=1}^{t}\lambda_i\mathbf{Y}_i^2
	\end{equation*}
	由\cref{prop:CharacteristicFunction}(4)和\cref{prop:Chi2Distribution}(3)可知：
	\begin{align*}
		\varphi_{\mathbf{X}^TA\mathbf{X}}(t)
		&=\prod_{i=1}^{t}\varphi_{\lambda_i\mathbf{Y}_i^2}(t)=\prod_{i=1}^t(1-2it)^{-\frac{1}{2}}\exp\left\{\frac{i\lambda_itc_i^2}{1-2i\lambda_it}\right\} \\
		&=(1-2it)^{-\frac{t}{2}}\prod_{i=1}^t\exp\left\{\frac{i\lambda_itc_i^2}{1-2i\lambda_it}\right\}
	\end{align*}
	因为$\mathbf{X}^TA\mathbf{X}\sim\chi_{r,\boldsymbol{\mu}^TA\boldsymbol{\mu}}^2$，所以：
	\begin{equation*}
		\varphi_{\mathbf{X}^TA\mathbf{X}}(t)=(1-2it)^{-\frac{r}{2}}\exp\left\{\frac{it\boldsymbol{\mu}^TA\boldsymbol{\mu}}{1-2it}\right\}
	\end{equation*}
	由\cref{prop:CharacteristicFunction}(6)可知：
	\begin{equation*}
		(1-2it)^{-\frac{t}{2}}\prod_{i=1}^t\exp\left\{\frac{i\lambda_itc_i^2}{1-2i\lambda_it}\right\}=(1-2it)^{-\frac{r}{2}}\exp\left\{\frac{it\boldsymbol{\mu}^TA\boldsymbol{\mu}}{1-2it}\right\}
	\end{equation*}
	所以$t=r$，同时有：
	\begin{equation*}
		\sum_{i=1}^{t}\frac{i\lambda_itc_i^2}{1-2i\lambda_it}=\frac{it\boldsymbol{\mu}^TA\boldsymbol{\mu}}{1-2it}
	\end{equation*}
	即：
	\begin{gather*}
		\sum_{i=1}^{t}\frac{i\lambda_itc_i^2}{1-2i\lambda_it}=\frac{1}{1-2it}it\boldsymbol{\mu}^TQ^T
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		Q\boldsymbol{\mu} \\
		\sum_{i=1}^{t}\frac{\lambda_ic_i^2}{1-2i\lambda_it}=\frac{1}{1-2it}c^T
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		c \\
		\sum_{i=1}^{t}\frac{\lambda_ic_i^2}{1-2i\lambda_it}=\frac{1}{1-2it}\sum_{i=1}^{t}\lambda_ic_i^2 \\
		\frac{\lambda_ic_i^2}{1-2i\lambda_it}=\frac{\lambda_ic_i^2}{1-2it},\;i=1,2,\dots,t
	\end{gather*}
	所以$\lambda_i=1$。
\end{proof}
\begin{corollary}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma),\;\Sigma>0$，$A$是一个非随机实对称阵，则$\mathbf{X}^TA\mathbf{X}\sim\chi_{r,\boldsymbol{\mu}^TA\boldsymbol{\mu}}^2$的充分必要条件为$A\Sigma A=A,\;\operatorname{rank}(A)=r$。
\end{corollary}
\begin{proof}
	因为$\Sigma>0$，所以存在$\Sigma^{-\frac{1}{2}}$。考虑随机向量$\mathbf{Y}=\Sigma^{-\frac{1}{2}}\mathbf{X}$，由\cref{theo:MultiNormalLinearTransform}可知$\mathbf{Y}\sim N_n(\Sigma^{-\frac{1}{2}}\boldsymbol{\mu},I_n)$。注意到：
	\begin{equation*}
		\mathbf{X}^TA\mathbf{X}=\mathbf{X}^T\Sigma^{-\frac{1}{2}}\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\mathbf{X}
	\end{equation*}
	由\cref{prop:ReverseSquareRootMat}(3)可得：
	\begin{equation*}
		\mathbf{X}^TA\mathbf{X}=\mathbf{X}^T(\Sigma^{-\frac{1}{2}})^T\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\mathbf{X}=\mathbf{Y}\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\mathbf{Y}
	\end{equation*}
	由\cref{theo:XAXChi2}可得$\mathbf{Y}\Sigma
	^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\mathbf{Y}\sim\chi_{r,\boldsymbol{\mu}^TA\boldsymbol{\mu}}^2$的充分必要条件为$\Sigma
	^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}$是一个对称阵且：
	\begin{gather*}
		(\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})^2=\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}},\;
		\operatorname{rank}(\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})=r,\;
		(\Sigma^{-\frac{1}{2}}\boldsymbol{\mu})^T\Sigma^{\frac{1}{2}} A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\boldsymbol{\mu}=\boldsymbol{\mu}^TA\boldsymbol{\mu}
	\end{gather*}
	第三式显然成立。因为$\Sigma>0$，所以$\operatorname{rank}(\Sigma
	^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})=\operatorname{rank}(A)$。注意到：
	\begin{align*}
		(\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})^2=\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}&\Leftrightarrow
		\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}=\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}} \\
		&\Leftrightarrow
		\Sigma^{\frac{1}{2}}A\Sigma A\Sigma^{\frac{1}{2}}=\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Leftrightarrow
		A\Sigma A=A\qedhere
	\end{align*}
\end{proof}
\begin{theorem}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},I_n)$，$A\in M_{n}(K)$是一个Hermitian矩阵，$B\in M_{m\times n}(K)$。若$BA=\mathbf{0}$，则$B\mathbf{X}$与$\mathbf{X}^TA\mathbf{X}$相互独立。
\end{theorem}
\begin{proof}
	因为$A$是一个Hermitian矩阵，由\cref{prop:HermitianMatEigen}(3)可知存在正交矩阵$Q$使得：
	\begin{equation*}
		Q^HAQ=
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
	\end{equation*}
	其中$\varLambda=\operatorname{diag}(\seq{\lambda}{r}),\;\lambda_i\ne0,\;i=1,2,\dots,r,\;\operatorname{rank}(A)=r$。因为$BA=\mathbf{0}$，所以有$BQQ^HAQ=BAQ=\mathbf{0}$，于是：
	\begin{equation*}
		BQ
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		=\mathbf{0}
	\end{equation*}
	设：
	\begin{equation*}
		C=BQ=
		\begin{pmatrix}
			C_{11} & C_{12} \\
			C_{21} & C_{22}
		\end{pmatrix}
	\end{equation*}
	则：
	\begin{equation*}
		BQ
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		=
		\begin{pmatrix}
			C_{11}\varLambda & \mathbf{0} \\
			C_{21}\varLambda & \mathbf{0}
		\end{pmatrix}
		=\begin{pmatrix}
			\mathbf{0} & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
	\end{equation*}
	于是有$C_{11}=\mathbf{0},\;C_{21}=\mathbf{0}$。对$C$和$Q$做对应分块：
	\begin{equation*}
		C=BQ=
		\begin{pmatrix}
			\mathbf{0} & C_1
		\end{pmatrix},\;
		Q=
		\begin{pmatrix}
			Q_1 & Q_2
		\end{pmatrix}
	\end{equation*}
	于是：
	\begin{equation*}
		B=CQ^H=
		\begin{pmatrix}
			\mathbf{0} & C_1
		\end{pmatrix}
		\begin{pmatrix}
			Q_1^H \\
			Q_2^H
		\end{pmatrix}
		=C_1Q_2^H
	\end{equation*}
	而：
	\begin{equation*}
		A=Q
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		Q^H
		=
		\begin{pmatrix}
			Q_1 & Q_2
		\end{pmatrix}
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		\begin{pmatrix}
			Q_1^H \\
			Q_2^H
		\end{pmatrix}
		=Q_1\varLambda Q_1^H
	\end{equation*}
	记$\mathbf{Y}=Q^H\mathbf{X}$，由\cref{cor:MultiNormalLinearTransform}(2)可得：
	\begin{equation*}
		\mathbf{Y}=
		\begin{pmatrix}
			\mathbf{Y_1} \\
			\mathbf{Y_2}
		\end{pmatrix}
		=
		\begin{pmatrix}
			Q_1^H\mathbf{X} \\
			Q_2^H\mathbf{X}
		\end{pmatrix}
		\sim N_n(Q^H\boldsymbol{\mu},\sigma^2I_n)
	\end{equation*}
	由\cref{theo:IndependentCorrelationNormal}可知$\mathbf{Y_1}$与$\mathbf{Y_2}$独立。因为：
	\begin{gather*}
		B\mathbf{X}=C_1Q_2^H\mathbf{X}=C_1\mathbf{Y_2} \\
		\mathbf{X}^TA\mathbf{X}=\mathbf{X}^TQ_1\varLambda Q_1^H\mathbf{X}=\mathbf{Y_1}^T\varLambda\mathbf{Y_1}
	\end{gather*}
	所以$B\mathbf{X}$与$\mathbf{X}^TA\mathbf{X}$独立。
\end{proof}
\begin{corollary}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma),\;\Sigma>0$，$A$为$n$阶对称阵。若$C\Sigma A=\mathbf{0}$，则$C\mathbf{X}$与$\mathbf{X}^TA\mathbf{X}$独立。
\end{corollary}
\begin{proof}
	
\end{proof}
\begin{theorem}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},I_n)$，$A,B$为$n$阶对称阵。若$AB=\mathbf{0}$，则$\mathbf{X}^TA\mathbf{X}$与$\mathbf{X}^TB\mathbf{X}$独立。
\end{theorem}
\begin{proof}
	因为$AB=\mathbf{0}$且$A,B$都是对称阵，所以$BA=B^TA^T$
\end{proof}

\subsection{矩阵正态分布的定义}
\subsubsection{密度函数定义}
\begin{definition}\label{def:MatNormal1}
	若$m\times n$随机矩阵$\mathbf{X}$满足以下概率密度函数：
	\begin{equation*}
		p(\mathbf{X})=\frac{1}{(2\pi)^{\frac{mn}{2}}(\det U)^{\frac{n}{2}}(\det V)^{\frac{m}{2}}}e^{-\frac{1}{2}\operatorname{tr}[V^{-1}(X-M)^TU^{-1}(X-M)]}
	\end{equation*}
	其中，$M\in M_{m\times n}(\mathbb{R}),\;U\in M_{m}(\mathbb{R}),\;V\in M_{n}(\mathbb{R})$，$U,V>0$。此时称$\mathbf{X}$服从矩阵正态分布，记作$\mathbf{X}\sim MN(M,U,V)$。
\end{definition}
\subsubsection{向量化定义}
\begin{definition}\label{def:MatNormal2}
	若随机矩阵$\mathbf{X}$满足$\operatorname{vec}(\mathbf{X})\sim N(\operatorname{vec}(M),V\otimes U)$，其中，$M\in M_{m\times n}(\mathbb{R}),\;U\in M_{m}(\mathbb{R}),\;V\in M_{n}(\mathbb{R})$，$U,V\geqslant0$。此时称$\mathbf{X}$服从矩阵正态分布，记作$\mathbf{X}\sim MN(M,U,V)$。
\end{definition}
\begin{theorem}
	设$\mathbf{X}$是一个$m\times n$随机矩阵，其行协方差矩阵$U$和列协方差矩阵$V$都是正定矩阵，则$\mathbf{X}$满足\cref{def:MatNormal1}的充分必要条件为满足\cref{def:MatNormal2}。
\end{theorem}
\begin{proof}
	由\cref{prop:Trace}(3)、\cref{prop:VecOperator}(1)(2)、\cref{prop:Kronecker}(2)(3)可得：
	\begin{align*}
		\operatorname{tr}[V^{-1}(\mathbf{X}-M)^TU^{-1}(\mathbf{X}-M)]
		&=\operatorname{tr}[(\mathbf{X}-M)^TU^{-1}(\mathbf{X}-M)V^{-1}] \\
		&=\operatorname{vec}(\mathbf{X}-M)^T\operatorname{vec}[U^{-1}(\mathbf{X}-M)V^{-1}] \\
		&=\operatorname{vec}(\mathbf{X}-M)^T[(V^{-1})^T\otimes U^{-1}]\operatorname{vec}(\mathbf{X}-M) \\
		&=\operatorname{vec}(\mathbf{X}-M)^T[(V^T)^{-1}\otimes U^{-1}]\operatorname{vec}(\mathbf{X}-M) \\
		&=\operatorname{vec}(\mathbf{X}-M)^T(V^{-1}\otimes U^{-1})\operatorname{vec}(\mathbf{X}-M) \\
		&=[\operatorname{vec}(\mathbf{X})-\operatorname{vec}(M)]^T(V\otimes U)^{-1}[\operatorname{vec}(\mathbf{X})-\operatorname{vec}(M)]
	\end{align*}
	因为$\det(V\otimes U)=(\det V)^m(\det U)^n$，所以$(\det U)^{\frac{n}{2}}(\det V)^{\frac{m}{2}}$可化作$[\det(V\otimes U)]^{\frac{1}{2}}$。\info{需要补充证明，但这里涉及到了Jordan标准形，学完再来补。}
\end{proof}
\begin{corollary}
	如果正态随机矩阵$\mathbf{X}\sim MN(M,U,V)$中的每个元素都服从标准正态分布，则$M=\mathbf{0},\;V\otimes U=I_{mn}$。
\end{corollary}
由此我们看到，$M$就是正态随机矩阵$X$的均值矩阵，仍然不明确的是$U,V$到底是什么，只能说$V\otimes U$对应着$X$被向量化后的协方差矩阵，那就先来研究一下$\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{kl})$到底对应着$V\otimes U$中的哪个元素。联想正态随机向量中两个元素的协方差在协方差矩阵中的位置，我们需要找到$\mathbf{X}_{ij}$和$\mathbf{X}_{kl}$在$\operatorname{vec}(\mathbf{X})$中的索引，注意到向量化算子$\operatorname{vec}$是按列拉直，那么$\mathbf{X}_{ij}$和$\mathbf{X}_{kl}$分别在$\operatorname{vec}(\mathbf{X})$的第$(j-1)m+i$位和第$(l-1)m+k$位，于是有：
\begin{equation*}
	\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{kl})=(V\otimes U)_{(j-1)s+i,(l-1)s+k}=V_{jl}U_{ik}
\end{equation*}\par
如果$U$是一个对角阵，那么$i\ne k$时有$U_{ik}=0$，就会导致：
\begin{equation*}
	\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{kl})=V_{jl}U_{ik}=0,\;i\ne k
\end{equation*}
这表明此时只要$X$中的元素处于不同行，它们就不相关。\par
如果$V$是一个对角阵，那么$j\ne l$时有$V_{jl}=0$，就会导致：
\begin{equation*}
	\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{kl})=V_{jl}U_{ik}=0,\;j\ne l
\end{equation*}
这表明此时只要$X$中的元素处于不同列，它们就不相关。\par
对于元素$\mathbf{X}_{ij}$，有：
\begin{equation*}
	\operatorname{Var}(\mathbf{X}_{ij})=\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{ij})=V_{jj}U_{ii}
\end{equation*}
这表明此时协方差由$V_{jj}U_{ii}$控制。\par
对于同一行的元素，有：
\begin{equation*}
	\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{il})=V_{jl}U_{ii}
\end{equation*}
这表明此时协方差由$V_{jl}$控制。\par
对于同一列的元素，有：
\begin{equation*}
	\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{kj})=V_{jj}U_{ik}
\end{equation*}
这表明此时协方差由$U_{ik}$控制。
\subsubsection{线性变换定义}
\begin{definition}\label{def:MatNormal3}
	$\mathbf{X}$为$m\times n$随机矩阵。若存在矩阵$A\in M_{q\times n}(\mathbb{R}),\;B\in M_{m\times p}(K)$使得$\mathbf{X}=B\mathbf{Y}A^T+M$，其中$\mathbf{Y}$是一个$p\times q$随机矩阵，$\mathbf{Y}_{ij}\sim N(0,1)$且互相独立，$i=1,2,\dots,p,\;j=1,2,\dots,q$，$M\in M_{p\times q}(\mathbb{R})$，则称$\mathbf{X}$服从矩阵正态分布，记作$X\sim MN(M,U,V)$。其中，$U=BB^T,\;V=AA^T$。
\end{definition}
\begin{theorem}
	$\mathbf{X}$是一个$m\times n$随机矩阵，则$\mathbf{X}$满足\cref{def:MatNormal2}的充分必要条件是满足\cref{def:MatNormal3}。
\end{theorem}
\begin{proof}
	\textbf{(1)必要性：}设$\mathbf{X}$满足\cref{def:MatNormal3}，由$\mathbf{Y}$的定义可知：
	\begin{equation*}
		\operatorname{vec}(\mathbf{Y})\sim N_{pq}(\mathbf{0},I_{pq})
	\end{equation*}
	由\cref{prop:VecOperator}(1)(3)可得：
	\begin{equation*}
		\operatorname{vec}(\mathbf{X})=\operatorname{vec}(B\mathbf{Y}A^T+M)=\operatorname{vec}(B\mathbf{Y}A^T)+\operatorname{vec}(M)=(A\otimes B)\operatorname{vec}(\mathbf{Y})+\operatorname{M}
	\end{equation*}
	由\cref{prop:CovMat}(3)和\cref{prop:Kronecker}(4)(2)可得：
	\begin{gather*}
		\operatorname{E}[\operatorname{vec}(\mathbf{X})] 
		= \operatorname{E}[(A \otimes B)\operatorname{vec}(\mathbf{Y}) + \operatorname{M}] 
		= (A \otimes B)\operatorname{E}[\operatorname{vec}(\mathbf{Y})] + \operatorname{M} 
		= \operatorname{M}, \\[1ex]
		\begin{aligned}
			\operatorname{Cov}[\operatorname{vec}(\mathbf{X})] 
			&= \operatorname{Cov}[(A \otimes B)\operatorname{vec}(\mathbf{Y})] \\
			&= (A \otimes B)\operatorname{Cov}[\operatorname{vec}(\mathbf{Y})](A \otimes B)^T \\
			&= (A \otimes B)I_{pq}(A^T \otimes B^T) \\
			&= AA^T \otimes BB^T.
		\end{aligned}
	\end{gather*}
	因为$\operatorname{vec}(\mathbf{X})=(A\otimes B)\operatorname{vec}(\mathbf{Y})+\operatorname{M}$，而$\operatorname{vec}(\mathbf{Y})\sim N_{pq}(\mathbf{0},I_{pq})$，由\cref{theo:MultiNormalLinearTransform}可知：
	\begin{equation*}
		\operatorname{vec}(\mathbf{X})\sim N(\operatorname{vec}(M),AA^T\otimes BB^T)
	\end{equation*}
	令$V=AA^T,\;U=BB^T$，则有$\operatorname{vec}(\mathbf{X})\sim N(\operatorname{vec}(M),V\otimes U)$，即$\mathbf{X}$满足\cref{def:MatNormal2}。\par
	\textbf{(2)充分性：}设$\mathbf{X}$满足\cref{def:MatNormal2}，因为$U,V\geqslant0$，所以存在$U^{\frac{1}{2}}$和$V^{\frac{1}{2}}$，令$B=U^{\frac{1}{2}},A=V^{\frac{1}{2}}$，于是$\operatorname{vec}(\mathbf{X})\sim N(\operatorname{vec}(M),V\otimes U)$可写作$\operatorname{vec}(\mathbf{X})\sim N(\operatorname{vec}(M),AA^T\otimes BB^T)$。设$\mathbf{Y}$是一个随机矩阵，其中的每一个元素都服从标准正态分布且互相独立，则$\operatorname{vec}(\mathbf{X})=(A\otimes B)\operatorname{vec}(\mathbf{Y})+\operatorname{M}$。由\cref{prop:VecOperator}(1)(3)可知：
	\begin{gather*}
		\operatorname{vec}(\mathbf{X})=\operatorname{vec}(B\mathbf{Y}A^T+M)
	\end{gather*}
	于是$\mathbf{X}=B\mathbf{Y}A^T+M$，即$\mathbf{X}$满足\cref{def:MatNormal3}。
\end{proof}
\subsection{矩阵正态分布的性质}
\begin{theorem}\label{theo:MatNormalLinearTransform}
	设$\mathbf{X}$为$m\times n$随机矩阵且服从矩阵正态分布$MN(M,U,V)$，$P\in M_{s\times m}(R),\;Q\in M_{n\times t}(R)$，则$P\mathbf{X}Q^T\sim$
\end{theorem}
\begin{proof}
	由\cref{def:MatNormal3}可知$\mathbf{X}=B\mathbf{Y}A^T+M$，于是：
	\begin{equation*}
		P\mathbf{X}Q^T=PB\mathbf{Y}A^TQ^T+PMQ^T
	\end{equation*}
	此时$PBB^TP^T=PUP^T,\;QAA^TQ^T=QVQ^T$，由\cref{def:MatNormal3}即可得到结论。
\end{proof}