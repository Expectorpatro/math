\section{树模型}

\subsection{信息度量与分裂准则}
\subsubsection{信息量}
\begin{definition}
	设$(X,\mathscr{A},P)$为概率空间，$f$是$X$上的离散型随机变量，$f$的取值集合为$\{x_n\}$。称$-\log[P(f=x_n)]$为事件$\{f=x_n\}$的\gls{InformationQuantity}，记作$I(f=x_n)$。
\end{definition}
\begin{note}
	信息量定义中对数底的选择是任意的，但在信息论中普遍使用$2$作为对数的底。\par
	为什么信息量这么定义呢？信息量应该具有如下两个性质：
	\begin{enumerate}
		\item 一件事发生的概率越小，那么这件事发生后产生的信息量越大；
		\item 如果两件事情独立，那么这两件事情都发生所产生的信息量应该等于每件事情各自发生产生的信息量之和。
	\end{enumerate}
	由(1)，$I(f=x_n)$需要与$P(f=x_n)$呈反比；由(2)，$I(f=x_n)$应具有对数的形式。
\end{note}
\begin{definition}
	设$(X,\mathscr{A},P)$为概率空间，$f$是$X$上的离散型随机变量，$f$的取值集合为$\{x_n\}$。称$H(f)=\operatorname{E}[I(f)]$为$f$的\gls{InformationEntropy}，由样本计算得到的信息熵被称为\gls{EmpiricalEntropy}，给定某一条件下的信息熵被称为\gls{ConditionalEntropy}。
\end{definition}
%\begin{note}
%	设$X,Y$是两个离散型随机变量，各自有$n$个和$m$个取值，分别为$x_1,x_2,\dots,x_n$和$y_1,y_2,\dots,y_m$，则$X$在$Y$下的条件熵$H(X|Y)$为：
%	\begin{align*}
	%		H(X|Y)
	%		&=E[I(X|Y)]=E\left[\sum_{i=1}^mP(Y=y_i)I(X|Y=y_i)\right] \\
	%		&=\sum_{i=1}^mP(Y=y_i)E\left[I(X|Y=y_i)\right] \\
	%		&=\sum_{i=1}^mP(Y=y_i)E\left[-\log_2\Big(P(X|Y=y_i)\Big)\right] \\
	%		&=-\sum_{i=1}^mP(Y=y_i)\sum_{j=1}^nP(X=x_j|Y=y_i)\log_2\Big[P(X=x_j|Y=y_i)\Big]
	%	\end{align*}
%	$X$在$Y=y_i$下的条件熵$H(X|Y=y_i)$为：
%	\begin{align*}
	%		H(X|Y=y_i)&=E[I(X|Y=y_i)] \\
	%		&=E\left[-\log_2\Big(P(X|Y=y_i)\Big)\right] \\
	%		&=-\sum_{j=1}^nP(X=x_j|Y=y_i)\log_2\Big[P(X=x_j|Y=y_i)\Big]
	%	\end{align*}
%\end{note}
\begin{property}\label{prop:InformationEntropy}
	设$(X,\mathscr{A},P)$为概率空间，$f$是$X$上的离散型随机变量，$f$的取值集合为$\{x_n\}$。信息熵具有如下性质：
	\begin{enumerate}
		\item $H(f)\geqslant0$；
		\item 若$f$的取值集合为$\{x_i\}_{i=1}^{n}$，则当$P(f=x_1)=P(f=x_2)=\cdots=P(f=x_n)$时，$H(f)$取得最大值$\log(n)$。
	\end{enumerate}
\end{property}
\begin{proof}
	(1)由信息熵的定义立即可得。\par
	(2)由于$\log(\cdot)$在$(0,+\infty)$上为凹函数\info{补充判断理由}，根据\cref{ineq:Jensen}可得：
	\begin{align*}
		H(f)&=\operatorname{E}[I(f)]=-\sum_{i=1}^{n}P(f=x_i)\log[P(f=x_i)]=\sum_{i=1}^{n}P(f=x_i)\log\left[\frac{1}{P(f=x_i)}\right] \\
		&\leqslant\log\left[\sum_{i=1}^{n}P(f=x_i)\frac{1}{P(f=x_i)}\right]=\log(n)
	\end{align*}
	上式取等当且仅当$\dfrac{1}{P(f=x_i)}$为常数，即$P(f=x_1)=P(f=x_2)=\cdots=P(f=x_n)$。
\end{proof}
\subsubsection{Gini系数}
\begin{definition}
	设分类问题的类别空间为$\mathcal{Y}=\{1,\dots,m\}$，$\mathcal{D}=\{(x_i,y_i)\}_{i=1}^n$为观测样本集，其中$y_i\in\mathcal{Y}$表示第$i$个样本的真实类别，记$p_i$为第$i$类样本在$\mathcal{D}$中所占比例。称：
	\begin{equation*}
		\operatorname{Gini}(\mathcal{D})=1-\sum_{i=1}^mp_i^2
	\end{equation*}
	为观测样本集$\mathcal{D}$的\gls{GiniIndex}。
\end{definition}
\begin{property}\label{prop:GiniIndex}
	设分类问题的类别空间为$\mathcal{Y}=\{1,\dots,m\}$，$\mathcal{D}=\{(x_i,y_i)\}_{i=1}^n$为观测样本集，其中$y_i\in\mathcal{Y}$表示第$i$个样本的真实类别，记$p_i$为第$i$类样本在$\mathcal{D}$中所占比例。Gini指数具有如下性质：
	\begin{enumerate}
		\item Gini指数是在样本集合中随机抽取两个样本时，它们属于不同类别的概率；
		\item $\operatorname{Gini}(\mathcal{D})\leqslant1-\dfrac{1}{m}$，上式取等当且仅当$p_1=\cdots=p_m=\dfrac{1}{m}$。
	\end{enumerate}
\end{property}
\begin{proof}
	(1)显然。\par
	(2)由\cref{ineq:cauchy-ineq-R}可得：
	\begin{equation*}
			\sum_{i=1}^{m}1^2\sum_{i=1}^{m}p_i^2\geqslant\left(\sum_{i=1}^{m}p_i\right)^2=1
	\end{equation*}
	上式取等当且仅当$p_1=\cdots=p_m=\frac{1}{m}$，因此：
	\begin{equation*}
		\operatorname{Gini}(\mathcal{D})=1-\sum_{i=1}^{m}p_i^2\leqslant1-\frac{1}{m}\qedhere
	\end{equation*}
\end{proof}
\begin{note}
	由\cref{prop:GiniIndex}(1)(2)可以看出Gini指数可以用来刻画样本类别分布的不纯度。当所有样本属于同一类别时$\operatorname{Gini}(\mathcal{D})=0$，表示完全纯净；当类别分布均匀时$\operatorname{Gini}(\mathcal{D})$达到最大值，表示类别混杂程度最高。
\end{note}
\subsubsection{交叉熵}
\begin{definition}
	设 $(X,\mathscr{A},P)$ 为概率空间，$f$为$X$上取值于有限集合
	$\{x_1,\dots,x_n\}$ 的离散型随机变量，其真实分布为：
	\begin{equation*}
		p_i=P(f=x_i),\quad i=1,\dots,n
	\end{equation*}
	另设$q=(q_1,\dots,q_n)$为定义在同一取值集合上的另一概率分布，满足$q_i>0$。称：
	\begin{equation*}
		H(p,q)=-\sum_{i=1}^np_i\log q_i
	\end{equation*}
	为分布$p$相对于分布$q$的\gls{CrossEntropy}。
\end{definition}
\begin{property}\label{prop:CrossEntropy}
	设$p=(p_1,\dots,p_n)$与$q=(q_1,\dots,q_n)$为定义在同一有限集合上的概率分布，则$H(p,q)\geqslant H(p,p)$，上式取等当且仅当$q_1=p_1,\dots,q_n=p_n$ 
\end{property}
\begin{proof}
	因为：
	\begin{equation*}
		H(p,q)=-\sum_{i=1}^np_i\log q_i=-\sum_{i=1}^np_i\log\frac{q_i}{p_i}+\sum_{i=1}^np_i\log p_i
	\end{equation*}
	并且$\log(\cdot)$在$(0,+\infty)$上为凹函数\info{补充判断理由}，由\cref{ineq:Jensen}可知：
	\begin{equation*}
		\sum_{i=1}^np_i\log\frac{q_i}{p_i}\leqslant\log\left[\sum_{i=1}^{n}p_i\frac{q_i}{p_i}\right]=0
	\end{equation*}
	上式取等当且仅当$\frac{q_i}{p_i}$为常数，即$q_i=p_i$。所以：
	\begin{equation*}
		H(p,q)\geqslant\sum_{i=1}^np_i\log p_i=H(p,p)
	\end{equation*}
	且当且仅当$q=p$时取等。
\end{proof}
\begin{note}
	由\cref{prop:CrossEntropy}可知，交叉熵在所有概率分布$q$中以$q=p$为唯一最小值。因而最小化经验交叉熵等价于逼近真实分布$p$，这正是对数损失函数的理论基础。
\end{note}

\subsection{决策树}
\begin{algorithm}[H]
	\caption{Classification and Regression Tree}
	\begin{algorithmic}[1]
		\State \textbf{Input:}
		dataset $\mathcal{D}$,
		feature set $\mathcal{F}$,
		maximum depth $D_{\max}$,
		minimum samples to split $N_{\text{split}}$,
		minimum samples per leaf $N_{\text{leaf}}$,
		minimum impurity decrease $\Delta_{\min}$,
		criterion
		
		\State \textbf{Output:} Decision tree node $T$
		
		\Function{BuildTree}{$\mathcal{D}, d$}
		\If{$|\mathcal{D}| < N_{\text{split}}$ \textbf{or} $d = D_{\max}$ 
			\textbf{or} all labels in $\mathcal{D}$ are identical 
			\textbf{or} no feature in $\mathcal{F}$ exhibits variation on $\mathcal{D}$}
		\State \Return a leaf node predicting the majority class with class distribution:
		\begin{equation*}
			\left\{p_i=\frac{n_i}{|\mathcal{D}|}:n_i=|\{(x,y)\in\mathcal{D}:y=i\}|,\;i=1,2,\dots,m,\;m=\text{number of classes}\right\}
		\end{equation*} 
		or the constant minimizing the chosen criterion (for regression) on $\mathcal{D}$
		\EndIf
		
		\State $I_{\text{parent}}\gets\Call{NodeImpurity}{\mathcal{D}, \text{criterion}}$\Comment{Compute parent node impurity}

		\State $(F, \tau, I_{\text{split}}) \gets \Call{FindBestSplit}{\mathcal{D}, \mathcal{F}, \text{criterion}, N_{\text{leaf}}}$\Comment{Search best binary split}
		\If{$(F, \tau, I_{\text{split}})=\texttt{None}$ \text{or} $\Delta=I_{\text{parent}}-I_{\text{split}}<\Delta_{\min}$}
		\State \Return Leaf node with prediction and class distribution or constant
		\EndIf
	
		\State $\mathcal{D}_L \gets \{(x,y)\in\mathcal{D}:x[F]\leqslant\tau\}$, $\mathcal{D}_R \gets \{(x,y)\in\mathcal{D}:x[F]>\tau\}$

		\State Initialize node 
		$T \gets \{\text{type: "node"}, \text{feature: } F, \text{threshold: } \tau, 
		\text{impurity decrease: }\Delta\}$
		\State $T.\text{left} \gets \Call{BuildTree}{\mathcal{D}_L, d+1}$, $T.\text{right} \gets \Call{BuildTree}{\mathcal{D}_R, d+1}$
		\State \Return $T$
		\EndFunction
		
		\State \Return \Call{BuildTree}{$\mathcal{D}, 0$}
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Node Impurity Function}
	\begin{algorithmic}[1]
		\State \textbf{Input:} node dataset $\mathcal{D}$, criterion
		\State \textbf{Output:} node impurity $I(\mathcal{D})$
		
		\State $n \gets |\mathcal{D}|$
		
		\If{criterion $\in \{\texttt{gini}, \texttt{entropy}, \texttt{cross entropy}\}$}\Comment{Classification impurity}
		\State Let class label set be $\{1,\dots,m\}$
		\State $n_i \gets |\{(x,y)\in\mathcal{D}:y=i\}|$, \quad $p_i \gets \dfrac{n_i}{n}$
		
		\If{criterion = \texttt{gini}}
		\State $I(\mathcal{D}) \gets 1 - \sum\limits_{i=1}^m p_i^2$
		\ElsIf{criterion = \texttt{entropy} \textbf{or} \texttt{cross entropy}}
		\State $I(\mathcal{D}) \gets - \sum\limits_{i=1}^m p_i \log p_i$
		\EndIf
		
		\ElsIf{criterion $\in \{\texttt{squared error}, \texttt{absolute error}\}$}\Comment{Regression impurity}
		
		\If{criterion = \texttt{squared error}}
		\State $\bar{y} \gets \dfrac{1}{n} \sum\limits_{(x,y)\in\mathcal{D}} y$, $I(\mathcal{D}) \gets \dfrac{1}{n} \sum\limits_{(x,y)\in\mathcal{D}} (y-\bar{y})^2$
		
		\ElsIf{criterion = \texttt{absolute error}}
		\State $m \gets \operatorname{median}\{y:(x,y)\in\mathcal{D}\}$, $I(\mathcal{D}) \gets \dfrac{1}{n} \sum\limits_{(x,y)\in\mathcal{D}} |y-m|$
		\EndIf
		\EndIf
		
		\State \Return $I(\mathcal{D})$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Find the Best Split}
	\begin{algorithmic}[1]
		\State \textbf{Input:} dataset $\mathcal{D}$, feature set $\mathcal{F}$, criterion, minimum samples per leaf $N_{\text{leaf}}$
		\State \textbf{Output:} best split $(F, \tau, I_{\text{split}})$ or \texttt{None}
		
		\State Initialize: $I_{\text{split}} \gets +\infty$, $(F,\tau) \gets \texttt{None}$
		
		\For{each feature $F' \in \mathcal{F}$}
		
		\If{all samples in $\mathcal{D}$ have identical values on $F'$}
		\State \textbf{continue} \Comment{This feature cannot be split}
		\EndIf
		
		\State Generate candidate thresholds $\{\tau'\}$ from sorted unique values of feature $F'$
		\Comment{Typically midpoints between consecutive distinct values}
		
		\For{each threshold $\tau'$}
		
		\State $\mathcal{D}_L \gets \{(x,y)\in\mathcal{D}: x[F']\leqslant\tau'\}$, $\mathcal{D}_R \gets \{(x,y)\in\mathcal{D}: x[F'] > \tau'\}$
		
		\If{$|\mathcal{D}_L| < N_{\text{leaf}}$ \textbf{or} $|\mathcal{D}_R| < N_{\text{leaf}}$}
		\State \textbf{continue} \Comment{Violates min\_samples\_leaf}
		\EndIf
		
		\State $I_L \gets \Call{NodeImpurity}{\mathcal{D}_L, \text{criterion}}$, $I_R \gets \Call{NodeImpurity}{\mathcal{D}_R, \text{criterion}}$
		\State $I_{\text{split}}' \gets \dfrac{|\mathcal{D}_L|}{|\mathcal{D}|} I_L 
		+ \dfrac{|\mathcal{D}_R|}{|\mathcal{D}|} I_R$
		
		\If{$I_{\text{split}}' < I_{\text{split}}$}
		\State $I_{\text{split}}' \gets I_{\text{split}}$, $(F,\tau) \gets (F',\tau')$
		\EndIf
		
		\EndFor
		\EndFor
		
		\If{$(F,\tau) = \texttt{None}$}
		\State \Return \texttt{None}
		\Else
		\State \Return $(F,\tau, I_{\text{split}})$
		\EndIf
	\end{algorithmic}
\end{algorithm}

上述算法基于\gls{CART}框架，采用自顶向下的递归贪心策略生成二叉树结构。树的生长过程由函数\textsc{BuildTree}控制，并在每个节点通过搜索最优特征与阈值对$(F,\tau)$实现二叉划分。\par
在节点生成阶段，算法设置了多重停止条件以限制树的复杂度，包括最大深度$D_{\max}$、最小可分裂样本数$N_{\text{split}}$、节点纯度一致性以及特征不可再分性等，从而实现预剪枝正则化并防止过拟合。\par
节点划分依据加权节点纯度最小化准则，通过最小化：
\begin{equation*}
	I_{\text{split}}=\frac{|\mathcal{D}_L|}{|\mathcal{D}|}I(\mathcal{D}_L)+\frac{|\mathcal{D}_R|}{|\mathcal{D}|}I(\mathcal{D}_R)
\end{equation*}
选择最优划分，并要求纯度提升量$\Delta=I_{\text{parent}}-I_{\text{split}}$不小于给定阈值$\Delta_{\min}$。\par
当递归终止时生成叶节点。对于分类问题，叶节点预测为多数类并记录类别经验分布以支持概率输出；对于回归问题，叶节点预测为最小化所选损失函数的常数解。最终得到的决策树以递归结构表示，预测阶段沿节点划分路径自顶向下遍历直至到达叶节点获得预测结果。  

\subsection{Bagging模型}
\begin{definition}
	设$\mathcal{D}=\{(x_i,y_i)\}_{i=1}^n$为观测样本集，其中$x_i$为第$i$个样本的特征向量，$y_i$是第$i$个样本的标签或值，$\mathcal{D}^{(1)},\mathcal{D}^{(2)},\dots,\mathcal{D}^{(B)}$是从$\mathcal{D}$通过自助采样独立重复抽取的规模为$n$的训练子集，在每个子集$\mathcal{D}^{(b)}$上训练一个\gls{BaseLearner}$f^{(b)}$。\gls{BaggingEnsembleModel}定义为各基学习器输出的聚合函数。对于回归问题，集成预测函数定义为简单平均；对于分类问题，集成预测函数定义为多数投票。
\end{definition}
\begin{property}\label{prop:Bagging}
	设在固定输入点$x$处，Bagging模型各个基学习器的预测为随机变量$\{f^{(b)}(x)\}_{b=1}^B$。Bagging模型具有如下性质：
	\begin{enumerate}
		\item 若对任意$b$和$b'\ne b$有$\operatorname{Var}(f_b)=\sigma^2$且$\operatorname{Corr}(f_b,f_{b'})=\rho$，则Bagging模型预测的方差满足：
		\begin{equation*}
			\operatorname{Var}[f_{\text{bag}}(x)]=\rho\sigma^2+\frac{1-\rho}{B}\sigma^2
		\end{equation*}
		\item 二分类多数投票情形下，若每个基分类器的错误率$p<\dfrac{1}{2}$，则在各分类器相互独立的假设下，Bagging 集成分类器的错误率随$B$指数下降；当基分类器之间存在正相关时，错误率下降速度受相关性限制，并趋于一个正值。
	\end{enumerate}
\end{property}
\begin{proof}
	(1)由\cref{prop:CovMat}(3)和\cref{prop:Variance}(3)可知：
	\begin{align*}
		\operatorname{Var}\left(f_{\text{bag}}(x)\right)
		&=\operatorname{Var}\left(\frac{1}{B}\sum_{b=1}^Bf_b\right)=\frac{1}{B^2}\left[\sum_{b=1}^B\operatorname{Var}(f_b)+2\sum_{1\leqslant b<b'\leqslant B}\operatorname{Cov}(f_b,f_{b'})\right] \\
		&=\frac{1}{B^2}\left[B\sigma^2+B(B-1)\rho\sigma^2\right]=\sigma^2\left(\frac{1}{B}+\frac{B-1}{B}\rho\right)
		=\sigma^2\left(\rho+\frac{1-\rho}{B}\right)
	\end{align*}\par
	(2)设二分类情形中类别为$\{-1,1\}$，样本$x$的真实类别为$y$，第$b$个基分类器的错误指示变量记为$I_b=I[f^{(b)}(x)\ne y]$。\par
	若各分类器相互独立，则由条件和\cref{prop:Binom}(1)可知：
	\begin{equation*}
		P(I_b=1)=p<\dfrac{1}{2},\quad S_B=\sum_{b=1}^BI_b\sim\operatorname{Binomial}(B,p)
	\end{equation*}
	且多数投票出错当且仅当$S_B>B/2$，因此：
	\begin{equation*}
		I_{\text{bag}}=P\left(S_B>\frac{B}{2}\right)
	\end{equation*}
	由\info{Hoeffding不等式}可知：
	\begin{equation*}
		P\left(S_B>\frac{B}{2}\right)=P\left(\frac{S_B}{B}-p\geqslant\frac{1}{2}-p\right)\leqslant\exp\left[-2B\left(\frac{1}{2}-p\right)^2\right]
	\end{equation*}
	即当$p<1/2$时，Bagging的错误率随基学习器个数$B$指数级下降。\par
	若误差之间满足$\operatorname{Corr}(I_b,I_{b'})=\rho>0$，则由\cref{prop:Variance}(3)和\cref{prop:Binom}(2)可知：
	\begin{align*}
		\operatorname{Var}(S_B)		&=\operatorname{Var}\left(\sum_{b=1}^BI_b\right)=\sum_{b=1}^B\operatorname{Var}(I_b)+2\sum_{1\leqslant b<b'\leqslant B}\operatorname{Cov}(I_b,I_{b'}) \\
		&=Bp(1-p)+B(B-1)\rho p(1-p)=Bp(1-p)[1+(B-1)\rho]
	\end{align*}
	根据\cref{prop:CovMat}(3)可得$\operatorname{E}(S_B)=Bp$。因为$\operatorname{Var}(S_B)\sim B^2\rho p(1-p)(B\to\infty)$，于是由\info{CLT}可知：
	\begin{equation*}
		\frac{S_B-Bp}{\sqrt{B^2\rho p(1-p)}}\overset{d}{\longrightarrow}\operatorname{N}(0,1)
	\end{equation*}
	所以：
	\begin{align*}
		P\left(S_B>\frac{B}{2}\right)&=P\left(\frac{S_B-Bp}{\sqrt{B^2\rho p(1-p)}}>\frac{\frac{1}{2}-p}{\sqrt{\rho p(1-p)}}
		\right)  \\
		&\longrightarrow1-\Phi\left(\frac{\frac{1}{2}-p}{\sqrt{\rho p(1-p)}}\right)<1\quad B\to\infty\qedhere
	\end{align*}
\end{proof}
\begin{note}
	根据\cref{prop:Bagging}(1)(2)，集成学习的有效性依赖于基学习器之间同时具备较高预测精度与足够的异质性。若所有基学习器高度相关，简单平均或投票并不能显著改善模型性能。\par
	在 Bagging 框架中，异质性主要通过随机化机制引入：
	\begin{enumerate}
		\item \textbf{样本层面的异质性}：通过自助采样从经验分布中生成不同训练子集，使各基学习器在不同样本扰动下独立训练；
		\item \textbf{特征层面的异质性}：在扩展模型（如随机森林）中，仅在随机选取的特征子集上进行划分搜索，进一步降低基模型之间的相关性。
	\end{enumerate}
\end{note}
\section{Boosting模型}
假设一共有$m$个基模型，分别为$f_1(x),f_2(x),\dots,f_m(x)$，$n$个样本，$\seq{x}{n}$，则XGBoost模型的损失函数、正则项和目标函数分别为：
\begin{gather*}
	L=\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(m)}),\;R=\sum_{i=1}^{m}\Omega(f_i) \\
	Obj=L+R=\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(m)})+\sum_{i=1}^{m}\Omega(f_i)
\end{gather*}
第$t$步的目标函数为：
\begin{align*}
	Obj
	&=\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(t)})+\sum_{i=1}^{t}\Omega(f_i) \\
	&=\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(t-1)}+f_t(x_i))+\sum_{i=1}^{t-1}\Omega(f_i)+\Omega(f_t) \\
	&=\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(t-1)}+f_t(x_i))+\Omega(f_t)+C
\end{align*}
因为$C$是一个常数，所以可以直接扔掉。我们的目标是最优化目标函数，方式是通过引入新的一个基模型$f_t(x)$，所以要研究$f_t(x)$对于每一个样本具体取怎样的值能够使目标函数值降低，换句话说就是，$f_t(x_i)$该在损失函数$l$的第二个分量上往前或往后走多少？由此我们想到Taylor展开，Taylor展开其实就是表示自变量变化一定值后函数值的变化。将目标函数进行二阶展开来近似可以得到：
\begin{align*}
	\operatorname{Obj}
	&\approx\sum_{i=1}^{n}\left[\ell(y_i,\hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)\right]+\Omega(f_t) \\
	&=\sum_{i=1}^{n}\left[\frac{1}{2}h_if_t^2(x_i)+g_if_t(x_i)\right]+\Omega(f_t)+\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(t-1)})
\end{align*}
其中：
\begin{equation*}
	g_i=\frac{\partial \ell(y_i,z)}{\partial z}\Big|_{z=\hat{y}_i^{(t-1)}},\quad
	h_i=\frac{\partial^2 \ell(y_i,z)}{\partial z^2}\Bigl|_{z=\hat{y}_i^{(t-1)}}
\end{equation*}
考虑到此时已经拟合成功了$t-1$个基模型，上式最后一项是一个常数，所以也可以扔掉，于是我们的目标函数变为了：
\begin{equation*}
	\operatorname{Obj}=\sum_{i=1}^{n}\left[\frac{1}{2}h_if_t^2(x_i)+g_if_t(x_i)\right]+\Omega(f_t)
\end{equation*}

\subsection{抽象到具体}
上面我们进行的讨论都带着$f_t$，$f_t$是一个抽象的模型，接下来要选择一个具体的基模型去讨论，也就是说，我们得选择一个具体的基模型去训练，用这个目标函数去指导基模型的最优化问题。在原始论文中陈天奇选择了回归决策树模型，接下来我们也使用该模型去进行介绍。\par
陈天奇在回归决策树模型中使用了如下的正则项：
\begin{equation*}
	\Omega(f)=\gamma T+\lambda\sum_{i=1}^{T}\omega_i^2
\end{equation*}
其中$\gamma,\lambda$是控制惩罚强度的超参数，$T$是模型叶节点的数量，$\omega_i$是这棵树第$i$个叶节点的输出值。记$q(x)$为一个函数，它将样本$x$映射到$x$属于的叶节点的编号，也就是说$q(x_i)=j$表示$x_i$这个样本最后被划分到了树的第$j$个叶节点，该样本的训练输出值为$\omega_j$。记$I_j=\{i:q(x_i)=j\}$，于是上述的抽象目标函数在回归决策树下的具体目标函数为：
\begin{align*}
	\operatorname{Obj}
	&=\sum_{i=1}^{n}\left[\frac{1}{2}h_if_t^2(x_i)+g_if_t(x_i)\right]+\Omega(f_t) \\
	&=\sum_{i=1}^{n}\left[\frac{1}{2}h_iw_{q(x_i)}^2+g_iw_{q(x_i)}\right]+\gamma T+\lambda\sum_{i=1}^{T}\omega_i^2 \\
	&=\sum_{j=1}^{T}\left[\frac{1}{2}\left(\sum_{i\in I_j}h_i+\lambda\right)w_j^2+\left(\sum_{i\in I_j}g_i\right)\omega_j\right]+\gamma T
\end{align*}
\begin{algorithm}[H]
	\caption{XGBoost 第 \( t \) 步生成回归树 $f_t$}
	\begin{algorithmic}[1]
		\State \textbf{Input:} 样本集 $\mathcal{D} = \{(x_i, y_i)\}$，当前预测值 $\{\hat{y}_i^{(t-1)}\}$，正则项参数 $\gamma,\lambda$
		\State \textbf{Output:} 本轮生成的回归树 $f_t$
		
		\State \textbf{计算损失函数的一阶导数与二阶导数：}
		\For{每个样本 $i = 1, \dots, n$}
		\State 计算 $g_i=\dfrac{\partial \ell(y_i,z)}{\partial z}\Big|_{z=\hat{y}_i^{(t-1)}}$
		\State 计算 $h_i=\dfrac{\partial^2 \ell(y_i,z)}{\partial z^2}\Bigl|_{z=\hat{y}_i^{(t-1)}}$
		\EndFor
		
		\State \textbf{初始化根节点：}
		\State 将所有样本分配到根节点，计算节点的 $G = \sum_i g_i$，$H = \sum_i h_i$
		\State 计算当前节点的最优输出 $\omega = -\frac{G}{H + \lambda}$，当前目标函数值为 $\text{Obj} = -\frac{1}{2}\frac{G^2}{H + \lambda} + \gamma$
		
		\State \textbf{递归分裂节点：}
		\Function{SplitNode}{$\text{Node}, \mathcal{D}$}
		\State 初始化$\text{BestScore}=-\infty,\;\text{Split}=\text{FALSE},\;F^{\star},\;s^{\star}$
		\For{每个特征 $F$}
		\For{每个候选分裂点 $s$}
		\State 将样本根据 $x[F] < s$ 分成左右子集 $\mathcal{D}_L, \mathcal{D}_R$
		\State 分别计算 $G_L, H_L, G_R, H_R$
		\State 计算当前分裂的目标函数值增益：
		\[
		\text{Gain} = \operatorname{Obj}-\operatorname{Obj}_{L}-\operatorname{Obj}_{R}=\frac{1}{2}\left( \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda} \right) - \gamma
		\]
		\If{Gain$>0$}
		\State $\text{Split}=\text{TRUE}$
		\If{Gain$>BestScore$}
		\State $F^{\star}=F,\;s^{\star}=s$
		\EndIf
		\EndIf
		\EndFor
		\EndFor
		\If{Split$=$TRUE}
		\State 用最佳划分点分裂当前节点，得到LeftChild和RightChild
		\State \Call{SplitNode}{LeftChild, $\mathcal{D}_L$}
		\State \Call{SplitNode}{RightChild, $\mathcal{D}_R$}
		\EndIf
		\EndFunction
		
		\State \Return 构造出的回归树 $f_t$
	\end{algorithmic}
\end{algorithm}