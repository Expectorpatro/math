\section{决策树}

\subsection{算法流程}
%决策树算法的思路非常简单，很类似于我们日常作决策的一连串行为。写到这里的时候是星期四，就以疯狂星期四举例，假设我现在想吃KFC了，那我能不能去吃呢？第一个问题就是我有没有钱，如果有钱，那我接下来需要考虑是否有时间；如果没钱，那在去不去紫金商业街KFC这件事的决策上，我只能选择不去。如果有钱有时间，那我可以去；如果有钱没时间，那我还是去不了。决策树的执行就是这样一步一步做判断的过程。\par
%我们把一切之初称为\gls{RootNode}，即为树根，将每一个条件判断称为分支节点，将每一次决策结果产生的地方称为叶子节点
\begin{algorithm}
	\caption{分类决策树生成算法}
	\begin{algorithmic}[1]
		\State \textbf{Input:}
		 训练数据集\(\mathcal{D}\), 
		 特征集\(\mathcal{F}\), 
		 最大深度\(D_{\max}\), 
		 内部节点划分最小样本数\(N_{\text{split}}\), 
		 叶节点最小样本数\(N_{\text{leaf}}\), 
		 最小纯度变化\(\Delta_{\min}\)
		\State \textbf{Output:} 决策树决策字典 \(T\)
		
		\State 初始化决策树当前深度\(d\gets 0\)
		\Function{DecisionTree}{$\mathcal{D}, \mathcal{F}, D_{\max}, d, N_{\text{split}}, N_{\text{leaf}}, \Delta_{\min}$}
		\If{\(|\mathcal{D}| < N_{\text{split}}\) \textbf{or} \(d = D_{\max}\)}
		\State$\text{MajorityClass$(\mathcal{D})$}$计算$\mathcal{D}$中所占比例最大的标签，$\text{Table}(\mathcal{D})$对$\mathcal{D}$中的各标签数目进行计数
		\State \Return \(\{\,\text{type: "leaf", prediction: MajorityClass$(\mathcal{D})$, tabu: Table$(\mathcal{D})$}\,\}\)
		\EndIf
		\If{所有样本在 \(\mathcal{D}\) 中属于同一类别}
		\State \Return \(\{\,\text{type: "leaf", prediction: MajorityClass$(\mathcal{D})$, tabu: Table$(\mathcal{D})$}\,\}\)
		\EndIf
		\If{\(\mathcal{F} = \varnothing\) \textbf{or} 数据集中所有样本在 \(\mathcal{F}\) 上取值均相同}
		\State \Return \(\{\,\text{type: "leaf", prediction: MajorityClass$(\mathcal{D})$, tabu: Table$(\mathcal{D})$}\,\}\)
		\EndIf
		\State 选择最优划分特征 \(F \in \mathcal{F}\)，分类特征先使用\Call{FindBestBinarySpilt}{}（可选），然后使用\Call{SelectBestFeature}{}，连续型特征使用\Call{FindBestSpiltContinue}{}，再使用\Call{SelectBestFeature}{}，记其划分带来的纯度变化为$\delta$
		\If{$\delta$ \(< \Delta_{\min}\)}
		\State \Return \(\{\,\text{type: "leaf", prediction: MajorityClass$(\mathcal{D})$, tabu: Table$(\mathcal{D})$}\,\}\)
		\EndIf
		\State 初始化决策字典 \(T \gets \{\,\text{type: "node", feature: } F,\, \text{purity: }\delta,\, \text{branches: \{\}}\}\)
		\For{特征 \(F\)中每个可能的分支取值 \(F_i\)}
		\State 定义子集 \(\mathcal{D}_i = \{(x,y) \in \mathcal{D} \mid x[F] = F_i\}\)，使用\Call{PrePruning}{}（可选）
		\If{\(|\mathcal{D}_i| < N_{\text{leaf}}\)}
		\State \(T.\text{branches}[F_i] \gets \{\,\text{type: "leaf", prediction: MajorityClass$(\mathcal{D}_i)$, tabu: Table$(\mathcal{D}_i)$}\,\}\)
		\Else
		\State \(T.\text{branches}[F_i] \gets\) \Call{DecisionTree}{$\mathcal{D}_i, \mathcal{F}\setminus F, D_{\max}, d+1, N_{\text{split}}, N_{\text{leaf}}, \Delta_{\min}$}
		\EndIf
		\EndFor
		\State \Return \(T\)
		\EndFunction
		\State 使用\Call{PostPruning}{}（可选）
		\State 使用\Call{CCPVal}{}（可选）
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{回归决策树生成算法}
	\begin{algorithmic}[1]
		\State \textbf{Input:}
		训练数据集\(\mathcal{D}\), 
		特征集\(\mathcal{F}\), 
		最大深度\(D_{\max}\), 
		内部节点划分最小样本数\(N_{\text{split}}\), 
		叶节点最小样本数\(N_{\text{leaf}}\), 
		最小纯度变化\(\Delta_{\min}\)
		\State \textbf{Output:} 决策树决策字典 \(T\)
		
		\State 初始化决策树当前深度\(d\gets 0\)
		\Function{DecisionTree}{$\mathcal{D}, \mathcal{F}, D_{\max}, d, N_{\text{split}}, N_{\text{leaf}}, \Delta_{\min}$}
		\If{\(|\mathcal{D}| < N_{\text{split}}\) \textbf{or} \(d = D_{\max}\)}
		\State$\text{Mean$(\mathcal{D})$}$计算$\mathcal{D}$中目标变量的均值
		\State \Return \(\{\,\text{type: "leaf", prediction: Mean$(\mathcal{D})$, tabu: $|\mathcal{D}|$\,}\}\)
		\EndIf
		\If{所有样本在 \(\mathcal{D}\) 中属于同一类别}
		\State \Return \(\{\,\text{type: "leaf", prediction: Mean$(\mathcal{D})$, tabu: $|\mathcal{D}|$\,}\}\)
		\EndIf
		\If{\(\mathcal{F} = \varnothing\) \textbf{or} 数据集中所有样本在 \(\mathcal{F}\) 上取值均相同}
		\State \Return \(\{\,\text{type: "leaf", prediction: Mean$(\mathcal{D})$, tabu: $|\mathcal{D}|$\,}\}\)
		\EndIf
		\State 选择最优划分特征 \(F \in \mathcal{F}\)，分类特征先使用\Call{FindBestBinarySpilt}{}（可选），然后使用\Call{SplitScoreContinue}{}，连续型特征使用\Call{FindBestSpiltContinue}{}，再使用\Call{SelectScoreContinue}{}，记其划分带来的纯度变化为$\delta$
		\If{$\delta$ \(< \Delta_{\min}\)}
		\State \Return \(\{\,\text{type: "leaf", prediction: Mean$(\mathcal{D})$, tabu: $|\mathcal{D}|$\,}\}\)
		\EndIf
		\State 初始化决策字典 \(T \gets \{\,\text{type: "node", feature: } F,\, \text{purity: }\delta,\, \text{branches: \{\}}\}\)
		\For{特征 \(F\)中每个可能的分支取值 \(F_i\)}
		\State 定义子集 \(\mathcal{D}_i = \{(x,y) \in \mathcal{D} \mid x[F] = F_i\}\)，使用\Call{PrePruning}{}（可选）
		\If{\(|\mathcal{D}_i| < N_{\text{leaf}}\)}
		\State \(T.\text{branches}[F_i] \gets \{\,\text{type: "leaf", prediction: Mean$(\mathcal{D})$, tabu: $|\mathcal{D}|$\,}\}\)
		\Else
		\State \(T.\text{branches}[F_i] \gets\) \Call{DecisionTree}{$\mathcal{D}_i, \mathcal{F}\setminus F, D_{\max}, d+1, N_{\text{split}}, N_{\text{leaf}}, \Delta_{\min}$}
		\EndIf
		\EndFor
		\State \Return \(T\)
		\EndFunction
		\State 使用\Call{PostPruning}{}（可选）
		\State 使用\Call{CCPVal}{}（可选）
	\end{algorithmic}
\end{algorithm}


%显然，决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回：
%\begin{enumerate}
%	\item 当前结点包含的样本全属于同一类别$C$，无需划分。此时将当前节点标记为叶节点，其类别设定为$C$；
%	\item 当前
%	属性集为空，或是所有样本在所有属性上取值相同，无法划分
%\end{enumerate}(1)(2)；(3)当前结点包
%含的样本集合为空，不能划分.
%在第⑵种情形下，我们把当前结点标记为叶结点，并将其类别设定为该结
%点所含样本最多的类别；在第⑶种情形下，同样把当前结点标记为叶结点，但
%将其类别设定为其父结点所含样本最多的类别.注意这两种情形的处理实质不
%同：情形⑵是在利用当前结点的后验分布，而情形⑶则是把父结点的样本分布
%作为当前结点的先验分布.
%
%\subsection{最优划分特征的选择}
%在
%我们使用信息熵来定义样本纯度
%信息熵可以用来度量样本集合纯度最常用的一种指标
%假定当前样本集合。中第k类样本所占的比例为也(k = 1,2,…，3),则D
%的信息嫡定义为
%\subsubsection{信息增益准则}
%\begin{definition}
%	样本集$E$由$n$类不同的样本构成，第$i$类样本在$E$中所占的比例为$p_i,\;i=1,2,\dots,n$，定义$-\sum\limits_{i=1}^{n}p_i\log(p_i)$为该样本集的信息熵，记作$\mathrm{Ent}(E)$。
%\end{definition}
%由样本集信息熵的定义可以看出
%\begin{definition}
%	假设离散特征$a$有$n$个取值，分别为$\seq{a}{n}$。使用$a$来对样本集$E$进行划分，即将$E$分成$n$份，其中第$i$份包含了$E$中所有在特征$a$上取值为$a_i$的样本，记为$E^i$，$i=1,2,\dots,n$。定义：
%	\begin{equation*}
%		\mathrm{Gain}(E,a)=\mathrm{Ent}(E)-\sum_{i=1}^{n}\frac{|E_i|}{|E|}\mathrm{Ent}(E_i)
%	\end{equation*}
%	为用属性$a$对样本集$E$进行上述划分所获得的\gls{InformationGain}。
%\end{definition}
%从信息增益的计算公式可以看出，
%我们可以使用不同特征对样本集$E$进行划分后所获得信息增益来选择最优划分特征，即取信息增益最大的特征作为最优划分特征。

\begin{algorithm}
	\caption{选择最优特征（分类特征）}
	\begin{algorithmic}[1]
		\State \textbf{Input:} 训练数据集 \(\mathcal{D}\), 特征集 \(\mathcal{F}\), 信息准则 \(\text{Criterion}\)
		\State \textbf{Output:} 最优特征 \(F^*\)，最优特征对应的评分$\text{BestScore}$
		
		\Function{SelectBestFeature}{$\mathcal{D}, \mathcal{F}, \text{Criterion}$}
		\State 初始化最大评分 \(\text{BestScore} \gets -\infty\)
		\State 初始化最优特征 \(F^* \gets \varnothing\)
		
		\For{每个特征 \(F \in \mathcal{F}\)}
		\State 计算特征 \(F\) 的取值集合 \(V_F=(F_i)\)

		\If{\(\text{Criterion} = \text{Information Gain}\)}
		\State 计算数据集$\mathcal{D}$中目标变量$Y$的经验熵$H(Y)$（$\mathcal{D}_{i}$为目标变量取值为$y_i$的样本的集合，$i=1,2,\dots,n$）：
		\begin{align*}
			H(Y)=-\sum_{i=1}^n\frac{|\mathcal{D}_i|}{|\mathcal{D}|}\log_2\left(\frac{|\mathcal{D}_i|}{|\mathcal{D}|}\right)
		\end{align*}
		\State 计算数据集 \(\mathcal{D}\) 按照 \(F\) 取值划分后的条件熵（$\mathcal{D}_{F_i}$为变量$F=F_i$的样本的集合，$\mathcal{D}_{F_ij}$为$\mathcal{D}_{F_i}$中目标变量$Y=y_j$的样本的集合）：
		\begin{equation*}
			H(Y|F)=\sum_{i=1}^{|V_F|}\frac{|\mathcal{D}_{F_i}|}{|\mathcal{D}|}\sum_{j=1}^{n}\frac{|\mathcal{D}_{F_ij}|}{|\mathcal{D}_{F_i}|}\log_2\left(\frac{|\mathcal{D}_{F_ij}|}{|\mathcal{D}_{F_i}|}\right)
		\end{equation*}
		\State 计算信息增益 \(G(F) = H(Y) - H(Y \mid F)\)
		\State 设定当前评分 \(\text{Score} = G(F)\)
		\ElsIf{\(\text{Criterion} = \text{Information Gain Ratio}\)}
		\State 计算信息增益$G(F)$
		\State 计算特征熵 \(H(F)=-\sum\limits_{i=1}^{|V_F|}\dfrac{|\mathcal{D}_{F_i}|}{|\mathcal{D}|}\log_2\left(\dfrac{|\mathcal{D}_{F_i}|}{|\mathcal{D}|}\right)\)
		\State 计算信息增益率 \(GR(F) = \dfrac{G(F)}{H(F) + \epsilon}\)\Comment{$\varepsilon$保证数值稳定}
		\State 设定当前评分 \(\text{Score} = GR(F)\)
		\ElsIf{\(\text{Criterion} = \text{Gini Index}\)}
		\State 计算基尼指数：
		\begin{equation*}
			Gini(F) = \sum_{i=1}^{|V_F|} \frac{|\mathcal{D}_{F_i}|}{|\mathcal{D}|} Gini(\mathcal{D}_v)=\sum_{i=1}^{|V_F|} \frac{|\mathcal{D}_{F_i}|}{|\mathcal{D}|}\sum_{j=1}^{n}\left[1-\left(\frac{|D_{F_ij}|}{|D_{F_i}|}\right)^2\right]
		\end{equation*} 
		\State 设定当前评分 \(\text{Score} = -Gini(F)\) \Comment{基尼指数越小越好}
		\EndIf
		
		\If{\(\text{Score} > \text{BestScore}\)}
		\State \(\text{BestScore} \gets \text{Score}\)
		\State \(F^* \gets F\)
		\EndIf
		\EndFor
		
		\State \Return \(F^*, \text{BestScore}\)
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\begin{algorithm} 
	\caption{对连续特征的处理}
	\begin{algorithmic}[1]
		\State \textbf{Input:} 训练数据集 \(\mathcal{D}\), 连续特征 \(F\)
		\State \textbf{Output:} 最优划分点 \(t^*\)
		
		\Function{FindBestSplitContinue}{$\mathcal{D}, F$}
		\State 令 \(V_F = \{ x[F] \mid (x,y) \in \mathcal{D} \}\) 为 \(F\) 的所有取值
		\State 将 \(V_F\) 按升序排序
		\State 初始化最优分割点 \(t^* \gets \varnothing\)
		\State 初始化最大评分 \(\text{BestScore} \gets -\infty\)
		
		\For{每个相邻取值 \((v_i, v_{i+1}) \in V_F\)}
		\State 计算候选划分点 \(t = \frac{v_i + v_{i+1}}{2}\)
		\State 将数据集 \(\mathcal{D}\) 根据 \(F \leqslant t\) 划分为 \(\mathcal{D}_1\) 和 \(\mathcal{D}_2\)
		\State 计算划分后的评分$\text{Score}=$\Call{SplitScoreContinue}{$\mathcal{D},\mathcal{D}_1,\mathcal{D}_2$}
		\If{\(\text{Score} > \text{BestScore}\)}
		\State \(\text{BestScore} \gets \text{Score}\)
		\State \(t^* \gets t\)
		\EndIf
		\EndFor
		
		\State \Return \(t^*\)
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\begin{algorithm} 
	\caption{对分类变量的二分处理}
	\begin{algorithmic}[1]
		\State \textbf{Input:} 训练数据集 \(\mathcal{D}\), 分类特征 \(F\)
		\State \textbf{Output:} 最优二分子集 \(S^*\)
		
		\Function{FindBestBinarySplit}{$\mathcal{D}, F$}
		\State 令 \(V_F = \{ x[F] \mid (x,y) \in \mathcal{D} \}\) 为 \(F\) 的所有取值
		\State 令 \(\mathcal{P}(V_F)\) 为 \(V_F\) 的所有可能的非空真子集
		\State 初始化最优二分子集 \(S^* \gets \varnothing\)
		\State 初始化最大评分 \(\text{BestScore} \gets -\infty\)
		
		\For{每个可能的二分子集 \(S \subset V_F\)}
		\State 将数据集 \(\mathcal{D}\) 根据 \(x[F] \in S\)与$x[F]\notin S$ 划分为 \(\mathcal{D}_1\) 和 \(\mathcal{D}_2\)
		\State 计算划分后的评分$\text{Score}=$\Call{SplitScoreContinue}{$\mathcal{D},\mathcal{D}_1,\mathcal{D}_2$}
		\If{\(\text{Score} > \text{BestScore}\)}
		\State \(\text{BestScore} \gets \text{Score}\)
		\State \(S^* \gets S\)
		\EndIf
		\EndFor
		
		\State \Return \(S^*\)
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{计算回归问题的分划得分}
	\begin{algorithmic}[1]
		\State \textbf{Input:} 训练数据集 \(\mathcal{D}\), 二分后的子集 \(\mathcal{D}_1, \mathcal{D}_{2}\)
		\State \textbf{Output:} 当前划分的得分 \(\text{Score}\)
		
		\Function{SplitScoreContinue}{$\mathcal{D}, \mathcal{D}_{1}, \mathcal{D}_{2}$}
		\State 计算两个子集的均方误差：
		\begin{gather*}
			\bar{y}_1=\frac{1}{|\mathcal{D}_1|}\sum_{(x,y)\in\mathcal{D}_1}y,\;\text{MSE}_{1} = \frac{1}{|\mathcal{D}_{1}|} \sum_{(x,y) \in \mathcal{D}_{1}} (y - \bar{y}_{1})^2 \\
			\bar{y}_2=\frac{1}{|\mathcal{D}_2|}\sum_{(x,y)\in\mathcal{D}_2}y,\;\text{MSE}_{2} = \frac{1}{|\mathcal{D}_{2}|} \sum_{(x,y) \in \mathcal{D}_{2}} (y - \bar{y}_{2})^2
		\end{gather*}
		\State 计算整体加权均方误差：
		\begin{equation*}
			\text{Score} = - \left( \frac{|\mathcal{D}_{1}|}{|\mathcal{D}|} \text{MSE}_{1} + \frac{|\mathcal{D}_{2}|}{|\mathcal{D}|} \text{MSE}_{2} \right)
		\end{equation*}
		\State \Return \(\text{Score}\)
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{预剪枝：利用验证集比较划分前后的误差}
	\begin{algorithmic}[1]
		\State \textbf{Input:} 当前节点训练数据$\mathcal{D}$，当前节点验证数据 \(\mathcal{D}_{val}\)，当前决策路径$T$
		\State \textbf{Output:} 是否继续划分（True/False）
		\Function{PrePruning}{$\mathcal{D},\mathcal{D}_{val},T$}
		\State 计算不划分的验证误差：不划分则当前节点化为叶节点，叶节点数值由$\mathcal{D}$决定，于是该条决策路径确定，可据此计算$\mathcal{D}_{val}$中在该条决策路径上的样本的误差$e_1$。
		
		\State 计算划分后的验证误差（加权平均误差）：若划分，将划分后的节点化为叶节点，叶节点数值由$\mathcal{D}$决定，于是该条决策路径确定，可据此计算$\mathcal{D}_{val}$中在该条决策路径上的样本的加权平均误差$e_2$。
		
		\If{\(e_2 < e_1\)}
		\State \Return \textbf{True} \Comment{候选划分能降低验证误差，继续划分}
		\Else
		\State \Return \textbf{False} \Comment{划分后误差未降低，不划分，将当前节点作为叶节点}
		\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{后剪枝算法}
	\begin{algorithmic}[1]
		\State \textbf{Input:} 已生成的决策树 \( T \)，验证数据集 \( \mathcal{D}_{val} \)
		\State \textbf{Output:} 剪枝后的决策树 \( T^* \)
		
		\Function{PostPruning}{$T, \mathcal{D}_{val}$}
		\For{树 \( T \) 中的每个内部节点 \( N \)（按照树的深度自底向上的顺序）}
		\State 计算节点 \( N \) 的当前子树在验证集 \( \mathcal{D}_{val} \) 上的误差 \( E_{\text{subtree}} \)
		\State 将节点 \( N \) 暂时替换为叶节点，计算替换后的树在验证集上的误差 \( E_{\text{pruned}} \)
		\If{\( E_{\text{pruned}} \leqslant E_{\text{subtree}} \)}\Comment{等于也要剪枝，这是基于奥卡姆剃刀准则}
		\State 将节点 \( N \) 永久替换为叶节点
		\Else
		\State 恢复节点 \( N \) 的原始子树结构
		\EndIf
		\EndFor
		\State \Return 剪枝后的决策树 \( T \)
		\EndFunction
	\end{algorithmic}
\end{algorithm}
	
\begin{algorithm}
	\caption{代价复杂度剪枝 (Cost-Complexity Pruning with Validation)}
	\begin{algorithmic}[1]
		\State \textbf{Input:} 验证集 $\mathcal{D}_{val}$, 未剪枝完全生长的初始决策树 $T_0$
		\State \textbf{Output:} 最优子树 $T^*$
		
		\Function{CCPVal}{$\mathcal{D}_{val},T_0$}
		\State 初始化候选树集 $\mathcal{T} \gets \{ T_0 \}$
		\State 初始化代价复杂度序列 $\alpha \gets \emptyset$
		\State 令 $T \gets T_0$
		
		\While{$T$ 不是一棵只有根节点的树}
		\For{每个内部节点 $t$ in $T$}
		\State 计算训练集子树 $T_t$（以 $t$ 为根）的损失 $R(T_t)$
		\State 计算节点 $t$ 的剪枝指标:
		\[
		\alpha_t = \frac{R(T_t) - R(t)}{|T_t| - 1}
		\]
		\EndFor
		\State 令 $\alpha_{\min} = \min \alpha_t$，将 $\alpha_{\min}$ 加入序列 $\alpha$，找到其对应节点 $t^*$，将 $T_{t^*}$ 剪去，令其变为叶节点，得到新树 $T'$
		\State 更新 $T \gets T'$ 并加入候选树集：$\mathcal{T} \gets \mathcal{T} \cup \{T\}$
		\EndWhile

		\State 令 $T^* = \arg\min_{T_i \in \mathcal{T}}\text{Error}(T_i, \mathcal{D}_{val})$，其中$\text{Error}(T_i, \mathcal{D}_{val})$是树$T_i$在验证集$\mathcal{D}_{val}$上的误差
		\State \Return $T^*$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊
\begin{enumerate}
	\item ID3=分类决策树生成+信息增益。
	\item C4.5=分类决策树生成+信息增益率+连续变量处理+缺失值处理+两种剪枝策略。
	\item CART=分类/回归决策树生成+Gini系数+分类变量二分处理+连续变量处理+回归时的变量选择问题+缺失值处理+三种剪枝策略。
	\item 算法前面的if判断是为了不让决策树过于复杂而使用的正则化技术以及无法再分枝时的决策树生长终止条件。
	\item 决策树最终得到的即为决策字典$T$，只需按照键的顺序往下走，即可得到样本的预测值或预测类别。
	\item 记录tabu是为了计算特征重要性以及分类问题时的预测概率（预测概率由该叶节点中训练集真实标签等于叶节点标签的样本比例）。
	\item 信息增益率考虑了特征熵，缓解了信息增益偏好类似于编号一类的特征的问题。
	\item Gini指数计算方便，不涉及对数运算，提高了运算效率。
	\item 标准CART中无论是连续型特征还是分类型特征都划分成两类，于是它构成二叉树，二叉树可以提高效率。
	\item 标准CART每个特征都可以使用多次，不需要每次递归时去除上一次使用的特征。
	\item 预剪枝是一种正则化手段，但是它容易欠拟合，因为当前没必要的分支可能在接下来的分支中提高预测精度。后剪枝也是一种正则化技术，它平衡了欠拟合与过拟合，但是运算效率最低，毕竟要完全生成树然后对完全生成树的每一个内部节点进行计算。
	\item 代价复杂度剪枝考虑代价：
	\begin{equation*}
		C_\alpha(T)=R(T)+\alpha|T|
	\end{equation*}
	其中$R(T)$为树$T$对于训练集的误差，$|T|$表示树$T$的叶节点数目，表征树$T$的复杂度，$\alpha$是一个超参数，平衡训练集误差与正则化项的重要性。对于一个内部节点$t\in T$，我们要判断是否要对其进行剪枝，也就是看剪枝前后以该节点为根节点的子树的代价变化（剪枝前很容易理解，即不看$t$节点之前的树，把$t$当作根节点考虑子树；剪枝后则$t$变为叶节点，只需考虑$t$这个叶节点的代价）。显然剪枝前后的代价分别为：
	\begin{equation*}
		C_\alpha(T_t)=R(T_t)+\alpha|T_t|,\;C_\alpha(t)=R(t)+\alpha
	\end{equation*}
	若$C_\alpha(t)\leqslant C_\alpha(T_t)$，则进行剪枝。$R(T_t)$和$R(t)$都是固定值，显然随着$\alpha$的增大，$C_\alpha(T_t)$会越来越大，所以我们要考虑的临界情况为：
	\begin{equation*}
		R(T_t)+\alpha|T_t|=R(t)+\alpha\longrightarrow\alpha=\frac{R(t)-R(T_t)}{|T_t|-1}
	\end{equation*}
	计算树$T$中所有内部节点的临界$\alpha$值，将其从小到大排序为$\seq{\alpha}{n}$，若剪掉$\alpha_1$对应的节点，则对于当前问题，新的树将是$\alpha\in[\alpha_1,\alpha_2)$中的最优树，这是因为剩余的节点中最小的临界值也是$\alpha_2$，当$\alpha\geqslant\alpha_2$时，因为$\alpha_2\leqslant\alpha$，则$\alpha_2$对应的节点也应该被剪掉。于是我们就可以使用递归的方案，求出$\seq{\alpha}{n}$对应的$n$棵树，然后用验证集测试它们的性能，选择最优的一个作为剪枝的结果。
	\item Sklearn中实现的代价复杂度剪枝并没有使用验证集验证，而是设置最大的$\alpha$值，将节点的临界值从小到大排序得到$\seq{\alpha}{n}$，剪枝剪到$\alpha_i>\alpha$为止，即对$\alpha_i\leqslant\alpha$的所有节点都进行剪枝。
\end{enumerate}

\section{XGBoost}
假设一共有$m$个基模型，分别为$f_1(x),f_2(x),\dots,f_m(x)$，$n$个样本，$\seq{x}{n}$，则XGBoost模型的损失函数、正则项和目标函数分别为：
\begin{gather*}
	L=\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(m)}),\;R=\sum_{i=1}^{m}\Omega(f_i) \\
	Obj=L+R=\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(m)})+\sum_{i=1}^{m}\Omega(f_i)
\end{gather*}
第$t$步的目标函数为：
\begin{align*}
	Obj
	&=\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(t)})+\sum_{i=1}^{t}\Omega(f_i) \\
	&=\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(t-1)}+f_t(x_i))+\sum_{i=1}^{t-1}\Omega(f_i)+\Omega(f_t) \\
	&=\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(t-1)}+f_t(x_i))+\Omega(f_t)+C
\end{align*}
因为$C$是一个常数，所以可以直接扔掉。我们的目标是最优化目标函数，方式是通过引入新的一个基模型$f_t(x)$，所以要研究$f_t(x)$对于每一个样本具体取怎样的值能够使目标函数值降低，换句话说就是，$f_t(x_i)$该在损失函数$l$的第二个分量上往前或往后走多少？由此我们想到Taylor展开，Taylor展开其实就是表示自变量变化一定值后函数值的变化。将目标函数进行二阶展开来近似可以得到：
\begin{align*}
	\operatorname{Obj}
	&\approx\sum_{i=1}^{n}\left[\ell(y_i,\hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)\right]+\Omega(f_t) \\
	&=\sum_{i=1}^{n}\left[\frac{1}{2}h_if_t^2(x_i)+g_if_t(x_i)\right]+\Omega(f_t)+\sum_{i=1}^{n}\ell(y_i,\hat{y}_i^{(t-1)})
\end{align*}
其中：
\begin{equation*}
	g_i=\frac{\partial \ell(y_i,z)}{\partial z}\Big|_{z=\hat{y}_i^{(t-1)}},\quad
	h_i=\frac{\partial^2 \ell(y_i,z)}{\partial z^2}\Bigl|_{z=\hat{y}_i^{(t-1)}}
\end{equation*}
考虑到此时已经拟合成功了$t-1$个基模型，上式最后一项是一个常数，所以也可以扔掉，于是我们的目标函数变为了：
\begin{equation*}
	\operatorname{Obj}=\sum_{i=1}^{n}\left[\frac{1}{2}h_if_t^2(x_i)+g_if_t(x_i)\right]+\Omega(f_t)
\end{equation*}

\subsection{抽象到具体}
上面我们进行的讨论都带着$f_t$，$f_t$是一个抽象的模型，接下来要选择一个具体的基模型去讨论，也就是说，我们得选择一个具体的基模型去训练，用这个目标函数去指导基模型的最优化问题。在原始论文中陈天奇选择了回归决策树模型，接下来我们也使用该模型去进行介绍。\par
陈天奇在回归决策树模型中使用了如下的正则项：
\begin{equation*}
	\Omega(f)=\gamma T+\lambda\sum_{i=1}^{T}\omega_i^2
\end{equation*}
其中$\gamma,\lambda$是控制惩罚强度的超参数，$T$是模型叶节点的数量，$\omega_i$是这棵树第$i$个叶节点的输出值。记$q(x)$为一个函数，它将样本$x$映射到$x$属于的叶节点的编号，也就是说$q(x_i)=j$表示$x_i$这个样本最后被划分到了树的第$j$个叶节点，该样本的训练输出值为$\omega_j$。记$I_j=\{i:q(x_i)=j\}$，于是上述的抽象目标函数在回归决策树下的具体目标函数为：
\begin{align*}
	\operatorname{Obj}
	&=\sum_{i=1}^{n}\left[\frac{1}{2}h_if_t^2(x_i)+g_if_t(x_i)\right]+\Omega(f_t) \\
	&=\sum_{i=1}^{n}\left[\frac{1}{2}h_iw_{q(x_i)}^2+g_iw_{q(x_i)}\right]+\gamma T+\lambda\sum_{i=1}^{T}\omega_i^2 \\
	&=\sum_{j=1}^{T}\left[\frac{1}{2}\left(\sum_{i\in I_j}h_i+\lambda\right)w_j^2+\left(\sum_{i\in I_j}g_i\right)\omega_j\right]+\gamma T
\end{align*}


\begin{algorithm}
	\caption{XGBoost 第 \( t \) 步生成回归树 $f_t$}
	\begin{algorithmic}[1]
		\State \textbf{Input:} 样本集 $\mathcal{D} = \{(x_i, y_i)\}$，当前预测值 $\{\hat{y}_i^{(t-1)}\}$，正则项参数 $\gamma,\lambda$
		\State \textbf{Output:} 本轮生成的回归树 $f_t$
		
		\State \textbf{计算损失函数的一阶导数与二阶导数：}
		\For{每个样本 $i = 1, \dots, n$}
		\State 计算 $g_i=\dfrac{\partial \ell(y_i,z)}{\partial z}\Big|_{z=\hat{y}_i^{(t-1)}}$
		\State 计算 $h_i=\dfrac{\partial^2 \ell(y_i,z)}{\partial z^2}\Bigl|_{z=\hat{y}_i^{(t-1)}}$
		\EndFor
		
		\State \textbf{初始化根节点：}
		\State 将所有样本分配到根节点，计算节点的 $G = \sum_i g_i$，$H = \sum_i h_i$
		\State 计算当前节点的最优输出 $\omega = -\frac{G}{H + \lambda}$，当前目标函数值为 $\text{Obj} = -\frac{1}{2}\frac{G^2}{H + \lambda} + \gamma$
		
		\State \textbf{递归分裂节点：}
		\Function{SplitNode}{$\text{Node}, \mathcal{D}$}
		\State 初始化$\text{BestScore}=-\infty,\;\text{Split}=\text{FALSE},\;F^*,\;s^*$
		\For{每个特征 $F$}
		\For{每个候选分裂点 $s$}
		\State 将样本根据 $x[F] < s$ 分成左右子集 $\mathcal{D}_L, \mathcal{D}_R$
		\State 分别计算 $G_L, H_L, G_R, H_R$
		\State 计算当前分裂的目标函数值增益：
		\[
		\text{Gain} = \operatorname{Obj}-\operatorname{Obj}_{L}-\operatorname{Obj}_{R}=\frac{1}{2}\left( \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda} \right) - \gamma
		\]
		\If{Gain$>0$}
		\State $\text{Split}=\text{TRUE}$
		\If{Gain$>BestScore$}
		\State $F^*=F,\;s^*=s$
		\EndIf
		\EndIf
		\EndFor
		\EndFor
		\If{Split$=$TRUE}
		\State 用最佳划分点分裂当前节点，得到LeftChild和RightChild
		\State \Call{SplitNode}{LeftChild, $\mathcal{D}_L$}
		\State \Call{SplitNode}{RightChild, $\mathcal{D}_R$}
		\EndIf
		\EndFunction
		
		\State \Return 构造出的回归树 $f_t$
	\end{algorithmic}
\end{algorithm}











