\section{信息论}

\subsection{信息量}
我们想要找一个函数$I(x)$来对事件产生的\gls{InformationQuantity}进行定量的分析，其中$x$代表着一个事件。显然信息量具有如下两个性质：
\begin{enumerate}
	\item 一件事发生的概率越小，那么这件事发生后产生的信息量越大；
	\item 如果两件事情独立，那么这两件事情都发生所产生的信息量应该等于每件事情各自发生产生的信息量之和。
\end{enumerate}
由(1)，$I(x)$需要与$x$发生的概率呈反比；由(2)，$I(x)$应具有对数的形式，因为如果$f(x)=\log\Big(\mathrm{Pr}(x)\Big)$，将$x,y$两个互相独立的事件同时发生的概率记为$\mathrm{Pr}(x,y)$、产生的信息量记为$f(x,y)$，那么：
\begin{align*}
	f(x,y)&=\log\Big(\mathrm{Pr}(x,y)\Big)=\log\Big(\mathrm{Pr}(x)\mathrm{Pr}(y)\Big) \\
	&=\log\Big(\mathrm{Pr}(x)\Big)+\log\Big(\mathrm{Pr}(y)\Big) \\
	&=f(x)+f(y)
\end{align*}
综合考虑以上两点，我们给出如下信息量的定义：
\begin{definition}
	一件事情发生所产生的信息量定义为它发生概率倒数的对数，即$I(x)=-\log\Big(\mathrm{Pr(x)}\Big)$。对数底的选择是任意的，但在信息论中普遍使用$2$作为对数的底。
\end{definition}

\subsection{信息熵}
\begin{definition}
	\gls{InformationEntropy}是可能产生的信息量的期望。设$X$是一个离散型随机变量，有$n$个取值，分别为$x_1,x_2,\dots,x_n$，则$X$的信息熵$H(X)$为：
	\begin{align*}
		H(X)&=E[I(X)]=E\left[-\log_2\Big(\mathrm{Pr}(X)\Big)\right] \\
		&=-\sum_{i=1}^n\mathrm{Pr}(X=x_i)\log_2[\mathrm{Pr}(X=x_i)]
	\end{align*}
	称由样本计算得到的信息熵为\gls{EmpiricalEntropy}。
\end{definition}
信息熵同样表征了不确定性的大小（思考是为什么）。 

\subsubsection{条件熵}
\begin{definition}
	\gls{ConditionalEntropy}是给定一定条件下某个随机变量的信息熵。设$X,Y$是两个离散型随机变量，各自有$n$个和$m$个取值，分别为$x_1,x_2,\dots,x_n$和$y_1,y_2,\dots,y_m$，则$X$在$Y$下的条件熵$H(X|Y)$为：
	\begin{align*}
		H(X|Y)
		&=E[I(X|Y)]=E\left[\sum_{i=1}^m\mathrm{Pr}(Y=y_i)I(X|Y=y_i)\right] \\
		&=\sum_{i=1}^m\mathrm{Pr}(Y=y_i)E\left[I(X|Y=y_i)\right] \\
		&=\sum_{i=1}^m\mathrm{Pr}(Y=y_i)E\left[-\log_2\Big(\mathrm{Pr}(X|Y=y_i)\Big)\right] \\
		&=-\sum_{i=1}^m\mathrm{Pr}(Y=y_i)\sum_{j=1}^n\mathrm{Pr}(X=x_j|Y=y_i)\log_2\Big[\mathrm{Pr}(X=x_j|Y=y_i)\Big]
	\end{align*}
	$X$在$Y=y_i$下的条件熵$H(X|Y=y_i)$为：
	\begin{align*}
		H(X|Y=y_i)&=E[I(X|Y=y_i)] \\
		&=E\left[-\log_2\Big(\mathrm{Pr}(X|Y=y_i)\Big)\right] \\
		&=-\sum_{j=1}^n\mathrm{Pr}(X=x_j|Y=y_i)\log_2\Big[\mathrm{Pr}(X=x_j|Y=y_i)\Big]
	\end{align*}
\end{definition}