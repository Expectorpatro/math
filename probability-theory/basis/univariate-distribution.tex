\section{分布族}

\subsection{二项分布族}
\begin{definition}
	若离散型随机变量$X$的分布列为：
	\begin{equation*}
		P(X=k)=\binom{n}{k}p^k(1-p)^{n-k},\;k=0,1,\dots,n
	\end{equation*}
	其中$n\in\mathbb{N}^+$和$p\in[0,1]$为参数，则称$X$服从\gls{BinomialDistribution}，记作$X\sim\operatorname{Binom}(n,p)$。$\operatorname{Binom}(1,p)$也称作\textbf{0-1分布}或\gls{BernoulliDistribution}。
\end{definition}
\begin{property}\label{prop:Binom}
	二项分布族具有如下性质：
	\begin{enumerate}
		\item 二项分布具有可加性，即若$X_i\sim\operatorname{Binom}(n_i,p)$且$X_i$相互独立，$i=1,2,\dots,m$，则$\sum\limits_{i=1}^{m}X_i\sim\operatorname{Binom}\left(\sum\limits_{i=1}^{m}n_i,p\right)$；
		\item 若$X\sim\operatorname{Binom}(n,p)$，则$\operatorname{E}(X)=np,\;\operatorname{Var}(X)=np(1-p)$；
		\item 若$X_1\sim\operatorname{Binom}(n,p_1),\;X_2\sim\operatorname{Binom}(n,p_2),\;p_1<p_2$，则$F_{X_1}(x)\geqslant F_{X_2}(x)$；
	\end{enumerate}
\end{property}
\begin{proof}
	(2)注意到：
	\begin{align*}
		&\operatorname{E}(X)=\sum_{i=0}^{n}i\binom{n}{i}p^i(1-p)^{n-i}=\sum_{i=1}^{n}i\frac{n!}{i!(n-i)!}p^i(1-p)^{n-i} \\
		=&\sum_{i=1}^{n}\frac{n(n-1)!}{(i-1)!(n-i)!}p^i(1-p)^{n-i}=np\sum_{i=1}^{n}\binom{n-1}{i-1}p^{i-1}(1-p)^{n-i}=np \\
		&\operatorname{E}(X^2)=\sum_{i=0}^{n}i^2\binom{n}{i}p^i(1-p)^{n-i}=\sum_{i=0}^{n}[i(i-1)+i]\binom{n}{i}p^i(1-p)^{n-i} \\
		=&\sum_{i=0}^{n}i(i-1)\binom{n}{i}p^i(1-p)^{n-i}+\operatorname{E}(X)=\sum_{i=2}^{n}i(i-1)\frac{n!}{i!(n-i)!}p^i(1-p)^{n-i}+np \\
		=&\sum_{i=2}^{n}\frac{n(n-1)(n-2)!}{(i-2)!(n-i)!}p^i(1-p)^{n-i}+np=n(n-1)p^2\sum_{i=2}^{n}\binom{n-2}{i-2}p^{i-2}(1-p)^{n-i}+np \\
		=&n(n-1)p^2+np
	\end{align*}
	由\cref{prop:Variance}(1)可得：
	\begin{equation*}
		\operatorname{Var}(X)=\operatorname{E}(X^2)-[\operatorname{E}(X)]^2=n(n-1)p^2+np-n^2p^2=n^2p^2-np^2+np-n^2p^2=np(1-p)
	\end{equation*}\par
	(3)设$X\sim\operatorname{Binom}(n,p)$，则其分布函数为：
	\begin{equation*}
		F(k)=\sum_{i=0}^{k}\binom{n}{k}p^k(1-p)^{n-k}
	\end{equation*}
	求其关于$p$的导数可得：
	\begin{align*}
		&F'(k)=\frac{\dif}{\dif p}\sum_{i=0}^{k}\binom{n}{i}p^i(1-p)^{n-i} \\
		=&\sum_{i=0}^{k}\binom{n}{i}ip^{i-1}(1-p)^{n-i}-\sum_{i=0}^{k}\binom{n}{i}(n-i)p^i(1-p)^{n-i-1} \\
		=&\sum_{i=0}^{k}n\binom{n-1}{i-1}p^{i-1}(1-p)^{n-i}-\sum_{i=0}^{k}n\binom{n-1}{i}p^i(1-p)^{n-i-1} \\
		=&\sum_{i=1}^{k}n\binom{n-1}{i-1}p^{i-1}(1-p)^{n-i}-\sum_{i=0}^{k}n\binom{n-1}{i}p^i(1-p)^{n-i-1} \\
		=&-n\binom{n-1}{k}p^k(1-p)^{n-k-1}<0\qedhere
	\end{align*}
\end{proof}

\subsection{负二项分布族}
\begin{definition}
	若离散型随机变量$X$的分布列为：
	\begin{equation*}
		P(X=k)=\binom{k-1}{r-1}p^r(1-p)^{k-r},\;k=r,r+1,\dots
	\end{equation*}
	其中$r\in\mathbb{N}^+$和$p\in[0,1]$为参数，则称$X$服从\gls{NegativeBinomialDistribution}，记作$X\sim\operatorname{NB}(r,p)$。$\operatorname{NB}(1,p)$也称作\gls{GeometricDistribution}，记作$X\sim\operatorname{Geom}(p)$。
\end{definition}
\begin{property}
	负二项分布族具有如下性质：
	\begin{enumerate}
		\item 负二项分布具有可加性，即若$X_i\sim\operatorname{NB}(r_i,p)$且$X_i$相互独立，$i=1,2,\dots,m$，则$\sum\limits_{i=1}^{m}X_i\sim\operatorname{NB}\left(\sum\limits_{i=1}^{m}r_i,p\right)$；
		\item 几何分布具有无记忆型，即若$X\sim\operatorname{Geom}(p)$，则对任意的$m,n\in\mathbb{N}^+$有$P(X>m+n|X>m)=P(X>n)$；
		\item 若$X\sim\operatorname{NB}(r,p)$，则$\operatorname{E}(X)=\dfrac{r}{p},\;\operatorname{Var}(X)=\dfrac{r(1-p)}{p^2}$。
	\end{enumerate}
\end{property}
\begin{proof}
	(3)注意到：
	\begin{gather*}
		\begin{aligned}
			\operatorname{E}(X)&=\sum_{n=r}^{+\infty}n\binom{n-1}{r-1}p^r(1-p)^{n-r}=\sum_{n=r}^{+\infty}n\frac{(n-1)!}{(r-1)!(n-r)!}p^r(1-p)^{n-r} \\
			&=\frac{r}{p}\sum_{n=r}^{+\infty}\frac{n!}{r!(n-r)!}p^{r+1}(1-p)^{n-r}=\frac{r}{p} \\
		\end{aligned} \\
		\begin{aligned}
			\operatorname{E}(X^2)&=\sum_{n=r}^{+\infty}n^2\binom{n-1}{r-1}p^r(1-p)^{n-r}=\sum_{n=r}^{+\infty}n^2\frac{(n-1)!}{(r-1)!(n-r)!}p^r(1-p)^{n-r} \\
			&=\sum_{n=r}^{+\infty}[n(n+1)-n]\frac{(n-1)!}{(r-1)!(n-r)!}p^r(1-p)^{n-r}= \\
			&=\sum_{n=r}^{+\infty}n(n+1)\frac{(n-1)!}{(r-1)!(n-r)!}p^r(1-p)^{n-r}-\operatorname{E}(X) \\
			&=r(r+1)\sum_{n=r}^{+\infty}\frac{(n+1)!}{(r+1)!(n-r)!}p^r(1-p)^{n-r}-\frac{r}{p} \\
			&=\frac{r(r+1)}{p^2}\sum_{n=r}^{+\infty}\frac{(n+1)!}{(r+1)!(n-r)!}p^{r+2}(1-p)^{n-r}-\frac{r}{p}=\frac{r(r+1)-rp}{p^2}
		\end{aligned}
	\end{gather*}
	由\cref{prop:Variance}(1)可得：
	\begin{equation*}
		\operatorname{Var}(X)=\operatorname{E}(X^2)-[\operatorname{E}(X)]^2=\frac{r(r+1)-rp-r^2}{p^2}=\frac{r(1-p)}{p^2}\qedhere
	\end{equation*}
\end{proof}

\subsection{超几何分布}
\begin{definition}
	若离散型随机变量$X$的分布列为：
	\begin{equation*}
		P(X=k)=\frac{\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{n}},\;k=0,1,\dots,\min\{n,M\}
	\end{equation*}
	其中$n,M,N\in\mathbb{N}^+$为参数，则称$X$服从\gls{HypergeometricDistribution}，记作$X\sim\operatorname{Hyper}(n,M,N)$。
\end{definition}
\begin{property}
	超几何分布具有如下性质：
	\begin{enumerate}
		\item 若$X\sim\operatorname{Hyper}(n,M,N)$，则$\operatorname{E}(X)=\dfrac{nM}{N},\;\operatorname{Var}(X)=\dfrac{nM(N-n)(N-M)}{N^2(N-1)}$；
	\end{enumerate}
\end{property}
\begin{proof}
	(1)注意到：
	\begin{gather*}
		\begin{aligned}
			&\operatorname{E}(X)=\sum_{i=0}^{\min\{n,M\}}i\frac{\binom{M}{i}\binom{N-M}{n-i}}{\binom{N}{n}}=\frac{1}{\binom{N}{n}}\sum_{i=1}^{\min\{n,M\}}i\frac{M!}{i!(M-i)!}\binom{N-M}{n-i} \\
			=&M\sum_{i=1}^{\min\{n,M\}}\frac{(M-1)!}{(i-1)!(M-i)!}\binom{N-M}{n-i}\frac{n!(N-n)!}{N!} \\
			=&\frac{nM}{N}\sum_{i=1}^{\min\{n,M\}}\binom{M-1}{i-1}\binom{N-M}{n-i}\frac{(n-1)!(N-n)!}{(N-1)!} \\
			=&\frac{nM}{N}\sum_{i=1}^{\min\{n,M\}}\frac{\binom{M-1}{i-1}\binom{N-M}{n-i}}{\binom{N-1}{n-1}}=\frac{nM}{N}
		\end{aligned} \\
		\begin{aligned}
			&\operatorname{E}(X^2)=\sum_{i=0}^{\min\{n,M\}}i^2\frac{\binom{M}{i}\binom{N-M}{n-i}}{\binom{N}{n}}=\sum_{i=0}^{\min\{n,M\}}[i(i-1)+i]\frac{\binom{M}{i}\binom{N-M}{n-i}}{\binom{N}{n}} \\
			=&\sum_{i=0}^{\min\{n,M\}}i(i-1)\frac{\binom{M}{i}\binom{N-M}{n-i}}{\binom{N}{n}}+\operatorname{E}(X) \\
			=&\sum_{i=2}^{\min\{n,M\}}i(i-1)\frac{M!}{i!(M-i)!}\binom{N-M}{n-i}\frac{n!(N-n)!}{N!}+\frac{nM}{N} \\
			=&\frac{M(M-1)n(n-1)}{N(N-1)}\sum_{i=2}^{\min\{n,M\}}\frac{(M-2)!}{(i-2)!(M-i)!}\binom{N-M}{n-i}\frac{(n-2)!(N-n)!}{(N-2)!}+\frac{nM}{N} \\
			=&\frac{M(M-1)n(n-1)}{N(N-1)}\sum_{i=2}^{\min\{n,M\}}\frac{\binom{M-2}{i-2}\binom{N-M}{n-i}}{\binom{N-2}{n-2}}+\frac{nM}{N}=\frac{M(M-1)n(n-1)}{N(N-1)}+\frac{nM}{N} \\
			=&\frac{M(M-1)n(n-1)+nM(N-1)}{N(N-1)}
		\end{aligned}
	\end{gather*}
	由\cref{prop:Variance}(1)可得：
	\begin{align*}
		&\operatorname{Var}(X)=\operatorname{E}(X^2)-[\operatorname{E}(X)]^2=\frac{M(M-1)n(n-1)+nM(N-1)}{N(N-1)}-\frac{n^2M^2}{N^2} \\
		=&\frac{M(M-1)n(n-1)+nM(N-1)}{N(N-1)}-\frac{n^2M^2}{N^2} \\
		=&\frac{M(M-1)n(n-1)N+nM(N-1)N-n^2M^2(N-1)}{N^2(N-1)} \\
		=&\frac{M^2n^2N-M^2nN-Mn^2N+MnN+nMN^2-nMN-n^2M^2N+n^2M^2}{N^2(N-1)} \\
		=&\frac{-M^2nN-Mn^2N+nMN^2+n^2M^2}{N^2(N-1)}=\frac{nM(-MN-nN+N^2+mn)}{N^2(N-1)} \\
		=&\frac{nM(N-n)(N-M)}{N^2(N-1)}\qedhere
	\end{align*}
\end{proof}

\subsection{Poisson分布族}
\begin{definition}
	若离散型随机变量$X$的分布列为：
	\begin{equation*}
		P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda},\;k\in\mathbb{N}
	\end{equation*}
	其中$\lambda\in\mathbb{R}^{+}$为参数，则称$X$服从\gls{PoissonDistribution}，记作$X\sim\operatorname{Poisson}(\lambda)$。
\end{definition}
\begin{property}
	Poisson分布族具有如下性质：
	\begin{enumerate}
		\item 若$X\sim\operatorname{Poisson}(\lambda)$，则$\operatorname{E}(X)=\lambda,\;\operatorname{Var}(X)=\lambda$；
		\item 若$\lim\limits_{n\to+\infty}np_n=\lambda$，则：
		\begin{equation*}
			\lim_{n\to+\infty}\binom{n}{k}p_n^k(1-p_n)^{n-k}=\frac{\lambda^k}{k!}e^{-\lambda}
		\end{equation*}
	\end{enumerate}
\end{property}
\begin{proof}
	(1)注意到：
	\begin{align*}
		&\operatorname{E}(X)=\sum_{n=1}^{+\infty}n\frac{\lambda^n}{n!}e^{-\lambda}=\lambda e^{-\lambda}\sum_{n=1}^{+\infty}\frac{\lambda^{n-1}}{(n-1)!}=\lambda e^{-\lambda}e^{\lambda}=\lambda \\
		&\operatorname{E}(X^2)=\sum_{n=1}^{+\infty}n^2\frac{\lambda^n}{n!}e^{-\lambda}=\sum_{n=1}^{+\infty}[n(n-1)+n]\frac{\lambda^n}{n!}e^{-\lambda}=\sum_{n=1}^{+\infty}n(n-1)\frac{\lambda^n}{n!}e^{-\lambda}+\operatorname{E}(X) \\
		=&\sum_{n=2}^{+\infty}n(n-1)\frac{\lambda^n}{n!}e^{-\lambda}+\lambda=\lambda^2\sum_{n=2}^{+\infty}\frac{\lambda^{n-2}}{(n-2)!}e^{-\lambda}+\lambda=\lambda^2e^{-\lambda}e^{\lambda}+\lambda=\lambda^2+\lambda
	\end{align*}
	由\cref{prop:Variance}(1)可得：
	\begin{equation*}
		\operatorname{Var}(X)=\operatorname{E}(X^2)-[\operatorname{E}(X)]^2=\lambda^2+\lambda-\lambda^2=\lambda\qedhere
	\end{equation*}
\end{proof}

\subsection{均匀分布族}
\begin{definition}
	若连续型随机变量$X$的概率密度函数为：
	\begin{equation*}
		p(x)=\frac{1}{b-a},\;x\in(a,b)
	\end{equation*}
	其中$a,b\in\mathbb{R}^{}$为参数，则称$X$服从\gls{UniformDistribution}，记作$X\sim\operatorname{U}(a,b)$。
\end{definition}
\begin{property}
	均匀分布族具有如下性质：
	\begin{enumerate}
		\item 若$X\sim\operatorname{U}(a,b)$，则$\operatorname{E}(X)=\dfrac{a+b}{2},\;\operatorname{Var}(X)=\dfrac{(b-a)^2}{12}$；
	\end{enumerate}
\end{property}
\begin{proof}
	(1)由\cref{theo:MeasurableCountableIntegral}可得：
	\begin{gather*}
		\operatorname{E}(X)=\int_{(a,b)}x\frac{1}{b-a}\dif\mu=\frac{1}{b-a}\frac{1}{2}x^2\Big|_a^b=\frac{1}{b-a}\frac{1}{2}(b^2-a^2)=\frac{1}{2}\frac{1}{b-a}(b-a)(b+a)=\frac{a+b}{2} \\
		\begin{aligned}
			\operatorname{E}(X^2)&=\int_{(a,b)}x^2\frac{1}{b-a}\dif\mu=\frac{1}{b-a}\frac{1}{3}x^3\Big|_a^b=\frac{1}{b-a}\frac{1}{3}(b^3-a^3)=\frac{1}{b-a}\frac{1}{3}(b-a)(b^2+ab+a^2) \\
			&=\frac{1}{3}(b^2+ab+a^2)
		\end{aligned}
	\end{gather*}
	根据\cref{prop:Variance}(1)可得：
	\begin{align*}
		\operatorname{Var}(X)&=\operatorname{E}(X^2)-[\operatorname{E}(X)]^2=\frac{1}{3}(b^2+ab+a^2)-\frac{(a+b)^2}{4} \\
		&=\frac{4b^2+4ab+4a^2-3a^2-6ab-3b^2}{12}=\frac{b^2-2ab+a^2}{12}=\frac{(b-a)^2}{12}\qedhere
	\end{align*}
\end{proof}

\subsection{正态分布族}
\subsubsection{一元正态分布}
\begin{definition}
	若连续型随机变量$X$的概率密度函数为：
	\begin{equation*}
		p(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]
	\end{equation*}
	其中$\mu\in\mathbb{R}^{}$和$\sigma^2\in\mathbb{R}^{+}$为参数，则称$X$服从\gls{NormalDistribution}，记作$X\sim\operatorname{N}(\mu,\sigma^2)$。称$\operatorname{N}(0,1)$为\gls{StandardNormalDistribution}。
\end{definition}
\begin{note}
	将标准正态分布的$\alpha$分位点记作$u_{\alpha}$。
\end{note}
\subsubsection{多元正态分布}
\begin{definition}\label{def:MultiNormal}
	$\mathbf{X}$为$n$维随机向量。若存在矩阵$A\in M_{n\times r}(\mathbb{R})$使得$\mathbf{X}=A\mathbf{U}+\boldsymbol{\mu}$，其中$\mathbf{U}=(\mathbf{U}_1,\mathbf{U}_2,\dots,\mathbf{U}_r)^{\top},\;\mathbf{U}_i\sim \operatorname{N}(0,1)$且相互独立，$\boldsymbol{\mu}$为$n$维非随机实向量，则称$\mathbf{X}$服从均值为$\boldsymbol{\mu}$、协方差矩阵为$\Sigma=AA^{\top}$的\gls{MultivariateNormalDistribution}，记为$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma)$，其中$\Sigma\geqslant\mathbf{0}$。若$|\Sigma|=0$，则称此时的分布为\gls{SingularNormalDistribution}。
\end{definition}
\begin{note}
	由\cref{prop:MeasurableIntegral}(5)和\cref{prop:CovMat}(3)(4)(5)可得$\operatorname{E}(\mathbf{X})=\boldsymbol{\mu},\;\operatorname{Cov}(\mathbf{X})=AA^{\top}$，所以上定义成立。
\end{note}
\begin{property}\label{prop:MultiNormal}
	多元正态分布具有如下性质：
	\begin{enumerate}
		\item 设$\mathbf{X}\sim\operatorname{N}_n(\boldsymbol{\mu},\Sigma),\Sigma\geqslant\mathbf{0}$，则当$\Sigma>\mathbf{0}$时，$\mathbf{X}$的概率函数为：
		\begin{align*}
			p(\mathbf{X})&=\frac{1}{(2\pi)^{\frac{n}{2}}(\det\Sigma)^{\frac{1}{2}}}\exp\left[-\frac{1}{2}(\mathbf{X}-\boldsymbol{\mu})^{\top}\Sigma^{-1}(\mathbf{X}-\boldsymbol{\mu})\right] \\
			&=\frac{1}{(2\pi)^{\frac{n}{2}}(\det\Sigma)^{\frac{1}{2}}}\exp\left\{-\frac{1}{2}\operatorname{tr}[(\mathbf{X}-\boldsymbol{\mu})(\mathbf{X}-\boldsymbol{\mu})^{\top}\Sigma^{-1}]\right\}
		\end{align*}
		当$\det\Sigma=0$时，记$\operatorname{rank}(\Sigma)=r$，$\Sigma$的非零特征值为$\seq{\lambda}{r}$，则$\mathbf{X}-\boldsymbol{\mu}$以概率为$1$落在$\mathcal{M}(\Sigma)$中，且在该子空间内有概率函数：
		\begin{equation*}
			p(\mathbf{X})=(2\pi)^{-\frac{r}{2}}\left(\prod\limits_{i=1}^r\lambda_i\right)^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\mathbf{X}-\boldsymbol{\mu})^{\top}\Sigma^-(\mathbf{X}-\boldsymbol{\mu})\right]
		\end{equation*}
		\item 设$\mathbf{X}\sim\operatorname{N}_n(\boldsymbol{\mu},\Sigma),\Sigma\geqslant\mathbf{0}$，$B\in M_{m\times n}(\mathbb{R}),\;c\in\mathbb{R}^{n}$，则$\mathbf{Y}=B\mathbf{X}+c\sim \operatorname{N}(B\boldsymbol{\mu}+c,B\Sigma B^{\top})$；
		\item 设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma),\;\boldsymbol{\mu}=(\seq{\mu}{n})^{\top},\;\Sigma=(\sigma_{ij})$，$i_1<i_2<\cdots<i_k,\;k\leqslant n$，则有$(\mathbf{X}_{i_1},\mathbf{X}_{i_2},\dots,\mathbf{X}_{i_k})^{\top}\sim \operatorname{N}(\boldsymbol{\mu}_0,\Sigma_0)$，其中：
		\begin{equation*}
			\boldsymbol{\mu}_0=
			\begin{pmatrix}
				\mu_{i_1} \\
				\mu_{i_2} \\
				\vdots \\
				\mu_{i_k}
			\end{pmatrix}
			,\quad
			\Sigma_0=
			\begin{pmatrix}
				\sigma_{i_1i_1} & \sigma_{i_1i_2} & \cdots & \sigma_{i_1i_k} \\
				\sigma_{i_2i_1} & \sigma_{i_2i_2} & \cdots & \sigma_{i_2i_k} \\
				\vdots & \vdots & \ddots & \vdots \\
				\sigma_{i_ki_1} & \sigma_{i_ki_2} & \cdots & \sigma_{i_ki_k}
			\end{pmatrix}
		\end{equation*}
		\item 设$\mathbf{X}$是一个$n$维随机向量，则$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma)$当且仅当它的特征函数为：
		\begin{equation*}
			\varphi_\mathbf{X}(t)=\exp\left(it^{\top}\boldsymbol{\mu}-\frac{t^{\top}\Sigma t}{2}\right),\;t\in\mathbb{R}^{n}
		\end{equation*}
		\item 设$\mathbf{X}$是一个$n$维随机向量，则$\mathbf{X}$服从$n$维多元正态分布的充分必要条件为对于任意的$\alpha\in\mathbb{R}^{n}$，$\alpha^{\top}\mathbf{X}$服从正态分布；
		\item 设$\mathbf{X}\sim\operatorname{N}_m(\boldsymbol{\mu}_1,\Sigma_1)$和$\mathbf{Y}\sim\operatorname{N}_n(\boldsymbol{\mu}_2,\Sigma_2)$独立，则：
		\begin{equation*}
			\mathbf{Z}=
			\begin{pmatrix}
				\mathbf{X} \\
				\mathbf{Y}
			\end{pmatrix}
			\sim N_{m+n}\left[
			\begin{pmatrix}
				\boldsymbol{\mu}_{1} \\
				\boldsymbol{\mu}_{2}
			\end{pmatrix},\;
			\begin{pmatrix}
				\Sigma_1 & \mathbf{0} \\
				\mathbf{0} & \Sigma_2
			\end{pmatrix}
			\right]
		\end{equation*}
		\item 设$\mathbf{X}\sim N_n(\boldsymbol{\mu}_1,\Sigma_1)$和$\mathbf{Y}\sim N_n(\boldsymbol{\mu}_2,\Sigma_2)$独立，则$\mathbf{X}+\mathbf{Y}\sim N_n(\boldsymbol{\mu}+\boldsymbol{\nu},\Sigma_\mathbf{X}+\Sigma_\mathbf{Y})$；
		\item 设$\mathbf{X_j}\sim\operatorname{N}_{r_j}(\boldsymbol{\mu_j},\Sigma_{j}),\;j=1,2,\dots,m$，$\mathbf{X_j}$的联合分布为正态分布。若$\mathbf{X_j}$不相关，则$\mathbf{X_j}$相互独立，即对于服从多维正态分布的随机向量而言，若它们的联合分布仍是多维正态分布，则不相关和独立等价；
		\item 设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma)$，$\Sigma>\mathbf{0}$，则$(\mathbf{X}-\boldsymbol{\mu})^{\top}\Sigma^{-1}(\mathbf{X}-\boldsymbol{\mu})\sim\chi_n^2$。
	\end{enumerate}
\end{property}
\begin{proof}
	(1)\textbf{当$\Sigma>\mathbf{0}$时：}因为$\Sigma$是实对称矩阵且$\Sigma>\mathbf{0}$，所以$\Sigma^{\frac{1}{2}},\Sigma^{-\frac{1}{2}}$存在，可将$\mathbf{X}$表示为$\mathbf{X}=\Sigma^{\frac{1}{2}}\mathbf{Y}+\boldsymbol{\mu}$，其中$\mathbf{Y}$是由相互独立的服从标准正态分布的随机变量构成的$n$维随机向量，有分布函数：
	\begin{equation*}
		p(\mathbf{Y})=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{\mathbf{Y}_i^2}{2}\right)=\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(-\frac{1}{2}\mathbf{Y}^{\top}\mathbf{Y}\right)
	\end{equation*}
	由求随机变量函数的分布中的变量变换法可知\info{随机变量函数分布的变量变换法，逆矩阵的行列式，正定矩阵的行列式，矩阵乘积的行列式，逆平方根阵的对称性，矩阵乘积的转置}：
	\begin{align*}
		&p(\mathbf{X})=p[\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu})]|\det\Sigma^{-\frac{1}{2}}| \\
		=&\frac{1}{(2\pi)^{\frac{n}{2}}(\det\Sigma)^\frac{1}{2}}\exp\left\{-\frac{1}{2}[\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu})]^{\top}[\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu})]\right\} \\
		=&\frac{1}{(2\pi)^{\frac{n}{2}}(\det\Sigma)^{\frac{1}{2}}}\exp\left\{-\frac{1}{2}(\mathbf{X}-\boldsymbol{\mu})^{\top}\Sigma^{-1}(\mathbf{X}-\boldsymbol{\mu})\right\}
	\end{align*}
	只需注意到二次型的迹就是自身以及\cref{prop:Trace}(3)就有：
	\begin{equation*}
		(\mathbf{X}-\boldsymbol{\mu})^{\top}\Sigma^{-1}(\mathbf{X}-\boldsymbol{\mu})=\operatorname{tr}[(\mathbf{X}-\boldsymbol{\mu})^{\top}\Sigma^{-1}(\mathbf{X}-\boldsymbol{\mu})]=\operatorname{tr}[(\mathbf{X}-\boldsymbol{\mu})(\mathbf{X}-\boldsymbol{\mu})^{\top}\Sigma^{-1}]
	\end{equation*}\par
	\textbf{当$\det\Sigma=0$时：}\par
	(2)因为$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma)$，所以存在$A\in M_{n\times r}(\mathbb{R}),\;\boldsymbol{\mu}\in\mathbb{R}^{n}$使得：
	\begin{equation*}
		\mathbf{X}=A\mathbf{U}+\boldsymbol{\mu},\;AA^{\top}=\Sigma,\;U\sim \operatorname{N}(\mathbf{0},I)
	\end{equation*}
	于是：
	\begin{equation*}
		\mathbf{Y}=B(A\mathbf{U}+\boldsymbol{\mu})+c=BA\mathbf{U}+B\boldsymbol{\mu}+c
	\end{equation*}
	注意到$BA(BA)^{\top}=BAA^{\top}B^{\top}=B\Sigma B^{\top}$，所以$\mathbf{Y}\sim \operatorname{N}(B\boldsymbol{\mu}+c,B\Sigma B^{\top})$。\par
	(3)取$A=(e_{i_1}^{\top};e_{i_2}^{\top};\cdots;e_{i_k}^{\top})$，其中$e_{i_j}$为单位列向量，只在第$i_j$位取$1$，其余位置上元素为$0$，$j=1,2,\dots,k$，于是有：
	\begin{gather*}
		A\boldsymbol{\mu}=(\mu_{i_i},\mu_{i_2},\dots,\mu_{i_k})^{\top}=\boldsymbol{\mu}_0
		\\
		A\Sigma A^{\top}=
		\begin{pmatrix}
			e_{i_1}^{\top}\Sigma e_{i_1} & e_{i_1}^{\top}\Sigma e_{i_2} & \cdots & e_{i_1}^{\top}\Sigma e_{i_k} \\
			e_{i_2}^{\top}\Sigma e_{i_1} & e_{i_2}^{\top}\Sigma e_{i_2} & \cdots & e_{i_2}^{\top}\Sigma e_{i_k} \\
			\vdots & \vdots & \ddots & \vdots \\
			e_{i_k}^{\top}\Sigma e_{i_1} & e_{i_k}^{\top}\Sigma e_{i_2} & \cdots & e_{i_k}^{\top}\Sigma e_{i_k}^{\top}
		\end{pmatrix}
		=
		\begin{pmatrix}
			\sigma_{i_1i_1} & \sigma_{i_1i_2} & \cdots & \sigma_{i_1i_k} \\
			\sigma_{i_2i_1} & \sigma_{i_2i_2} & \cdots & \sigma_{i_2i_k} \\
			\vdots & \vdots & \ddots & \vdots \\
			\sigma_{i_ki_1} & \sigma_{i_ki_2} & \cdots & \sigma_{i_ki_k}
		\end{pmatrix}
		=\Sigma_0
	\end{gather*}
	由(2)可得$(\mathbf{X}_{i_1},\mathbf{X}_{i_2},\dots,\mathbf{X}_{i_k})^{\top}\sim \operatorname{N}(\boldsymbol{\mu}_0,\Sigma_0)$。\par
	(4)	若$X\sim N_n(\boldsymbol{\mu},\Sigma)$，则存在矩阵$A\in M_{n\times r}(\mathbb{R})$使得$\mathbf{X}=A\mathbf{U}+\boldsymbol{\mu}$，其中$\mathbf{U}=(\mathbf{U}_1,\mathbf{U}_2,\dots,\mathbf{U}_r)^{\top}$，$\mathbf{U}_i\sim \operatorname{N}(0,1)$且互相独立，$\boldsymbol{\mu}$为$n$维非随机实向量，$\Sigma=AA^{\top}$。由\cref{prop:CharacteristicFunction}(5)可得\info{需要证明1维正态分布的特征函数}：
	\begin{equation*}
		\varphi_\mathbf{U}(t)=\prod_{i=1}^n\varphi_{\mathbf{U}_i}(t_i)
		=\prod_{i=1}^ne^{-\frac{t_i^2}{2}}=e^{-\frac{t^{\top}t}{2}},\;t\in\mathbb{R}^{n}
	\end{equation*}
	于是：
	\begin{align*}
		\varphi_\mathbf{X}(t)
		&=\operatorname{E}(e^{it^{\top}\mathbf{X}})
		=\operatorname{E}[e^{it^{\top}(A\mathbf{U}+\boldsymbol{\mu})}]
		=e^{it\boldsymbol{\mu}}\operatorname{E}(e^{it^{\top}A\mathbf{U}}) \\
		&=e^{it^{\top}\boldsymbol{\mu}}\varphi_\mathbf{U}(A^{\top}t)
		=e^{it^{\top}\boldsymbol{\mu}}e^{-\frac{t^{\top}AA^{\top}t}{2}}
		=e^{it^{\top}\boldsymbol{\mu}}e^{-\frac{t^{\top}\Sigma t}{2}}
		=\exp\left(it^{\top}\boldsymbol{\mu}-\frac{t^{\top}\Sigma t}{2}\right)
	\end{align*}
	由\cref{prop:CharacteristicFunction}(6)可知结论成立。\par
	(5)\textbf{必要性：}由(2)直接得到。\par
	\textbf{充分性：}由(4)可知此时$\alpha^{\top}\mathbf{X}$的特征函数为：
	\begin{equation*}
		\varphi_{\alpha^{\top}\mathbf{X}}(t)=\exp\left(it\mu-\frac{1}{2}t^2\sigma^2\right)
	\end{equation*}
	其中$\mu$和$\sigma^2$分别为$\alpha^{\top}\mathbf{X}$的均值与方差。由\info{期望的性质}和\cref{prop:CovMat}(3)可得：
	\begin{gather*}
		\mu=\operatorname{E}(\alpha^{\top}\mathbf{X})=\alpha^{\top}\operatorname{E}(\mathbf{X}),\;
		\sigma^2=\operatorname{Cov}(\alpha^{\top}\mathbf{X})=\alpha^{\top}\operatorname{Cov}(\mathbf{X})\alpha
	\end{gather*}
	于是有：
	\begin{equation*}
		\varphi_{\alpha^{\top}\mathbf{X}}(t)=\exp\left[it\alpha^{\top}\operatorname{E}(\mathbf{X})-\frac{t\alpha^{\top}\operatorname{Cov}(\mathbf{X})\alpha t}{2}\right]
	\end{equation*}
	由$\alpha$的任意性，上式可写作：
	\begin{equation*}
		\varphi_{\mathbf{X}}(\beta)=\exp\left[i\beta^{\top}\operatorname{E}(\mathbf{X})-\frac{\beta^{\top}\operatorname{Cov}(\mathbf{X})\beta}{2}\right]
	\end{equation*}
	由\cref{prop:CharacteristicFunction}(6)和(4)可知$\mathbf{X}$服从多元正态分布。\par
	(6)由\cref{def:MultiNormal}可得。\par
	(7)	因为：
	\begin{equation*}
		\mathbf{X}+\mathbf{Y}=
		\begin{pmatrix}
			I_n & I_n
		\end{pmatrix}
		\begin{pmatrix}
			\mathbf{X} \\
			\mathbf{Y}
		\end{pmatrix}
	\end{equation*}
	由(6)可得：
	\begin{equation*}
		\mathbf{Z}=
		\begin{pmatrix}
			\mathbf{X} \\
			\mathbf{Y}
		\end{pmatrix}
		\sim N_{2n}\left[
		\begin{pmatrix}
			\boldsymbol{\mu}_1 \\
			\boldsymbol{\mu}_2
		\end{pmatrix},\;
		\begin{pmatrix}
			\Sigma_1 & \mathbf{0} \\
			\mathbf{0} & \Sigma_2
		\end{pmatrix}
		\right]
	\end{equation*}
	根据(2)即可得到结论。\par
	(8)设$\mathbf{X}=(\mathbf{X_1},\mathbf{X_2},\dots,\mathbf{X_m})^{\top}\sim\operatorname{N}_n(\boldsymbol{\mu},\Sigma)$。因为$\mathbf{X_j}$之间不相关，于是：
	\begin{equation*}
		\boldsymbol{\mu}=
		\begin{pmatrix}
			\boldsymbol{\mu_1} \\
			\boldsymbol{\mu_2} \\
			\cdots \\
			\boldsymbol{\mu_m}
		\end{pmatrix},\quad
		\Sigma=
		\begin{pmatrix}
			\Sigma_1 & \mathbf{0} & \cdots & \mathbf{0} \\
			\mathbf{0} & \Sigma_2 & \cdots & \mathbf{0} \\
			\vdots & \vdots & \ddots & \vdots \\
			\mathbf{0} & \mathbf{0} & \cdots & \Sigma_m
		\end{pmatrix}
	\end{equation*}
	由(4)可得：
	\begin{align*}
		\varphi_\mathbf{X}(t)
		&=\exp\left(it\boldsymbol{\mu}-\frac{t^{\top}\Sigma t}{2}\right)
		=\exp\left(i\sum_{j=1}^{m}t_j^{\top}\boldsymbol{\mu_j}-\frac{\sum\limits_{j=1}^{m}t_j^{\top}\Sigma_j t_j}{2}\right) \\
		&=\prod_{j=1}^m\exp\left(it_j\boldsymbol{\mu_j}-\frac{t_j^{\top}\Sigma_j t_j}{2}\right)=\prod_{j=1}^m\varphi_\mathbf{X_j}(t_j)
	\end{align*}
	由\cref{prop:CharacteristicFunction}(5)可知$\mathbf{X_j}$相互独立，$j=1,2,\dots,m$。\par
	(9)因为$\Sigma>\mathbf{0}$，所以存在$\Sigma^{-\frac{1}{2}}$。由(2)可得：
	\begin{equation*}
		\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu})\sim N_n(\mathbf{0},I_n)
	\end{equation*}
	于是根据\cref{prop:ReverseSquareRootMat}(1)和(3)可得：
	\begin{align*}
		(\mathbf{X}-\boldsymbol{\mu})^{\top}\Sigma^{-1}(\mathbf{X}-\boldsymbol{\mu})&=(\mathbf{X}-\boldsymbol{\mu})^{\top}(\Sigma^{-\frac{1}{2}}\Sigma^{-\frac{1}{2}})(\mathbf{X}-\boldsymbol{\mu}) =(\mathbf{X}-\boldsymbol{\mu})^{\top}(\Sigma^{-\frac{1}{2}})^{\top}\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu}) \\
		&=[\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu})]^{\top}\Sigma^{-\frac{1}{2}}(\mathbf{X}-\boldsymbol{\mu})\sim\chi_n^2\qedhere
	\end{align*}
\end{proof}
\subsubsection{正态随机向量的二次型}
\begin{theorem}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma),\;\Sigma>\mathbf{0}$，$A$为$n$阶非随机实对称阵，则：
	\begin{equation*}
		\operatorname{E}(\mathbf{X}^{\top}A\mathbf{X})=\boldsymbol{\mu}^{\top}A\boldsymbol{\mu}+\operatorname{tr}(A\Sigma),\;
		\operatorname{Var}(\mathbf{X}^{\top}A\mathbf{X})=2\operatorname{tr}[(A\Sigma)^2]+4\boldsymbol{\mu}^{\top}A\Sigma A\boldsymbol{\mu}
	\end{equation*}
\end{theorem}
\begin{proof}
	期望可直接由\cref{theo:ERVQuadraticForm}得到。记$\mathbf{Y}=\Sigma^{-\frac{1}{2}}\mathbf{X}$，由\cref{prop:MultiNormal}(2)可知$\mathbf{Y}\sim N_n(\Sigma^{-\frac{1}{2}}\boldsymbol{\mu},I_n)$，根据\cref{prop:MultiNormal}(8)，$\mathbf{Y}$的各分量相互独立。注意到$\mathbf{Y}$的各分量的三阶中心矩和四阶中心矩分别为$0$和$3$，由\cref{theo:VRVQuadraticForm}、\cref{prop:ReverseSquareRootMat}(3)和\cref{prop:Trace}(3)可得：
	\begin{align*}
		\operatorname{Var}(\mathbf{X}^{\top}A\mathbf{X})
		&=\operatorname{Var}(\mathbf{Y}^{\top}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\mathbf{Y}) \\
		&=3\sum_{i=1}^{n}(\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})_{ii}^2+2\operatorname{tr}(\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})-3\sum_{i=1}^{n}(\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})_{ii}^2 \\
		&\quad+4\boldsymbol{\mu}^{\top}\Sigma^{-\frac{1}{2}}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\boldsymbol{\mu} \\
		&=2\operatorname{tr}(\Sigma^{\frac{1}{2}}A\Sigma A\Sigma^{\frac{1}{2}})+4\boldsymbol{\mu}^{\top}A\Sigma A\boldsymbol{\mu} \\
		&=2\operatorname{tr}(A\Sigma A\Sigma^{\frac{1}{2}}\Sigma^{\frac{1}{2}})+4\boldsymbol{\mu}^{\top}A\Sigma A\boldsymbol{\mu} \\
		&=2\operatorname{tr}[(A\Sigma)^2]+4\boldsymbol{\mu}^{\top}A\Sigma A\boldsymbol{\mu}\qedhere
	\end{align*}
\end{proof}
\begin{theorem}\label{theo:XAXChi2}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma),\;\Sigma>\mathbf{0}$，$A\in M_{n}(K)$是一个非随机实对称矩阵，则$\mathbf{X}^{\top}A\mathbf{X}\sim\chi_{r,\boldsymbol{\mu}^{\top}A\boldsymbol{\mu}}^2$的充分必要条件为$A\Sigma A=A$且$\operatorname{rank}(A)=r$。
\end{theorem}
\begin{proof}
	先证明$\Sigma=I_n$时的情况。\par
	\textbf{(1)充分性：}因为$A$是一个幂等阵，由\cref{prop:IdempotentMat}(1)可知$A$的特征值只能为$0$或$1$。根据\cref{prop:HermitianMatEigen}(3)可知存在正交矩阵$Q$使得：
	\begin{equation*}
		A=Q^{-1}
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}Q
	\end{equation*}
	令$\mathbf{Y}=Q\mathbf{X}$，由\cref{prop:MultiNormal}(2)可知$\mathbf{Y}\sim N_n(Q\boldsymbol{\mu},I_n)$。对$\mathbf{Y}$和$Q$进行分块：
	\begin{equation*}
		\mathbf{Y}=
		\begin{pmatrix}
			\mathbf{Y_1} \\
			\mathbf{Y_2}
		\end{pmatrix},\;
		Q=
		\begin{pmatrix}
			Q_1 \\
			Q_2
		\end{pmatrix}
	\end{equation*}
	其中$\mathbf{Y_1}$为$r$维随机向量，$Q_1$为$r\times n$矩阵，所以：
	\begin{align*}
		\mathbf{X}^{\top}A\mathbf{X}&=\mathbf{X}^{\top}Q^{-1}
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}Q\mathbf{X}=\mathbf{X}^{\top}Q^{\top}
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}Q\mathbf{X} \\
		&=\mathbf{Y}^{\top}
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}\mathbf{Y}
		=
		\begin{pmatrix}
			\mathbf{Y_1}^{\top} & \mathbf{Y_2}^{\top}
		\end{pmatrix}
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		\begin{pmatrix}
			\mathbf{Y_1} \\
			\mathbf{Y_2}
		\end{pmatrix}
		=\mathbf{Y_1}^{\top}\mathbf{Y_1}\sim\chi_{r,\lambda}^2
	\end{align*}
	其中：
	\begin{equation*}
		\lambda=(Q_1\boldsymbol{\mu})^{\top}Q_1\boldsymbol{\mu}=\boldsymbol{\mu}^{\top}Q_1^{\top}Q_1\boldsymbol{\mu}=\boldsymbol{\mu}^{\top}A\boldsymbol{\mu}
	\end{equation*}
	这是因为\cref{prop:MultiNormal}(3)和：
	\begin{equation*}
		A=Q^{-1}
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}Q
		=Q^{\top}
		\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}Q
		=
		\begin{pmatrix}
			Q_1^{\top} & Q_2^{\top}
		\end{pmatrix}
			\begin{pmatrix}
			I_r & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		\begin{pmatrix}
			Q_1 \\
			Q_2
		\end{pmatrix}
		=Q_1^{\top}Q_1
	\end{equation*}\par
	\textbf{(2)必要性：}设$\operatorname{rank}(A)=t$。因为$A$是实对称矩阵，由\cref{prop:HermitianMatEigen}(3)可知存在正交阵$Q$使得：
	\begin{equation*}
		A=Q^{-1}
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		Q
	\end{equation*}
	其中$\varLambda=\operatorname{diag}\{\seq{\lambda}{t}\}$，$\seq{\lambda}{t}$是$A$的非零特征值。若能证得$\lambda_i=1,\;i=1,2,\dots,t$且$t=r$，则$A$是一个幂等阵且$\operatorname{rank}(A)=r$。注意到：
	\begin{equation*}
		\mathbf{X}^{\top}A\mathbf{X}=\mathbf{X}^{\top}Q^{\top}
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		Q\mathbf{X}
	\end{equation*}
	令$\mathbf{Y}=Q\mathbf{X}$，由\cref{prop:MultiNormal}(2)可知$\mathbf{Y}=(\seq{\mathbf{Y}}{n})\sim N_n(Q\boldsymbol{\mu},I_n)$，根据\cref{prop:MultiNormal}(8)可得$\mathbf{Y}_j$之间彼此独立。令$c=Q\boldsymbol{\mu}=(\seq{c}{n})^{\top}$，由\cref{prop:MultiNormal}(3)可知$\mathbf{Y}_j\sim \operatorname{N}(c_j,1)$。而：
	\begin{equation*}
		\mathbf{X}^{\top}A\mathbf{X}=\mathbf{Y}^{\top}
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}\mathbf{Y}=\sum_{j=1}^{t}\lambda_j\mathbf{Y}_j^2
	\end{equation*}
	由\cref{prop:CharacteristicFunction}(4)和\cref{prop:Chi2Distribution}(3)可知：
	\begin{align*}
		\varphi_{\mathbf{X}^{\top}A\mathbf{X}}(t)
		&=\prod_{j=1}^{t}\varphi_{\lambda_j\mathbf{Y}_j^2}(t)=\prod_{j=1}^t(1-2it)^{-\frac{1}{2}}\exp\left\{\frac{i\lambda_jtc_j^2}{1-2i\lambda_jt}\right\} \\
		&=(1-2it)^{-\frac{t}{2}}\prod_{j=1}^t\exp\left\{\frac{i\lambda_jtc_j^2}{1-2i\lambda_jt}\right\}
	\end{align*}
	因为$\mathbf{X}^{\top}A\mathbf{X}\sim\chi_{r,\boldsymbol{\mu}^{\top}A\boldsymbol{\mu}}^2$，所以：
	\begin{equation*}
		\varphi_{\mathbf{X}^{\top}A\mathbf{X}}(t)=(1-2it)^{-\frac{r}{2}}\exp\left\{\frac{it\boldsymbol{\mu}^{\top}A\boldsymbol{\mu}}{1-2it}\right\}
	\end{equation*}
	由\cref{prop:CharacteristicFunction}(6)可知：
	\begin{equation*}
		(1-2it)^{-\frac{t}{2}}\prod_{j=1}^t\exp\left\{\frac{i\lambda_jtc_j^2}{1-2i\lambda_jt}\right\}=(1-2it)^{-\frac{r}{2}}\exp\left\{\frac{it\boldsymbol{\mu}^{\top}A\boldsymbol{\mu}}{1-2it}\right\}
	\end{equation*}
	所以$t=r$，同时有：
	\begin{equation*}
		\sum_{j=1}^{t}\frac{i\lambda_jtc_j^2}{1-2i\lambda_jt}=\frac{it\boldsymbol{\mu}^{\top}A\boldsymbol{\mu}}{1-2it}
	\end{equation*}
	即：
	\begin{gather*}
		\sum_{j=1}^{t}\frac{i\lambda_jtc_j^2}{1-2i\lambda_jt}=\frac{1}{1-2it}it\boldsymbol{\mu}^{\top}Q^{\top}
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		Q\boldsymbol{\mu} \\
		\sum_{j=1}^{t}\frac{\lambda_jc_j^2}{1-2i\lambda_jt}=\frac{1}{1-2it}c^{\top}
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		c \\
		\sum_{j=1}^{t}\frac{\lambda_jc_j^2}{1-2i\lambda_jt}=\frac{1}{1-2it}\sum_{j=1}^{t}\lambda_jc_j^2 \\
		\frac{\lambda_jc_j^2}{1-2i\lambda_jt}=\frac{\lambda_jc_j^2}{1-2it},\;j=1,2,\dots,t
	\end{gather*}
	所以$\lambda_j=1$。\par
	当$\Sigma$为一般正定阵时，因为$\Sigma>\mathbf{0}$，所以存在$\Sigma^{-\frac{1}{2}}$。考虑随机向量$\mathbf{Y}=\Sigma^{-\frac{1}{2}}\mathbf{X}$，由\cref{prop:MultiNormal}(2)可知$\mathbf{Y}\sim N_n(\Sigma^{-\frac{1}{2}}\boldsymbol{\mu},I_n)$。注意到：
	\begin{equation*}
		\mathbf{X}^{\top}A\mathbf{X}=\mathbf{X}^{\top}\Sigma^{-\frac{1}{2}}\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\mathbf{X}
	\end{equation*}
	由\cref{prop:ReverseSquareRootMat}(3)可得：
	\begin{equation*}
		\mathbf{X}^{\top}A\mathbf{X}=\mathbf{X}^{\top}(\Sigma^{-\frac{1}{2}})^{\top}\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\mathbf{X}=\mathbf{Y}\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\mathbf{Y}
	\end{equation*}
	由\cref{theo:XAXChi2}可得$\mathbf{Y}\Sigma
	^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\mathbf{Y}\sim\chi_{r,\boldsymbol{\mu}^{\top}A\boldsymbol{\mu}}^2$的充分必要条件为$\Sigma
	^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}$是一个对称阵且：
	\begin{gather*}
		(\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})^2=\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}},\;
		\operatorname{rank}(\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})=r,\;
		(\Sigma^{-\frac{1}{2}}\boldsymbol{\mu})^{\top}\Sigma^{\frac{1}{2}} A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\boldsymbol{\mu}=\boldsymbol{\mu}^{\top}A\boldsymbol{\mu}
	\end{gather*}
	第三式显然成立。因为$\Sigma>\mathbf{0}$，所以$\operatorname{rank}(\Sigma
	^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})=\operatorname{rank}(A)$。注意到：
	\begin{align*}
		(\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}})^2=\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}&\Leftrightarrow
		\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}=\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}} \\
		&\Leftrightarrow
		\Sigma^{\frac{1}{2}}A\Sigma A\Sigma^{\frac{1}{2}}=\Sigma
		^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Leftrightarrow
		A\Sigma A=A\qedhere
	\end{align*}
\end{proof}
\begin{theorem}\label{theo:BXXAXIndependent}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma),\;\Sigma>\mathbf{0}$，$A\in M_{n}(\mathbb{R}^{})$是一个对称矩阵，$B\in M_{m\times n}(\mathbb{R}^{})$。若$B\Sigma A=\mathbf{0}$，则$B\mathbf{X}$与$\mathbf{X}^{\top}A\mathbf{X}$相互独立。
\end{theorem}
\begin{proof}
	先证明$\Sigma=I_n$时的情况。\par
	因为$A$是一个实对称矩阵，由\cref{prop:HermitianMatEigen}(3)可知存在正交矩阵$Q$使得：
	\begin{equation*}
		Q^{\top}AQ=
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
	\end{equation*}
	其中$\varLambda=\operatorname{diag}(\seq{\lambda}{r}),\;\lambda_i\ne0,\;i=1,2,\dots,r,\;\operatorname{rank}(A)=r$。因为$BA=\mathbf{0}$，所以有$BQQ^{\top}AQ=BAQ=\mathbf{0}$，于是：
	\begin{equation*}
		BQ
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		=\mathbf{0}
	\end{equation*}
	设：
	\begin{equation*}
		C=BQ=
		\begin{pmatrix}
			C_{11} & C_{12} \\
			C_{21} & C_{22}
		\end{pmatrix}
	\end{equation*}
	则：
	\begin{equation*}
		BQ
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		=
		\begin{pmatrix}
			C_{11}\varLambda & \mathbf{0} \\
			C_{21}\varLambda & \mathbf{0}
		\end{pmatrix}
		=\begin{pmatrix}
			\mathbf{0} & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
	\end{equation*}
	于是有$C_{11}=\mathbf{0},\;C_{21}=\mathbf{0}$。对$C$和$Q$做对应分块：
	\begin{equation*}
		C=BQ=
		\begin{pmatrix}
			\mathbf{0} & C_1
		\end{pmatrix},\;
		Q=
		\begin{pmatrix}
			Q_1 & Q_2
		\end{pmatrix}
	\end{equation*}
	于是：
	\begin{equation*}
		B=CQ^{\top}=
		\begin{pmatrix}
			\mathbf{0} & C_1
		\end{pmatrix}
		\begin{pmatrix}
			Q_1^{\top} \\
			Q_2^{\top}
		\end{pmatrix}
		=C_1Q_2^{\top}
	\end{equation*}
	而：
	\begin{equation*}
		A=Q
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		Q^{\top}
		=
		\begin{pmatrix}
			Q_1 & Q_2
		\end{pmatrix}
		\begin{pmatrix}
			\varLambda & \mathbf{0} \\
			\mathbf{0} & \mathbf{0}
		\end{pmatrix}
		\begin{pmatrix}
			Q_1^{\top} \\
			Q_2^{\top}
		\end{pmatrix}
		=Q_1\varLambda Q_1^{\top}
	\end{equation*}
	记$\mathbf{Y}=Q^{\top}\mathbf{X}$，由\cref{prop:MultiNormal}(2)可得：
	\begin{equation*}
		\mathbf{Y}=
		\begin{pmatrix}
			\mathbf{Y_1} \\
			\mathbf{Y_2}
		\end{pmatrix}
		=
		\begin{pmatrix}
			Q_1^{\top}\mathbf{X} \\
			Q_2^{\top}\mathbf{X}
		\end{pmatrix}
		\sim N_n(Q^{\top}\boldsymbol{\mu},\sigma^2I_n)
	\end{equation*}
	由\cref{prop:MultiNormal}(8)可知$\mathbf{Y_1}$与$\mathbf{Y_2}$独立。因为：
	\begin{gather*}
		B\mathbf{X}=C_1Q_2^{\top}\mathbf{X}=C_1\mathbf{Y_2} \\
		\mathbf{X}^{\top}A\mathbf{X}=\mathbf{X}^{\top}Q_1\varLambda Q_1^{\top}\mathbf{X}=\mathbf{Y_1}^{\top}\varLambda\mathbf{Y_1}
	\end{gather*}
	所以$B\mathbf{X}$与$\mathbf{X}^{\top}A\mathbf{X}$独立，结论在$\Sigma=I_n$时成立。\par
	当$\Sigma$为一般正定阵时，存在$\Sigma^{-\frac{1}{2}}$。由\cref{prop:ReverseSquareRootMat}(3)和\cref{prop:MultiNormal}(2)可得此时有：
	\begin{gather*}
		\Sigma^{-\frac{1}{2}}\mathbf{X}\sim\operatorname{N}_n(\Sigma^{-\frac{1}{2}}\boldsymbol{\mu},I_n),\quad B\mathbf{X}=B\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\mathbf{X} \\
		\mathbf{X}^{\top}A\mathbf{X}=\mathbf{X}^{\top}\Sigma^{-\frac{1}{2}}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\mathbf{X}=(\Sigma^{-\frac{1}{2}}\mathbf{X})^{\top}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\mathbf{X}
	\end{gather*}
	于是当：
	\begin{equation*}
		B\Sigma^{\frac{1}{2}}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}=B\Sigma A\Sigma^{\frac{1}{2}}=\mathbf{0}
	\end{equation*}
	时，$B\mathbf{X}$与$\mathbf{X}^{\top}A\mathbf{X}$相互独立，上式等式两边同时右乘$\Sigma^{-\frac{1}{2}}$即可得到其等价于$B\Sigma A=\mathbf{0}$。
\end{proof}
\begin{theorem}\label{theo:XAXXBXIndependent}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu},\Sigma)$，$A,B$为$n$阶实对称阵。若$A\Sigma B=\mathbf{0}$，则$\mathbf{X}^{\top}A\mathbf{X}$与$\mathbf{X}^{\top}B\mathbf{X}$独立。
\end{theorem}
\begin{proof}
	先证明$\Sigma=I_n$时的情况。\par
	因为$AB=\mathbf{0}$且$A,B$都是对称阵，所以$BA=B^{\top}A^{\top}=(AB)^{\top}=\mathbf{0}$，即$AB=BA$，所以由\cref{prop:HermitianMatEigen}(3)可知存在正交矩阵$Q$可使得$A,B$同时对角化，即：
	\begin{equation*}
		Q^{\top}AQ=\varLambda_1=\operatorname{diag}\{\seq{\lambda^{(1)}}{n}\},\quad Q^{\top}BQ=\varLambda_2=\operatorname{diag}\{\seq{\lambda^{(2)}}{n}\}
	\end{equation*}
	因为$AB=\mathbf{0}$，所以：
	\begin{equation*}
		Q\varLambda_1Q^{\top}Q\varLambda_2Q^{\top}=Q\varLambda_1\varLambda_2Q^{\top}=\mathbf{0}
	\end{equation*}
	等式两边先同时左乘$Q^{\top}$再同时右乘$Q$即可得到$\varLambda_1\varLambda_2=\mathbf{0}$，即$\lambda_i^{(1)}$和$\lambda_i^{(2)}$中至少有一个为$0,\;i=1,2,\dots,n$。令$\mathbf{Y}=Q^{\top}\mathbf{X}$，由\cref{prop:MultiNormal}(2)可得$\mathbf{Y}\sim\operatorname{N}_n(Q^{\top}\boldsymbol{\mu},I_n)$，所以$\mathbf{Y}$的各分量相互独立。因为：
	\begin{equation*}
		\mathbf{X}^{\top}A\mathbf{X}=\mathbf{X}^{\top}Q\varLambda_1Q^{\top}\mathbf{X}=\mathbf{Y}^{\top}\varLambda_1\mathbf{Y},\quad\mathbf{X}^{\top}B\mathbf{X}=\mathbf{X}^{\top}Q\varLambda_2Q^{\top}\mathbf{X}=\mathbf{Y}^{\top}\varLambda_2\mathbf{Y}
	\end{equation*}
	二者依赖的$\mathbf{Y}$的分量不同，所以$\mathbf{X}^{\top}A\mathbf{X}$与$\mathbf{X}^{\top}B\mathbf{X}$独立。\par
	当$\Sigma$为一般正定矩阵时，存在$\Sigma^{-\frac{1}{2}}$。由\cref{prop:ReverseSquareRootMat}(3)和\cref{prop:MultiNormal}(2)可得此时有：
	\begin{gather*}
		\Sigma^{-\frac{1}{2}}\mathbf{X}\sim\operatorname{N}_n(\Sigma^{-\frac{1}{2}}\boldsymbol{\mu},I_n) \\
		\mathbf{X}^{\top}A\mathbf{X}=\mathbf{X}^{\top}\Sigma^{-\frac{1}{2}}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\mathbf{X}=(\Sigma^{-\frac{1}{2}}\mathbf{X})^{\top}\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\mathbf{X} \\
		\mathbf{X}^{\top}B\mathbf{X}=\mathbf{X}^{\top}\Sigma^{-\frac{1}{2}}\Sigma^{\frac{1}{2}}B\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\mathbf{X}=(\Sigma^{-\frac{1}{2}}\mathbf{X})^{\top}\Sigma^{\frac{1}{2}}B\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}\mathbf{X}
	\end{gather*}
	于是当：
	\begin{equation*}
		\Sigma^{\frac{1}{2}}A\Sigma^{\frac{1}{2}}\Sigma^{\frac{1}{2}}B\Sigma^{\frac{1}{2}}=\Sigma^{\frac{1}{2}}A\Sigma B\Sigma^{\frac{1}{2}}=\mathbf{0}
	\end{equation*}
	时，$\mathbf{X}^{\top}A\mathbf{X}$与$\mathbf{X}^{\top}B\mathbf{X}$相互独立，上式等式两边同时左乘$\Sigma^{-\frac{1}{2}}$再右乘$\Sigma^{-\frac{1}{2}}$即可得到其等价于$B\Sigma A=\mathbf{0}$。
\end{proof}

\subsubsection{矩阵正态分布}
\begin{definition}\label{def:MatNormal1}
	若$m\times n$随机矩阵$\mathbf{X}$满足以下概率密度函数：
	\begin{equation*}
		p(\mathbf{X})=\frac{1}{(2\pi)^{\frac{mn}{2}}(\det U)^{\frac{n}{2}}(\det V)^{\frac{m}{2}}}e^{-\frac{1}{2}\operatorname{tr}[V^{-1}(X-M)^{\top}U^{-1}(X-M)]}
	\end{equation*}
	其中，$M\in M_{m\times n}(\mathbb{R}),\;U\in M_{m}(\mathbb{R}),\;V\in M_{n}(\mathbb{R})$，$U,V>0$。此时称$\mathbf{X}$服从矩阵正态分布，记作$\mathbf{X}\sim MN(M,U,V)$。
\end{definition}
\begin{definition}\label{def:MatNormal2}
	若随机矩阵$\mathbf{X}$满足$\operatorname{vec}(\mathbf{X})\sim N(\operatorname{vec}(M),V\otimes U)$，其中，$M\in M_{m\times n}(\mathbb{R}),\;U\in M_{m}(\mathbb{R}),\;V\in M_{n}(\mathbb{R})$，$U,V\geqslant0$。此时称$\mathbf{X}$服从矩阵正态分布，记作$\mathbf{X}\sim MN(M,U,V)$。
\end{definition}
\begin{theorem}
	设$\mathbf{X}$是一个$m\times n$随机矩阵，其行协方差矩阵$U$和列协方差矩阵$V$都是正定矩阵，则$\mathbf{X}$满足\cref{def:MatNormal1}的充分必要条件为满足\cref{def:MatNormal2}。
\end{theorem}
\begin{proof}
	由\cref{prop:Trace}(3)、\cref{prop:VecOperator}(1)(2)、\cref{prop:Kronecker}(2)(3)可得：
	\begin{align*}
		\operatorname{tr}[V^{-1}(\mathbf{X}-M)^{\top}U^{-1}(\mathbf{X}-M)]
		&=\operatorname{tr}[(\mathbf{X}-M)^{\top}U^{-1}(\mathbf{X}-M)V^{-1}] \\
		&=\operatorname{vec}(\mathbf{X}-M)^{\top}\operatorname{vec}[U^{-1}(\mathbf{X}-M)V^{-1}] \\
		&=\operatorname{vec}(\mathbf{X}-M)^{\top}[(V^{-1})^{\top}\otimes U^{-1}]\operatorname{vec}(\mathbf{X}-M) \\
		&=\operatorname{vec}(\mathbf{X}-M)^{\top}[(V^{\top})^{-1}\otimes U^{-1}]\operatorname{vec}(\mathbf{X}-M) \\
		&=\operatorname{vec}(\mathbf{X}-M)^{\top}(V^{-1}\otimes U^{-1})\operatorname{vec}(\mathbf{X}-M) \\
		&=[\operatorname{vec}(\mathbf{X})-\operatorname{vec}(M)]^{\top}(V\otimes U)^{-1}[\operatorname{vec}(\mathbf{X})-\operatorname{vec}(M)]
	\end{align*}
	因为$\det(V\otimes U)=(\det V)^m(\det U)^n$，所以$(\det U)^{\frac{n}{2}}(\det V)^{\frac{m}{2}}$可化作$[\det(V\otimes U)]^{\frac{1}{2}}$。\info{需要补充证明，但这里涉及到了Jordan标准形，学完再来补。}
\end{proof}
\begin{corollary}
	如果正态随机矩阵$\mathbf{X}\sim MN(M,U,V)$中的每个元素都服从标准正态分布，则$M=\mathbf{0},\;V\otimes U=I_{mn}$。
\end{corollary}
由此我们看到，$M$就是正态随机矩阵$X$的均值矩阵，仍然不明确的是$U,V$到底是什么，只能说$V\otimes U$对应着$X$被向量化后的协方差矩阵，那就先来研究一下$\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{kl})$到底对应着$V\otimes U$中的哪个元素。联想正态随机向量中两个元素的协方差在协方差矩阵中的位置，我们需要找到$\mathbf{X}_{ij}$和$\mathbf{X}_{kl}$在$\operatorname{vec}(\mathbf{X})$中的索引，注意到向量化算子$\operatorname{vec}$是按列拉直，那么$\mathbf{X}_{ij}$和$\mathbf{X}_{kl}$分别在$\operatorname{vec}(\mathbf{X})$的第$(j-1)m+i$位和第$(l-1)m+k$位，于是有：
\begin{equation*}
	\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{kl})=(V\otimes U)_{(j-1)s+i,(l-1)s+k}=V_{jl}U_{ik}
\end{equation*}\par
如果$U$是一个对角阵，那么$i\ne k$时有$U_{ik}=0$，就会导致：
\begin{equation*}
	\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{kl})=V_{jl}U_{ik}=0,\;i\ne k
\end{equation*}
这表明此时只要$X$中的元素处于不同行，它们就不相关。\par
如果$V$是一个对角阵，那么$j\ne l$时有$V_{jl}=0$，就会导致：
\begin{equation*}
	\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{kl})=V_{jl}U_{ik}=0,\;j\ne l
\end{equation*}
这表明此时只要$X$中的元素处于不同列，它们就不相关。\par
对于元素$\mathbf{X}_{ij}$，有：
\begin{equation*}
	\operatorname{Var}(\mathbf{X}_{ij})=\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{ij})=V_{jj}U_{ii}
\end{equation*}
这表明此时协方差由$V_{jj}U_{ii}$控制。\par
对于同一行的元素，有：
\begin{equation*}
	\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{il})=V_{jl}U_{ii}
\end{equation*}
这表明此时协方差由$V_{jl}$控制。\par
对于同一列的元素，有：
\begin{equation*}
	\operatorname{Cov}(\mathbf{X}_{ij},\mathbf{X}_{kj})=V_{jj}U_{ik}
\end{equation*}
这表明此时协方差由$U_{ik}$控制。
\begin{definition}\label{def:MatNormal3}
	$\mathbf{X}$为$m\times n$随机矩阵。若存在矩阵$A\in M_{q\times n}(\mathbb{R}),\;B\in M_{m\times p}(K)$使得$\mathbf{X}=B\mathbf{Y}A^{\top}+M$，其中$\mathbf{Y}$是一个$p\times q$随机矩阵，$\mathbf{Y}_{ij}\sim N(0,1)$且互相独立，$i=1,2,\dots,p,\;j=1,2,\dots,q$，$M\in M_{p\times q}(\mathbb{R})$，则称$\mathbf{X}$服从矩阵正态分布，记作$X\sim MN(M,U,V)$。其中，$U=BB^{\top},\;V=AA^{\top}$。
\end{definition}
\begin{theorem}
	$\mathbf{X}$是一个$m\times n$随机矩阵，则$\mathbf{X}$满足\cref{def:MatNormal2}的充分必要条件是满足\cref{def:MatNormal3}。
\end{theorem}
\begin{proof}
	\textbf{(1)必要性：}设$\mathbf{X}$满足\cref{def:MatNormal3}，由$\mathbf{Y}$的定义可知：
	\begin{equation*}
		\operatorname{vec}(\mathbf{Y})\sim N_{pq}(\mathbf{0},I_{pq})
	\end{equation*}
	由\cref{prop:VecOperator}(1)(3)可得：
	\begin{equation*}
		\operatorname{vec}(\mathbf{X})=\operatorname{vec}(B\mathbf{Y}A^{\top}+M)=\operatorname{vec}(B\mathbf{Y}A^{\top})+\operatorname{vec}(M)=(A\otimes B)\operatorname{vec}(\mathbf{Y})+\operatorname{M}
	\end{equation*}
	由\cref{prop:CovMat}(3)和\cref{prop:Kronecker}(4)(2)可得：
	\begin{gather*}
		\operatorname{E}[\operatorname{vec}(\mathbf{X})] 
		= \operatorname{E}[(A \otimes B)\operatorname{vec}(\mathbf{Y}) + \operatorname{M}] 
		= (A \otimes B)\operatorname{E}[\operatorname{vec}(\mathbf{Y})] + \operatorname{M} 
		= \operatorname{M}, \\[1ex]
		\begin{aligned}
			\operatorname{Cov}[\operatorname{vec}(\mathbf{X})] 
			&= \operatorname{Cov}[(A \otimes B)\operatorname{vec}(\mathbf{Y})] \\
			&= (A \otimes B)\operatorname{Cov}[\operatorname{vec}(\mathbf{Y})](A \otimes B)^{\top} \\
			&= (A \otimes B)I_{pq}(A^{\top} \otimes B^{\top}) \\
			&= AA^{\top} \otimes BB^{\top}.
		\end{aligned}
	\end{gather*}
	因为$\operatorname{vec}(\mathbf{X})=(A\otimes B)\operatorname{vec}(\mathbf{Y})+\operatorname{M}$，而$\operatorname{vec}(\mathbf{Y})\sim N_{pq}(\mathbf{0},I_{pq})$，由\cref{prop:MultiNormal}(2)可知：
	\begin{equation*}
		\operatorname{vec}(\mathbf{X})\sim N(\operatorname{vec}(M),AA^{\top}\otimes BB^{\top})
	\end{equation*}
	令$V=AA^{\top},\;U=BB^{\top}$，则有$\operatorname{vec}(\mathbf{X})\sim N(\operatorname{vec}(M),V\otimes U)$，即$\mathbf{X}$满足\cref{def:MatNormal2}。\par
	\textbf{(2)充分性：}设$\mathbf{X}$满足\cref{def:MatNormal2}，因为$U,V\geqslant0$，所以存在$U^{\frac{1}{2}}$和$V^{\frac{1}{2}}$，令$B=U^{\frac{1}{2}},A=V^{\frac{1}{2}}$，于是$\operatorname{vec}(\mathbf{X})\sim N(\operatorname{vec}(M),V\otimes U)$可写作$\operatorname{vec}(\mathbf{X})\sim N(\operatorname{vec}(M),AA^{\top}\otimes BB^{\top})$。设$\mathbf{Y}$是一个随机矩阵，其中的每一个元素都服从标准正态分布且互相独立，则$\operatorname{vec}(\mathbf{X})=(A\otimes B)\operatorname{vec}(\mathbf{Y})+\operatorname{M}$。由\cref{prop:VecOperator}(1)(3)可知：
	\begin{gather*}
		\operatorname{vec}(\mathbf{X})=\operatorname{vec}(B\mathbf{Y}A^{\top}+M)
	\end{gather*}
	于是$\mathbf{X}=B\mathbf{Y}A^{\top}+M$，即$\mathbf{X}$满足\cref{def:MatNormal3}。
\end{proof}
\begin{theorem}\label{theo:MatNormalLinearTransform}
	设$\mathbf{X}$为$m\times n$随机矩阵且服从矩阵正态分布$MN(M,U,V)$，$P\in M_{s\times m}(R),\;Q\in M_{n\times t}(R)$，则$P\mathbf{X}Q^{\top}\sim$
\end{theorem}
\begin{proof}
	由\cref{def:MatNormal3}可知$\mathbf{X}=B\mathbf{Y}A^{\top}+M$，于是：
	\begin{equation*}
		P\mathbf{X}Q^{\top}=PB\mathbf{Y}A^{\top}Q^{\top}+PMQ^{\top}
	\end{equation*}
	此时$PBB^{\top}P^{\top}=PUP^{\top},\;QAA^{\top}Q^{\top}=QVQ^{\top}$，由\cref{def:MatNormal3}即可得到结论。
\end{proof}

\subsection{$\chi^2$分布}
\begin{definition}
	设$\mathbf{X}\sim N_n(\boldsymbol{\mu}, I_n)$，则随机变量$Y=X^{\top}X$的分布称为自由度为$n$、非中心参数为$\lambda=\boldsymbol{\mu}^{\top}\boldsymbol{\mu}$的$\chi^2$分布，记为$\mathbf{Y}\sim \chi^2_{n,\lambda}$。当$\lambda=0$时，称$Y$的分布为中心$\chi^2$分布，记为$Y\sim\chi_n^2$。
\end{definition}
\begin{property}\label{prop:Chi2Distribution}
	$\chi^2$分布具有如下性质：
	\begin{enumerate}
		\item 设$Y_i\sim\chi_{n_i,\lambda_i}^2,\;i=1,2,\dots,k$相互独立，则：
		\begin{gather*}
			\sum_{i=1}^{k}Y_i\sim\chi_{n,\lambda}^2,\quad\text{其中}
			n=\sum_{i=1}^{k}n_i,\;\lambda=\sum_{i=1}^{k}\lambda_i
		\end{gather*}
		\item 设$Y\sim\chi_{n,\lambda}^2$，则$\operatorname{E}(Y)=n+\lambda,\;\operatorname{Var}(Y)=2n+4\lambda$；
		\item 设$Y\sim\chi_{n,\lambda}^2$，$\mathbf{X}\sim N_n(\boldsymbol{\mu},I_n),\;Y=\mathbf{X}^{\top}\mathbf{X}$，则：
		\begin{equation*}
			\varphi_{Y}(t)=(1-2it)^{-\frac{n}{2}}\exp\left\{\frac{it\lambda}{1-2it}\right\}
		\end{equation*}
	\end{enumerate}
\end{property}
\begin{proof}
	(1)设$Y_i=\mathbf{X_i}^{\top}\mathbf{X_i}$，其中$\mathbf{X_i}\sim N_{n_i}(\boldsymbol{\mu_i},I_{n_i})$。令$\mathbf{X}=(\mathbf{X_1},\mathbf{X_2},\dots,\mathbf{X_k})^{\top}$，则有
	\begin{equation*}
		\sum_{i=1}^{k}Y_i=\sum_{i=1}^{k}\mathbf{X_i}^{\top}\mathbf{X_i}=(\mathbf{X_1},\mathbf{X_2},\dots,\mathbf{X_k})(\mathbf{X_1},\mathbf{X_2},\dots,\mathbf{X_k})^{\top}=\mathbf{X}^{\top}\mathbf{X}
	\end{equation*}
	因为$Y_i$相互独立，所以$\mathbf{X_i}$也相互独立，由\cref{prop:MultiNormal}(6)可得$\mathbf{X}\sim N_n(\boldsymbol{\mu},I_n)$，其中：
	\begin{equation*}
		n=\sum_{i=1}^{k}n_i,\;\boldsymbol{\mu}=(\boldsymbol{\mu_1},\boldsymbol{\mu_2},\dots,\boldsymbol{\mu_n})^{\top}
	\end{equation*}
	因此有：
	\begin{equation*}
		\sum_{i=1}^{k}Y_i\sim\chi_{n,\lambda}^2,\;\lambda=\boldsymbol{\mu}^{\top}\boldsymbol{\mu}=\sum_{i=1}^{k}\boldsymbol{\mu_i}^{\top}\boldsymbol{\mu_i}=\sum_{i=1}^{k}\lambda_i
	\end{equation*}\par
	(2)因为$Y\sim\chi_{n,\lambda}^2$，由定义可知$Y$可以表示为：
	\begin{equation*}
		Y=\sum_{i=1}^{n}X_i^2,\;X_i\sim N(\mu_i,1),\;\sum_{i=1}^{n}\mu_i^2=\lambda
	\end{equation*}
	其中$X_i$相互独立。由\cref{prop:Variance}(1)可知：
	\begin{equation*}
		\operatorname{E}(Y)=\operatorname{E}\left(\sum_{i=1}^{n}X_i^2\right)=\sum_{i=1}^{n}\operatorname{E}(X_i^2)=\sum_{i=1}^{n}\{\operatorname{Var}(X_i)+[\operatorname{E}(X_i)]^2\}=\sum_{i=1}^{n}(1+\mu_i^2)=n+\lambda
	\end{equation*}
	因为$X_i$相互独立，由\cref{prop:Variance}(3)和\cref{prop:CovMat}(7)可知：
	\begin{align*}
		\operatorname{Var}(Y)
		&=\operatorname{Var}\left(\sum_{i=1}^{n}X_i^2\right)=\sum_{i=1}^{n}\operatorname{Var}(X_i^2)=\sum_{i=1}^{n}\{\operatorname{E}(X_i^4)-[\operatorname{E}(X_i^2)]^2\} \\
		&=\sum_{i=1}^{n}\operatorname{E}(X_i^4)-\sum_{i=1}^{n}[\operatorname{E}(X_i^2)]^2
	\end{align*}
	由\cref{prop:Variance}(1)可知：
	\begin{equation*}
		\operatorname{E}(X_i^2)=\operatorname{Var}(X_i)+[\operatorname{E}(X_i)]^2=1+\mu_i^2
	\end{equation*}
	所以：
	\begin{equation*}
		\sum_{i=1}^{n}[\operatorname{E}(X_i^2)]^2=\sum_{i=1}^{n}(\mu_i^4+2\mu_i^2+1)=\sum_{i=1}^{n}\mu_i^4+2\sum_{i=1}^{n}\mu_i^2+n=\sum_{i=1}^{n}\mu_i^4+2\lambda+n
	\end{equation*}
	而：
	\begin{equation*}
		\operatorname{E}(X_i^4)=\mu_i^4+6\mu_i^2+3
	\end{equation*}
	于是：
	\begin{align*}
		\operatorname{Var}(Y)
		&=\sum_{i=1}^{n}\operatorname{E}(X_i^4)-\sum_{i=1}^{n}[\operatorname{E}(X_i^2)]^2 \\
		&=\sum_{i=1}^{n}\mu_i^4+6\sum_{i=1}^{n}\mu_i^2+3n-\sum_{i=1}^{n}\mu_i^4-2\lambda-n \\
		&=6\lambda+3n-2\lambda-n=2n+4\lambda
	\end{align*}\par
	(3)因为$\mathbf{X}\sim N_n(\boldsymbol{\mu},I_n)$，由\cref{prop:MultiNormal}(8)可知$\mathbf{X}_i$相互独立，所以$\mathbf{X}_i^2$相互独立。因为$Y=\mathbf{X}^{\top}\mathbf{X}=\sum\limits_{i=1}^n\mathbf{X}_i^2$，由\cref{prop:CharacteristicFunction}(4)可知：
	\begin{equation*}
		\varphi_{Y}(t)=\prod_{i=1}^n\varphi_{\mathbf{X}_i^2}(t)
	\end{equation*}
	下面来求$\varphi_{\mathbf{X}_i^2}$。\par
	由\cref{prop:MultiNormal}(3)可知$\mathbf{X}_i\sim N(\mu_i,1)$，于是：
	\begin{align*}
		\varphi_{\mathbf{X}_i^2}(t)
		&=\operatorname{E}(e^{it\mathbf{X}_i^2}) \\
		&=\int_{-\infty}^{+\infty}e^{itx^2}\frac{1}{\sqrt{2\pi}}e^{-\frac{(x-\mu_i)^2}{2}}\dif x \\
		&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}\exp\left\{-\frac{x^2 - 2\mu_ix + \mu_i^2}{2} + itx^2\right\}\dif x \\
		&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}\exp\left\{-\frac{x^2}{2} (1 - 2it) + \mu_ix - \frac{\mu_i^2}{2}\right\}\dif x \\
		&=\frac{1}{\sqrt{2\pi}}e^{-\frac{\mu_i^2}{2}} \int_{-\infty}^{\infty} \exp\left\{-\frac{x^2}{2} (1 - 2it) + \mu_ix\right\}\dif x
	\end{align*}
	这是一个Gaussian积分，由Gaussian积分公式可得：
	\begin{align*}
		\varphi_{\mathbf{X}_i^2}(t)
		&=\frac{1}{\sqrt{2\pi}}e^{-\frac{\mu_i^2}{2}} \int_{-\infty}^{\infty}\exp\left\{-\frac{x^2}{2} (1 - 2it) + \mu_i x\right\}\dif x \\
		&=\frac{1}{\sqrt{2\pi}}e^{-\frac{\mu_i^2}{2}}\sqrt{\frac{2\pi}{1-2it}}e^{\frac{\mu_i^2}{2-4it}}
		=(1-2it)^{-\frac{1}{2}}\exp\left\{\frac{\mu_i^2}{2-4it}-\frac{\mu_i^2}{2}\right\} \\
		&=(1-2it)^{-\frac{1}{2}}\exp\left\{\frac{it\mu_i^2}{1-2it}\right\}
	\end{align*}
	于是：
	\begin{equation*}
		\varphi_{Y}=\prod_{i=1}^n(1-2it)^{-\frac{1}{2}}\exp\left\{\frac{it\mu_i^2}{1-2it}\right\}=(1-2it)^{-\frac{n}{2}}\exp\left\{\frac{it\lambda}{1-2it}\right\}
	\end{equation*}\par
%	(4)先对$Y=X^2,\;X\sim N(\mu,1)$的情况进行讨论，设$Y$的分布函数为$F(y)$。因为$Y\geqslant0$，所以当$y\leqslant0$时$F(y)=0$。当$y>0$时有：
%	\begin{equation*}
%		F(y)=P(Y\leqslant y)=P(X^2\leqslant y)=P(-\sqrt{y}\leqslant X\leqslant\sqrt{y})=\int_{[-\sqrt{y},\sqrt{y}]}\frac{1}{\sqrt{2\pi}}\exp\left[-\frac{(x-\mu)^2}{2}\right]\dif\mu
%	\end{equation*}
%	根据\info{莱布尼兹法则，分布与密度的关系}可得：
%	\begin{equation*}
%		p(y)=
%	\end{equation*}
\end{proof}

\subsection{$t$分布}
\begin{definition}
	设随机变量$X\sim N(0,1),\;Y\sim\chi_n^2$且$X$与$Y$独立，则称：
	\begin{equation*}
		T=\frac{X}{\sqrt{Y/ n}}
	\end{equation*}
	为自由度是$n$的$t$变量，其分布称为自由度为$n$的$t$分布，记为$T\sim \operatorname{t}_n$。
\end{definition}

\subsection{$F$分布}
\begin{definition}
	设随机变量$X\sim \chi_m^2,\;Y\sim\chi_n^2$且$X$与$Y$独立，则称：
	\begin{equation*}
		F=\frac{X/m}{Y/n}
	\end{equation*}
	为自由度是$m$和$n$的$F$变量，其分布称为自由度为$m$和$n$的$F$分布，记为$F\sim \operatorname{F}_{m,n}$。
\end{definition}
\begin{property}\label{prop:FDistribution}
	$F$分布具有如下性质：
	\begin{enumerate}
		\item 若$F\sim \operatorname{F}_{m,n}$，则有$\frac{1}{F}\sim \operatorname{F}_{n,m}$；
		\item 若$T\sim \operatorname{t}_n$，则有$T^2\sim \operatorname{F}_{1,n}$；
		\item $\operatorname{F}_{m,n}(1-\alpha)=\dfrac{1}{\operatorname{F}_{n,m}(\alpha)}$；
	\end{enumerate}
\end{property}
\begin{proof}
	(1)由$F$分布的定义直接可得。\par
	(2)设：
	\begin{equation*}
		T=\frac{X}{\sqrt{Y/n}}
	\end{equation*}
	其中$X\sim N(0,1),\;Y\sim\chi_n^2$且$X$与$Y$独立，于是：
	\begin{equation*}
		T^2=\frac{X^2}{Y/n}=\frac{X^2/1}{Y/n}
	\end{equation*}
	注意到$X^2\sim\chi_1^2$且有$X^2$与$Y$独立，由$F$分布的定义即可得到$T^2\sim \operatorname{F}_{1,n}$。\par
	(3)由分位数的定义、\cref{prop:Measure}(2)和\cref{prop:ContinuousRV}(1)可得：
	\begin{gather*}
		P[F>\operatorname{F}_{m,n}(1-\alpha)]=\alpha \\
		P\left(\frac{X/m}{Y/n}>\operatorname{F}_{m,n}(1-\alpha)\right)=\alpha \\
		P\left(\frac{Y/n}{X/m}<\frac{1}{\operatorname{F}_{m,n}(1-\alpha)}\right)=\alpha \\
		P\left(\frac{Y/n}{X/m}\geqslant\frac{1}{\operatorname{F}_{m,n}(1-\alpha)}\right)=1-\alpha \\
		P\left(\frac{Y/n}{X/m}>\frac{1}{\operatorname{F}_{m,n}(1-\alpha)}\right)=1-\alpha
	\end{gather*}
	即：
	\begin{equation*}
		\operatorname{F}_{m,n}(1-\alpha)=\frac{1}{\operatorname{F}_{n,m}(\alpha)}
	\end{equation*}
\end{proof}

\subsection{Gamma分布}
\begin{definition}
	若连续型随机变量$X$的概率密度函数为：
	\begin{equation*}
		p(x)=\frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x},\;x\geqslant0
	\end{equation*}
	其中$\alpha,\lambda\in\mathbb{R}^{+}$为参数，则称$X$服从Gamma分布，记作$X\sim\operatorname{Gamma}(\alpha,\lambda)$。$\operatorname{Gamma}(1,\lambda)$也称作\gls{ExponentialDistribution}，记作$X\sim\operatorname{Exp}(\lambda)$。
\end{definition}
\begin{property}
	Gamma分布具有如下性质：
	\begin{enumerate}
		\item 若$X\sim\operatorname{Gamma}(\alpha,\lambda)$，则$\operatorname{E}(X)=\dfrac{\alpha}{\lambda},\;\operatorname{Var}(X)=\dfrac{\alpha}{\lambda^2}$；
		\item 自由度为$n$的中心$\chi^2$分布为$\operatorname{Gamma}\left(\dfrac{n}{2},\dfrac{1}{2}\right)$；
	\end{enumerate}
\end{property}
\begin{proof}
	(1)由\cref{theo:MeasurableCountableIntegral}可得：
	\begin{gather*}
		\operatorname{E}(X)=\int_{(0,+\infty)}x\frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}\dif\mu=\frac{1}{\Gamma(\alpha)}\int_{(0,+\infty)}(\lambda x)^{\alpha}e^{-\lambda x}\dif\mu=\frac{\Gamma(\alpha+1)}{\lambda\Gamma(\alpha)}=\frac{\alpha}{\lambda} \\
		\begin{aligned}
			\operatorname{E}(X^2)&=\int_{(0,+\infty)}x^2\frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}\dif\mu=\frac{1}{\lambda\Gamma(\alpha)}\int_{(0,+\infty)}(\lambda x)^{\alpha+1}e^{-\lambda x}\dif\mu \\
			&=\frac{\Gamma(\alpha+2)}{\lambda^2\Gamma(\alpha)}=\frac{(\alpha+1)\alpha}{\lambda^2}
		\end{aligned}
	\end{gather*}
	根据\cref{prop:Variance}(1)可得：
	\begin{equation*}
		\operatorname{Var}(X)=\operatorname{E}(X^2)-[\operatorname{E}(X)]^2=\frac{(\alpha+1)\alpha}{\lambda^2}-\frac{\alpha^2}{\lambda^2}=\frac{\alpha}{\lambda^2}
	\end{equation*}\par
	(2)
\end{proof}

\subsection{Beta分布}
\begin{definition}
	若连续型随机变量$X$的概率密度函数为：
	\begin{equation*}
		p(x)=\frac{1}{\operatorname{B}(a,b)}x^{a-1}(1-x)^{b-1},\;x\in(0,1)
	\end{equation*}
	其中$a,b\in\mathbb{R}^{+}$为参数，则称$X$服从Beta分布，记作$X\sim\operatorname{Beta}(\alpha,\lambda)$。
\end{definition}
\begin{property}
	Beta分布具有如下性质：
	\begin{enumerate}
		\item 若$X\sim\operatorname{Beta}(a,b)$，则$\operatorname{E}(X)=\dfrac{a}{a+b},\;\operatorname{Var}(X)=\dfrac{ab}{(a+b)^2(a+b+1)}$；
	\end{enumerate}
\end{property}
\begin{proof}
	(1)由\cref{theo:MeasurableCountableIntegral}可得：
	\begin{gather*}
		\operatorname{E}(X)=\int_{(0,1)}x\frac{1}{\operatorname{B}(a,b)}x^{a-1}(1-x)^{b-1}\dif\mu=\frac{\operatorname{Beta}(a+1,b)}{\operatorname{B}(a,b)}=\frac{\Gamma(a+1)\Gamma(b)\Gamma(a+b)}{\Gamma(a+b+1)\Gamma(a)\Gamma(b)}=\frac{a}{a+b} \\
		\begin{aligned}
			\operatorname{E}(X^2)&=\int_{(0,1)}x^2\frac{1}{\operatorname{B}(a,b)}x^{a-1}(1-x)^{b-1}\dif\mu=\frac{\operatorname{Beta}(a+2,b)}{\operatorname{B}(a,b)} \\
			&=\frac{\Gamma(a+2)\Gamma(b)\Gamma(a+b)}{\Gamma(a+b+2)\Gamma(a)\Gamma(b)}=\frac{a(a+1)}{(a+b)(a+b+1)}
		\end{aligned}
	\end{gather*}
	根据\cref{prop:Variance}(1)可得：
	\begin{align*}
		\operatorname{Var}(X)&=\operatorname{E}(X^2)-[\operatorname{E}(X)]^2=\frac{a(a+1)}{(a+b)(a+b+1)}-\frac{a^2}{(a+b)^2} \\
		&=\frac{a(a+1)(a+b)-a^2(a+b+1)}{(a+b)^2(a+b+1)}=\frac{a^3+a^2b+a^2+ab-a^3-a^2b-a^2}{(a+b)^2(a+b+1)} \\
		&=\frac{ab}{(a+b)^2(a+b+1)}\qedhere
	\end{align*}
\end{proof}

\subsection{随机数的生成}
\begin{theorem}\label{theo:AcceptanceRejectionSampling}
	设$(X,\mathscr{A},P)$是一个概率空间，$\mu\ll P$，$\tilde{\pi}:X\to[0,\infty)$是$\mathscr{A}$上的可测函数，且满足：
	\begin{equation*}
		Z:=\int_{X}\tilde{\pi}(x)\dif\mu\in(0,+\infty)
	\end{equation*}
	目标概率测度$\pi$为：
	\begin{equation*}
		\pi(A):=\frac{1}{Z}\int_{A}\tilde{\pi}(x)\dif\mu,\quad\forall A\in\mathscr{A}
	\end{equation*}
	$Q$是一个提议概率测度且$Q\ll\mu$，记其Radon-Nikodym导数为$q=\dif Q/\dif\mu$。若存在常数$M\in(0,+\infty)$使得：
	\begin{equation*}
		\tilde{\pi}(x)\leqslant Mq(x)\;\text{a.e. 于}(X,\mathscr{A},\mu)
	\end{equation*}
	考虑如下接受--拒绝采样算法：
	\begin{enumerate}
		\item 生成$f\sim Q$；
		\item 在与$f$独立的条件下生成$U\sim\mathrm{Unif}(0,1)$；
		\item 若
		\begin{equation*}
			U\leqslant\alpha(f):=\frac{\tilde{\pi}(f)}{Mq(f)},
		\end{equation*}
		则接受$f$，否则拒绝并返回第$1$步重新生成。
	\end{enumerate}
	记单次试验中的接受事件为$B=\{U\leqslant\alpha(f)\}$，则有以下结论成立：
	\begin{enumerate}
		\item 给定接受事件$B$，随机变量$f$的条件分布恰为目标分布$\pi$，即：
		\begin{equation*}
			P(f\in A|B)=\pi(A),\quad\forall A\in\mathscr{A}
		\end{equation*}
		\item 单次试验的接受概率为：
		\begin{equation*}
			P(A)=\frac{Z}{M}
		\end{equation*}
		\item 设$Y$为算法第一次被接受时输出的样本，则
		\begin{equation*}
			Y\sim \pi
		\end{equation*}
	\end{enumerate}
\end{theorem}
\begin{proof}
	(1)任取$A\in\mathscr{A}$。由全概率公式以及$U$与$f$的独立性，
	\begin{equation*}
		P(f\in A,B)=\int_{A}P(f=x,B)Q(\dif x)=\int_{A}\frac{P(B)Q(\dif x)}{Q(\dif x)}Q(\dif x)=\int_{A}P(B|f=x)Q(\dif x)
	\end{equation*}
	又由于$U\sim\mathrm{Unif}(0,1)$，且由支配条件$\tilde{\pi}(x)\le M q(x)$可知$0\le \alpha(x)\le 1$，从而
	\begin{equation*}
		\mathbb{P}(A\mid X=x)
		=\mathbb{P}(U\le \alpha(x))
		=\alpha(x)
		=\frac{\tilde{\pi}(x)}{M q(x)},\qquad \mu\text{-a.e.}
	\end{equation*}
	利用$Q(dx)=q(x)\,d\mu(x)$，得到
	\begin{equation*}
		\mathbb{P}(X\in B,A)
		=\int_{B}\frac{\tilde{\pi}(x)}{M q(x)}\,q(x)\,d\mu(x)
		=\frac{1}{M}\int_{B}\tilde{\pi}(x)\,d\mu(x).
	\end{equation*}
	
	\medskip
	\noindent\textbf{第二步：计算接受概率$\mathbb{P}(A)$。}
	
	取$B=\mathcal{X}$，由上式直接得到
	\begin{equation*}
		\mathbb{P}(A)
		=\frac{1}{M}\int_{\mathcal{X}}\tilde{\pi}(x)\,d\mu(x)
		=\frac{Z}{M},
	\end{equation*}
	从而证明(ii)。
	
	\medskip
	\noindent\textbf{第三步：确定条件分布$\mathcal{L}(X\mid A)$。}
	
	由条件概率定义以及前两步，
	\begin{align*}
		\mathbb{P}(X\in B\mid A)
		&=\frac{\mathbb{P}(X\in B,A)}{\mathbb{P}(A)} \\
		&=\frac{\frac{1}{M}\int_{B}\tilde{\pi}(x)\,d\mu(x)}{\frac{1}{M}\int_{\mathcal{X}}\tilde{\pi}(x)\,d\mu(x)} \\
		&=\frac{1}{Z}\int_{B}\tilde{\pi}(x)\,d\mu(x)
		=\Pi(B),
	\end{align*}
	这即证明了(i)。
	
	\medskip
	\noindent\textbf{第四步：算法输出样本的分布。}
	
	设$(X_k,U_k)_{k\ge1}$是独立同分布的试验序列，定义
	\begin{equation*}
		A_k:=\{U_k\le \alpha(X_k)\},\qquad 
		T:=\inf\{k\ge1:A_k\text{发生}\},\qquad 
		Y:=X_T.
	\end{equation*}
	则$Y$即算法的输出样本。
	
	对任意$B\in\mathcal{B}$，有
	\begin{align*}
		\mathbb{P}(Y\in B)
		&=\sum_{k=1}^{\infty}\mathbb{P}(T=k,X_k\in B) \\
		&=\sum_{k=1}^{\infty}\mathbb{P}(A_1^c,\dots,A_{k-1}^c)\,\mathbb{P}(A_k,X_k\in B).
	\end{align*}
	由于各次试验独立同分布，
	\begin{equation*}
		\mathbb{P}(A_1^c,\dots,A_{k-1}^c)=(1-\mathbb{P}(A))^{k-1},
		\qquad 
		\mathbb{P}(A_k,X_k\in B)=\mathbb{P}(A,X\in B).
	\end{equation*}
	因此
	\begin{align*}
		\mathbb{P}(Y\in B)
		&=\left[\sum_{k=1}^{\infty}(1-\mathbb{P}(A))^{k-1}\right]\mathbb{P}(A,X\in B) \\
		&=\frac{1}{\mathbb{P}(A)}\mathbb{P}(A,X\in B)
		=\mathbb{P}(X\in B\mid A)
		=\Pi(B),
	\end{align*}
	其中最后一步由(i)成立。
	
	故$Y\sim\Pi$，从而证明(iii)。证毕。
\end{proof}
